<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 3 Analyse de regroupements | Analyse multidimensionnelle appliquée</title>
  <meta name="description" content="Recueil de notes pour MATH 60602 Analyse multidimensionnelle appliquée, avec des exemples d’applications en SAS et en R." />
  <meta name="generator" content="bookdown 0.21.6 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 3 Analyse de regroupements | Analyse multidimensionnelle appliquée" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Recueil de notes pour MATH 60602 Analyse multidimensionnelle appliquée, avec des exemples d’applications en SAS et en R." />
  <meta name="github-repo" content="lbelzile/math60602" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 3 Analyse de regroupements | Analyse multidimensionnelle appliquée" />
  
  <meta name="twitter:description" content="Recueil de notes pour MATH 60602 Analyse multidimensionnelle appliquée, avec des exemples d’applications en SAS et en R." />
  

<meta name="author" content="(c) Denis Larocque, Léo Belzile" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="analyse-factorielle.html"/>
<link rel="next" href="selection-modele.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Analyse multidimensionnelle appliquée</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#analyse-exploratoire"><i class="fa fa-check"></i><b>1.1</b> Analyse exploratoire de données</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html"><i class="fa fa-check"></i><b>2</b> Analyse factorielle exploratoire</a>
<ul>
<li class="chapter" data-level="2.1" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html#rappels-sur-le-coefficient-de-corrélation-linéaire"><i class="fa fa-check"></i><b>2.2</b> Rappels sur le coefficient de corrélation linéaire</a></li>
<li class="chapter" data-level="2.3" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html#exemple-de-questionnaire"><i class="fa fa-check"></i><b>2.3</b> Exemple de questionnaire</a></li>
<li class="chapter" data-level="2.4" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html#description-du-modèle-danalyse-factorielle"><i class="fa fa-check"></i><b>2.4</b> Description du modèle d’analyse factorielle</a></li>
<li class="chapter" data-level="2.5" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html#estimation-des-facteurs"><i class="fa fa-check"></i><b>2.5</b> Estimation des facteurs</a></li>
<li class="chapter" data-level="2.6" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html#choix-du-nombre-de-facteurs"><i class="fa fa-check"></i><b>2.6</b> Choix du nombre de facteurs</a></li>
<li class="chapter" data-level="2.7" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html#construction-déchelles-à-partir-des-facteurs"><i class="fa fa-check"></i><b>2.7</b> Construction d’échelles à partir des facteurs</a></li>
<li class="chapter" data-level="2.8" data-path="analyse-factorielle.html"><a href="analyse-factorielle.html#compléments-dinformation"><i class="fa fa-check"></i><b>2.8</b> Compléments d’information</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html"><i class="fa fa-check"></i><b>3</b> Analyse de regroupements</a>
<ul>
<li class="chapter" data-level="3.1" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#segmentation-de-seniors-en-voyage-organisé"><i class="fa fa-check"></i><b>3.2</b> Segmentation de seniors en voyage organisé</a></li>
<li class="chapter" data-level="3.3" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#exploration-graphique-préalable-et-analyse-en-composantes-principales"><i class="fa fa-check"></i><b>3.3</b> Exploration graphique préalable et analyse en composantes principales</a></li>
<li class="chapter" data-level="3.4" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#méthodes-hiérarchiques"><i class="fa fa-check"></i><b>3.4</b> Méthodes hiérarchiques</a></li>
<li class="chapter" data-level="3.5" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#calcul-alternatif-des-distances-pour-le-regroupement-hiérarchique"><i class="fa fa-check"></i><b>3.5</b> Calcul alternatif des distances pour le regroupement hiérarchique</a></li>
<li class="chapter" data-level="3.6" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#standardisation-des-variables"><i class="fa fa-check"></i><b>3.6</b> Standardisation des variables</a></li>
<li class="chapter" data-level="3.7" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#autres-mesures-de-dissemblance"><i class="fa fa-check"></i><b>3.7</b> Autres mesures de dissemblance</a></li>
<li class="chapter" data-level="3.8" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#méthodes-non-hiérarchiques"><i class="fa fa-check"></i><b>3.8</b> Méthodes non hiérarchiques</a></li>
<li class="chapter" data-level="3.9" data-path="analyse-regroupements.html"><a href="analyse-regroupements.html#considérations-pratiques"><i class="fa fa-check"></i><b>3.9</b> Considérations pratiques</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="selection-modele.html"><a href="selection-modele.html"><i class="fa fa-check"></i><b>4</b> Sélection de variables et de modèles</a>
<ul>
<li class="chapter" data-level="4.1" data-path="selection-modele.html"><a href="selection-modele.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="selection-modele.html"><a href="selection-modele.html#sélection-de-variables-et-de-modèles-selon-les-buts-de-létude"><i class="fa fa-check"></i><b>4.2</b> Sélection de variables et de modèles selon les buts de l’étude</a></li>
<li class="chapter" data-level="4.3" data-path="selection-modele.html"><a href="selection-modele.html#mieux-vaut-plus-que-moins"><i class="fa fa-check"></i><b>4.3</b> Mieux vaut plus que moins</a></li>
<li class="chapter" data-level="4.4" data-path="selection-modele.html"><a href="selection-modele.html#trop-beau-pour-être-vrai"><i class="fa fa-check"></i><b>4.4</b> Trop beau pour être vrai</a></li>
<li class="chapter" data-level="4.5" data-path="selection-modele.html"><a href="selection-modele.html#principes-généraux"><i class="fa fa-check"></i><b>4.5</b> Principes généraux</a></li>
<li class="chapter" data-level="4.6" data-path="selection-modele.html"><a href="selection-modele.html#pénalisation-et-critères-dinformation"><i class="fa fa-check"></i><b>4.6</b> Pénalisation et critères d’information</a></li>
<li class="chapter" data-level="4.7" data-path="selection-modele.html"><a href="selection-modele.html#division-de-léchantillon-et-validation-croisée"><i class="fa fa-check"></i><b>4.7</b> Division de l’échantillon et validation croisée</a></li>
<li class="chapter" data-level="4.8" data-path="selection-modele.html"><a href="selection-modele.html#cibler-les-clients-pour-lenvoi-dun-catalogue"><i class="fa fa-check"></i><b>4.8</b> Cibler les clients pour l’envoi d’un catalogue</a></li>
<li class="chapter" data-level="4.9" data-path="selection-modele.html"><a href="selection-modele.html#recherche-automatique-du-meilleur-modèle"><i class="fa fa-check"></i><b>4.9</b> Recherche automatique du meilleur modèle</a></li>
<li class="chapter" data-level="4.10" data-path="selection-modele.html"><a href="selection-modele.html#méthodes-classiques-de-sélection"><i class="fa fa-check"></i><b>4.10</b> Méthodes classiques de sélection</a></li>
<li class="chapter" data-level="4.11" data-path="selection-modele.html"><a href="selection-modele.html#recherche-séquentielle-automatique-limitée"><i class="fa fa-check"></i><b>4.11</b> Recherche séquentielle automatique limitée</a></li>
<li class="chapter" data-level="4.12" data-path="selection-modele.html"><a href="selection-modele.html#méthodes-de-régression-avec-régularisation"><i class="fa fa-check"></i><b>4.12</b> Méthodes de régression avec régularisation</a></li>
<li class="chapter" data-level="4.13" data-path="selection-modele.html"><a href="selection-modele.html#moyenne-de-modèles"><i class="fa fa-check"></i><b>4.13</b> Moyenne de modèles</a></li>
<li class="chapter" data-level="4.14" data-path="selection-modele.html"><a href="selection-modele.html#évaluation-de-la-performance"><i class="fa fa-check"></i><b>4.14</b> Évaluation de la performance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-logistique.html"><a href="regression-logistique.html"><i class="fa fa-check"></i><b>5</b> Régression logistique</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression-logistique.html"><a href="regression-logistique.html#introduction-4"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="regression-logistique.html"><a href="regression-logistique.html#modèle-de-régression-logistique"><i class="fa fa-check"></i><b>5.2</b> Modèle de régression logistique</a></li>
<li class="chapter" data-level="5.3" data-path="regression-logistique.html"><a href="regression-logistique.html#estimation-des-paramètres"><i class="fa fa-check"></i><b>5.3</b> Estimation des paramètres</a></li>
<li class="chapter" data-level="5.4" data-path="regression-logistique.html"><a href="regression-logistique.html#cowboy"><i class="fa fa-check"></i><b>5.4</b> Exemple du <em>Professional Rodeo Cowboys Association</em></a></li>
<li class="chapter" data-level="5.5" data-path="regression-logistique.html"><a href="regression-logistique.html#classification-et-prédiction-à-laide-de-la-régression-logistique"><i class="fa fa-check"></i><b>5.5</b> Classification et prédiction à l’aide de la régression logistique</a></li>
<li class="chapter" data-level="5.6" data-path="regression-logistique.html"><a href="regression-logistique.html#classification-avec-une-matrice-de-gain"><i class="fa fa-check"></i><b>5.6</b> Classification avec une matrice de gain</a></li>
<li class="chapter" data-level="5.7" data-path="regression-logistique.html"><a href="regression-logistique.html#sélection-de-variables-en-régression-logistique"><i class="fa fa-check"></i><b>5.7</b> Sélection de variables en régression logistique</a></li>
<li class="chapter" data-level="5.8" data-path="regression-logistique.html"><a href="regression-logistique.html#performance-des-différents-modèles-pour-lexemple-des-clients-cibles"><i class="fa fa-check"></i><b>5.8</b> Performance des différents modèles pour l’exemple des clients cibles</a></li>
<li class="chapter" data-level="5.9" data-path="regression-logistique.html"><a href="regression-logistique.html#extensions-du-modèle-de-régression-logistique-à-plus-de-deux-catégories"><i class="fa fa-check"></i><b>5.9</b> Extensions du modèle de régression logistique à plus de deux catégories</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="analyse-survie.html"><a href="analyse-survie.html"><i class="fa fa-check"></i><b>6</b> Analyse de survie</a>
<ul>
<li class="chapter" data-level="6.1" data-path="analyse-survie.html"><a href="analyse-survie.html#introduction-5"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="analyse-survie.html"><a href="analyse-survie.html#fonctions-de-survie-et-de-risque"><i class="fa fa-check"></i><b>6.2</b> Fonctions de survie et de risque</a></li>
<li class="chapter" data-level="6.3" data-path="analyse-survie.html"><a href="analyse-survie.html#estimation-dune-courbe-de-survie-et-de-risque"><i class="fa fa-check"></i><b>6.3</b> Estimation d’une courbe de survie et de risque</a></li>
<li class="chapter" data-level="6.4" data-path="analyse-survie.html"><a href="analyse-survie.html#comparaison-de-deux-courbes-de-survie"><i class="fa fa-check"></i><b>6.4</b> Comparaison de deux courbes de survie</a></li>
<li class="chapter" data-level="6.5" data-path="analyse-survie.html"><a href="analyse-survie.html#modèle-à-risques-proportionnels-de-cox"><i class="fa fa-check"></i><b>6.5</b> Modèle à risques proportionnels de Cox</a></li>
<li class="chapter" data-level="6.6" data-path="analyse-survie.html"><a href="analyse-survie.html#extensions-du-modèle-de-cox"><i class="fa fa-check"></i><b>6.6</b> Extensions du modèle de Cox</a></li>
<li class="chapter" data-level="6.7" data-path="analyse-survie.html"><a href="analyse-survie.html#risques-non-proportionnels"><i class="fa fa-check"></i><b>6.7</b> Risques non proportionnels</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="données-manquantes-donnees-manquantes.html"><a href="données-manquantes-donnees-manquantes.html"><i class="fa fa-check"></i><b>7</b> Données manquantes {donnees-manquantes}</a>
<ul>
<li class="chapter" data-level="7.1" data-path="données-manquantes-donnees-manquantes.html"><a href="données-manquantes-donnees-manquantes.html#terminologie"><i class="fa fa-check"></i><b>7.1</b> Terminologie</a></li>
<li class="chapter" data-level="7.2" data-path="données-manquantes-donnees-manquantes.html"><a href="données-manquantes-donnees-manquantes.html#quelques-méthodes"><i class="fa fa-check"></i><b>7.2</b> Quelques méthodes</a></li>
<li class="chapter" data-level="7.3" data-path="données-manquantes-donnees-manquantes.html"><a href="données-manquantes-donnees-manquantes.html#example-dapplication-de-limputation"><i class="fa fa-check"></i><b>7.3</b> Example d’application de l’imputation</a></li>
<li class="chapter" data-level="7.4" data-path="données-manquantes-donnees-manquantes.html"><a href="données-manquantes-donnees-manquantes.html#valeurs-manquantes-dans-un-contexte-de-prédiction"><i class="fa fa-check"></i><b>7.4</b> Valeurs manquantes dans un contexte de prédiction</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publié avec bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Analyse multidimensionnelle appliquée</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analyse-regroupements" class="section level1" number="3">
<h1><span class="header-section-number">Chapitre 3</span> Analyse de regroupements</h1>
<div id="introduction-2" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>On cherche à créer des groupes (<em>clusters</em>) d’individus homogènes en utilisant <span class="math inline">\(p\)</span> variables <span class="math inline">\(X_1, \ldots, X_p\)</span>. Plus précisément, on veut combiner des sujets en groupes (interprétables) de telle sorte que les individus d’un même groupe soient le plus semblables possible par rapport à certaines caractéristiques et que les groupes soient le plus différent possible.<br />
Nous disposons des observations pour <span class="math inline">\(n\)</span> individus et <span class="math inline">\(X_{ij}\)</span> dénote la valeur de la <span class="math inline">\(j\)</span>e variable explicative pour le <span class="math inline">\(i\)</span>e sujet: les variables correspondant au sujet <span class="math inline">\(S_i\)</span> sont donc <span class="math inline">\(X_{i1}, \ldots, X_{ip}\)</span>.</p>
<p>Il y a une certaine analogie avec l’analyse factorielle. En analyse factorielle, on cherche à déterminer s’il y a des groupes de <strong>variables</strong> corrélées entre elles. On cherche donc à regrouper des variables. En analyse de regroupements, on cherche plutôt à créer des groupes de <strong>sujets</strong> similaires. Les deux méthodes servent pour l’analyse exploratoire: en particulier, on ne peut pas regrouper les données pour ensuite comparer la moyenne des segments obtenus à l’aide de statistiques sans ajustement préalable.</p>
<p>Étapes d’une analyse de regroupements</p>
<ol style="list-style-type: decimal">
<li>Choisir les variables pertinentes à l’analyse.</li>
<li>Décider comment seront mesurées les « distances » entre les sujets. Cela revient à choisir une mesure de dissemblance.</li>
<li>Décider quelle méthode sera utilisée (méthode hiérarchique, méthode non hiérarchique).</li>
<li>Choisir le nombre de groupes, soit à partir de connaissances à priori, soit en se basant sur l’analyse de regroupements elle-même.</li>
<li>Interpréter les groupes obtenus.</li>
<li>Utiliser ces groupes dans d’autres analyses, le cas échéant.</li>
</ol>
<div id="mesures-de-dissemblance" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Mesures de dissemblance</h3>
<p>Une mesure de dissemblance sert à quantifier la distance entre deux sujets <span class="math inline">\(S_i\)</span> et <span class="math inline">\(S_j\)</span> en se basant sur les <span class="math inline">\(p\)</span> variables <span class="math inline">\(X_1, \ldots, X_p\)</span>. Plus cette mesure est petite, plus les sujets <span class="math inline">\(S_i\)</span> et <span class="math inline">\(S_j\)</span> sont similaires. Même s’il y a des exceptions, la plupart des mesures de dissemblances <span class="math inline">\(d\)</span> ont les propriétés suivantes :</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(d(S_i, S_j) \geq 0\)</span> (positivité);</li>
<li><span class="math inline">\(d(S_i, S_i)=0\)</span>;</li>
<li><span class="math inline">\(d(S_i, S_j)=d(S_j, S_i)\)</span> (symmétrie);</li>
<li><span class="math inline">\(d(S_i, S_j)\)</span> augmente au fur et à mesure que les deux sujets deviennent plus différents.</li>
</ol>
<p>Lorsque toutes les variables sont continues, une mesure de dissemblance souvent utilisée est la distance euclidienne entre sujets, soit
<span class="math display">\[\begin{align*}
d(S_i, S_j) = \sqrt{(X_{i1}-X_{j1})^2 + \cdots + (X_{ip}-X_{jp})^2}.
\end{align*}\]</span></p>
<p>La distance euclidienne est tout simplement la longueur du segment qui relie les deux points dans l’espace.</p>
<p>Nous verrons plus tard d’autres mesures de dissemblances incluant des mesures qui peuvent être utilisées avec des variables binaires, nominales et ordinales.</p>
</div>
<div id="méthodes-hiérarchiques-et-non-hiérarchiques" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Méthodes hiérarchiques et non hiérarchiques</h3>
<p>Les <strong>méthodes hiérarchiques</strong> assignent les individus aux groupes à l’aide d’un algorithme glouton en partant du cas à <span class="math inline">\(n\)</span> groupes où chaque sujet est un groupe à part entière. La distance entre chaque paire de groupe est calculée. Les deux groupes ayant la distance la plus petite sont regroupés pour ne laisser que <span class="math inline">\(n-1\)</span> groupes. La distance entre chaque paire de groupe est à nouveau calculée (pour les groupes). Les deux groupes ayant la distance la plus petite sont regroupés pour ne former qu’un seul groupe et ainsi de suite. Le processus se continue ainsi jusqu’à ce que tous les sujets soient regroupés en un seul groupe.</p>
<p>Avec une méthode hiérarchique, on n’a pas besoin de spécifier le nombre de groupes à priori. Cependant, une fois qu’un sujet est assigné à un groupe, il ne peut le quitter pour être réassigné à un autre groupe plus tard. Ce qui différencie les différentes méthodes hiérarchiques est la manière dont est calculée la distance entre deux groupes.</p>
<p>Pour les <strong>méthodes non hiérarchiques</strong>, le nombre de groupe est spécifié au départ et un algorithme cherche, à partir d’une solution initiale, la meilleure distribution des sujets à travers ce nombre de groupe d’une manière itérative. Avec ces méthodes, l’assignation d’un sujet peut être modifiée d’une itération à l’autre. Il faut cependant spécifier le nombre de groupe et les « centres » de ces groupes au départ. La solution peut être très sensible au choix des centres initiaux.</p>
</div>
</div>
<div id="segmentation-de-seniors-en-voyage-organisé" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Segmentation de seniors en voyage organisé</h2>
<p>Les données sont inspirées de</p>
<blockquote>
<p>Hsu, C. H. C. et Lee E.-J. (2002). Segmentation of Senior Motorcoach Travelers. <em>Journal of Travel Research</em>, <strong>40</strong>, 364-373.</p>
</blockquote>
<p>Les buts de l’étude étaient</p>
<ol style="list-style-type: decimal">
<li>Regrouper les gens de 55 et plus qui participent à des voyages organisés en autobus en groupes homogènes selon des caractéristiques reliées au choix de l’opérateur et du voyage.</li>
<li>Examiner les caractéristiques de ces groupes.</li>
<li>Examiner les caractéristiques démographiques de ces groupes.</li>
</ol>
<p>Nous allons nous intéresser principalement aux deux premiers points ici. Un questionnaire a été élaboré afin d’évaluer l’importance de 55 caractéristiques des opérateurs de voyages organisés en autobus et des voyages eux-mêmes à l’aide d’une échelle de Likert à cinq points, allant de extrêmement important (<span class="math inline">\(\texttt{5}\)</span>) à pas important du tout (<span class="math inline">\(\texttt{1}\)</span>). Des données sont disponibles pour 150 sujets (il y en avait 817 dans l’article). Elles se trouvent dans le fichier <code>cluster1.sas7bdat</code>.</p>
<p>Au lieu de faire une analyse de regroupements avec les 55 items du questionnaire, les auteurs ont choisi de faire une analyse factorielle avec rotation varimax au préalable afin de réduire le nombre de variables à six facteurs interprétables :</p>
<ul>
<li>Activités sociales (<span class="math inline">\(X_1\)</span>): formé de cinq items</li>
<li>Politiques de l’opérateur et références (<span class="math inline">\(X_2\)</span>) : formé de six items.</li>
<li>Horaires flexibles (<span class="math inline">\(X_3\)</span>): formé de trois items.</li>
<li>Santé et sécurité (<span class="math inline">\(X_4\)</span>) : formé de quatre items.</li>
<li>Matériel publicitaire (<span class="math inline">\(X_5\)</span>): formé de deux items.</li>
<li>Réputation (<span class="math inline">\(X_6\)</span>): formé de deux items.</li>
</ul>
<p>On voit donc que 22 items, parmi les 55, sont utilisés dans la définition de ces six facteurs. Dans l’article, les auteurs ont décidé d’inclure ces 22 items dans l’analyse de regroupements. Pour notre part et afin de simplifier l’exemple, nous allons plutôt créer six nouvelles échelles en faisant la moyenne des items de chaque facteur ci-haut et utiliser seulement ces six échelles dans l’analyse de regroupements. Les valeurs de ces six variables pour les 150 sujets se trouvent dans le fichier <code>cluster1a.sas7bdat</code> et sont toutes dans l’intervalle <span class="math inline">\([1,5]\)</span>, puisqu’elles représentent la moyenne d’échelles de Likert.</p>
<p><img src="figures/04-clustering-e1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="exploration-graphique-préalable-et-analyse-en-composantes-principales" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Exploration graphique préalable et analyse en composantes principales</h2>
<p>Comme c’est le cas avec n’importe quelle analyse statistique, il est nécessaire de tenter d’explorer les données graphiquement. On peut parfois réussir à visualiser les groupes d’observations, ce qui nous permettra une fois l’analyse de regroupement complétée de vérifier la qualité de cette dernière.</p>
<p>Une première idée consiste à faire le graphe de toutes les paires de variables mais ceci possède deux limites,</p>
<ol style="list-style-type: lower-roman">
<li>il y aura beaucoup de graphes si le nombre de variables est grand et ii) on examine seulement les relations bivariées.</li>
</ol>
<p><img src="MATH60602_files/figure-html/04-pairplot-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Il n’est pas nécessairement évident de détecter des groupes d’observations ainsi, alors qu’on n’a déjà que six variables. On connaît les vrais groupes ici mais ce n’est pas le cas en pratique.</p>
<div id="analyse-en-composantes-principales" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Analyse en composantes principales</h3>
<p>L’analyse en composantes principales peut être vue comme une méthode de réduction de la dimensionnalité. En fait, elle peut servir à faire de l’analyse factorielle et nous en avons déjà parlé dans le chapitre correspondant. En partant de <span class="math inline">\(p\)</span> variables <span class="math inline">\(X_1, \ldots, X_p\)</span>, on forme de nouvelles variables qui sont des combinaisons linéaires des variables originales,
<span class="math display">\[\begin{align*}
C_j &amp;= w_{j1} X_1 + w_{j2} X_2 + \cdots + w_{jp} X_p, \qquad (j=1, \ldots, p),
\end{align*}\]</span>
de telle sorte que</p>
<ul>
<li>La première variable formée, <span class="math inline">\(C_1\)</span>, appelée première composante principale, possède la variance maximale parmi toutes les combinaisons linéaires sous la contrainte <span class="math inline">\(w_{11}^2 + \cdots + w_{1p}^2=1\)</span>.</li>
<li>La deuxième composante principale <span class="math inline">\(C_2\)</span> possède la variance maximale parmi toutes les combinaisons linéaires qui sont non corrélées avec <span class="math inline">\(C_1\)</span> sous la contrainte <span class="math inline">\(w_{21}^2 + \cdots + w_{2p}^2=1\)</span>.</li>
<li>La troisième composante principale <span class="math inline">\(C_3\)</span> possède la variance maximale parmi toutes les combinaisons linéaires qui sont non corrélées avec <span class="math inline">\(C_1\)</span> et <span class="math inline">\(C_2\)</span> sous la contrainte <span class="math inline">\(w_{31}^2 + \cdots + w_{3p}^2=1\)</span>.</li>
</ul>
<p>et ainsi de suite. Les contraintes sont nécessaires afin de standardiser le problème car il serait possible d’avoir des variances infinies sinon. Ainsi, les composantes principales forment un ensemble de variables non corrélées entre elles, qui récupèrent en ordre décroisant le plus possible de la variance des variables originales. La somme des variances des <span class="math inline">\(p\)</span> composantes principales est égale à la somme des variances des <span class="math inline">\(p\)</span> variables originales.</p>
<p>Mathématiquement, les composantes principales correspondent aux vecteurs propres de la matrice de covariance. On peut également utiliser la matrice de corrélation, et cette dernière est sélectionnée par défaut dans la méthode <code>princomp</code> à moins de spécifier <code>cov</code> dans l’appel. L’avantage de la matrice de corrélation (ou de la standardisation des variables) est que l’unité de mesure n’impacte pas le résultat; autrement, un point plus important est attribué aux variables qui ont la plus forte hétérogénéité.</p>
<p>On espère donc en général qu’un petit nombre de composantes principales réussira à expliquer la plus grande partie de la variance totale. Ces composantes pourraient servir de variables pour l’analyse de regroupement: une fois les étiquettes obtenues, on pourrait alors calculer les statistiques descriptives sur les variables originales. Dans notre exemple, on va seulement s’en servir comme outil graphique pour une analyse de regroupements en réduisant la dimension afin de permettre la visualisation des regroupements obtenus.</p>
<p><img src="figures/04-clustering-e18.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Cette étape est généralement effectuée avant l’analyse de regroupement.
Le fichier <code>cluster7_acp.sas</code> contient les commandes pour faire une analyse en composantes principales et sauvegarder les composantes principales afin d’en faire des graphiques. La sortie inclut une mesure de la variance cumulée des <span class="math inline">\(K\)</span> premières valeurs propres. La colonne <code>Proportion</code> donne la proportion de la variance totale qui est expliquée par la composante correspondante. La colonne <code>Cumulé</code> donne le cumul de variance totale expliquée par les composantes jusque là. Ainsi, les deux premières composantes principales reproduisent 76,7% de la variance totale originale.</p>
<p>Même en ne connaissant pas l’appartenance des observations au regroupement, on distingue assez clairement trois groupes. Le panneau droit du graphique <a href="analyse-regroupements.html#fig:04-acp">3.1</a> montre les deux composantes principales, mais avec l’identification des vrais groupes (qu’on ne connaîtra pas en pratique).</p>
<div class="figure" style="text-align: center"><span id="fig:04-acp"></span>
<img src="MATH60602_files/figure-html/04-acp-1.png" alt="Projection des observations sur les composantes principales avec la vraie identité des groupes" width="90%" />
<p class="caption">
Figure 3.1: Projection des observations sur les composantes principales avec la vraie identité des groupes
</p>
</div>
</div>
</div>
<div id="méthodes-hiérarchiques" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Méthodes hiérarchiques</h2>
<p>Cette méthode débute avec <span class="math inline">\(n\)</span> groupes, un par sujet, et procède en regroupant des groupes formés au préalable d’une manière hiérarchique jusqu’à ce que tous les sujets ne forment qu’un seul groupe. Le nombre de groupe retenu pourra être sélectionné à l’aide de certains critères que nous verrons plus tard.</p>
<p>À une étape donnée, il faut choisir quels groupes seront combinés. Les deux groupes dont la distance est la plus faible seront combinés. Il faut donc être en mesure de calculer la distance entre deux groupes. Nous allons décrire la méthode de Ward, qui compte parmi les plus populaires. Nous reviendrons brièvement sur d’autres méthodes plus loin.</p>
<div id="méthode-de-ward" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Méthode de Ward</h3>
<p>Cette méthode est basée sur un critère d’homogénéité global des groupes. Pour un groupe donné, cette homogénéité est mesurée par la somme des carrés des observations par rapport à la moyenne du groupe. L’homogénéité globale est alors la somme des homogénéités de tous les groupes. Plus l’homogénéité globale est petite, plus les groupes sont homogènes. À une étape donnée, les deux groupes qui causent la plus petite hausse de l’homogénéité globale (la plus petite perte d’information) sont regroupés. La méthode de Ward donne des groupes compacts d’apparence sphérique.</p>
<p>Plus précisément, supposons qu’à une étape du processus hiérarchique, nous avons <span class="math inline">\(M\)</span> groupes et que nous voulons passer à <span class="math inline">\(M-1\)</span> groupes. Pour un groupe <span class="math inline">\(K\)</span> (parmi <span class="math inline">\(1, 2, \ldots, M\)</span>), définissons la somme des carrés des distances par rapport à la moyenne du groupe, <span class="math inline">\(\mathsf{SCD}_k\)</span>. Plus <span class="math inline">\(\mathsf{SCD}_k\)</span> est petite, plus le groupe est compact et homogène.</p>
<p>On peut calculer cette distance pour tous les <span class="math inline">\(M\)</span> groupes et définir l’<strong>homogénéité globale</strong> comme la somme de l’homogénéité de tous les groupes,
<span class="math display">\[\begin{align*}
\mathsf{SCD}_G = \mathsf{SCD}_1 + \cdots + \mathsf{SCD}_M.
\end{align*}\]</span>
Plus l’homogénéité globale <span class="math inline">\(\mathsf{SCD}_G\)</span> est petite, mieux c’est. Pour passer de <span class="math inline">\(M\)</span> à <span class="math inline">\(M-1\)</span> groupes, la méthode de Ward va regrouper les deux groupes qui feront que <span class="math inline">\(\mathsf{SCD}_G\)</span> sera la plus petite possible.</p>
<p>On procède à une analyse simplifiée des données pour le voyage organisé avec deux variables et vingt observations afin d’être en mesure de visualiser l’algorithme de groupement. Les données se trouvent dans le fichier <code>cluster1a</code>.</p>
<p><img src="MATH60602_files/figure-html/04-plottrueclust-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Comme souvent, les données ont été simulées et le nombre de regroupements est donc connu (de même que l’appartenance des différentes observations aux regroupements). La variable qui donne l’identité du vrai groupe est <code>cluster_vrai</code>. Il est important de bien comprendre qu’en pratique, on ne connaît ni le nombre de groupes, ni quelle observation appartient à quel groupe. Ici, on se servira du fait qu’on connaît les vrais groupes pour examiner la performance de l’analyse de regroupements.</p>
<p>La première analyse utilise la méthode de Ward. Les commandes <strong>SAS</strong> se trouvent dans <code>cluster1_simplifie.sas</code>; la présentation de la procédure et de la syntaxe est différée. L’historique de regroupement est décrit dans la sortie <strong>SAS</strong>. La première colonne donne le nombre de groupes. Au départ, les observations 16 et 19 sont regroupées, il y a maintenant 19 groupes. Ensuite, les observations 11 et 13 sont regroupées, il y a maintenant 18 groupes. Au moment de passer de 14 à 13 groupes, c’est le groupe formé à l’étape 16 qui est fusionné avec l’observation 2 et ainsi de suite. La colonne <code>Fréq</code> donne le nombre d’observations dans le groupe qui vient d’être formé.</p>
<p><img src="figures/04-clustering-e3.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Les quantités <code>sprsq</code> et <code>rsq</code> sont des statistiques qui peuvent servir de guide pour choisir le nombre de groupes. Le <span class="math inline">\(\mathsf{RSQ}\)</span> est une mesure similaire au <span class="math inline">\(R^2\)</span> régression linéaire qui mesure globalement à quel point les groupes sont homogènes. Elle prend une valeur entre 0 et 1 où 0 et plus le <span class="math inline">\(\mathsf{RSQ}\)</span> est élevé, meilleur le regroupement.
On définit le <span class="math inline">\(\mathsf{RSQ}\)</span> comme la proportion de la variabilité expliquée par les groupes. C’est une version standardisée de la somme des homogénéités, <span class="math inline">\(\mathsf{SCD}_G\)</span>,
<span class="math display">\[\begin{align*}
\mathsf{RSQ} = 1-\frac{\mathsf{SCD}_G}{\mathsf{SCD}_T},
\end{align*}\]</span>
où <span class="math inline">\(\mathsf{SCD}_T\)</span> est la somme des carrés des distances par rapport à la moyenne lorsque toutes les observations sont dans un même groupe. Le graphique <a href="analyse-regroupements.html#fig:fig4-e4">3.2</a> montre l’évolution du <span class="math inline">\(\mathsf{RSQ}\)</span> en fonction du nombre de groupes.</p>
<div class="figure" style="text-align: center"><span id="fig:fig4-e4"></span>
<img src="figures/04-clustering-e4.png" alt="R carré en fonction du nombre de groupes" width="80%" />
<p class="caption">
Figure 3.2: R carré en fonction du nombre de groupes
</p>
</div>
<p>L’idée est généralement de choisir un petit nombre de groupe avec un <span class="math inline">\(\mathsf{RSQ}\)</span> assez élevé.
Ici, on voit que le <span class="math inline">\(\mathsf{RSQ}\)</span> chute brutalement en passant de trois à deux groupes (il passe de 78,2% de variabilité expliquée à 48,6%). Ainsi, choisir trois groupes semble raisonnable.</p>
<p>L’autre mesure, le <span class="math inline">\(\mathsf{SPRSQ}\)</span> ou <span class="math inline">\(R\)</span> carré semi-partiel, mesure la perte d’homogénéité résultant du fait que l’on vient de former un nouveau groupe. Comme on veut des groupes homogènes, on veut qu’elle soit petite. Plus précisément, supposons que les groupes <span class="math inline">\(k_1\)</span> et <span class="math inline">\(k_2\)</span> viennent d’être regroupés à une étape donnée. Soient <span class="math inline">\(\mathsf{SCD}_{k_1}\)</span> et <span class="math inline">\(\mathsf{SCD}_{k_2}\)</span> les homogénéités de ces deux groupes et <span class="math inline">\(\mathsf{SCD}_{k}\)</span> l’homogénéité du nouveau groupe formé en fusionnant les deux.
On définit la perte d’homogénéité (relative) en combinant ces deux groupes
<span class="math display">\[\begin{align*}
\mathsf{SPRSQ} = \frac{\mathsf{SCD}_k - \mathsf{SCD}_{k_1} - \mathsf{SCD}_{k_2}}{\mathsf{SCD}_T}
\end{align*}\]</span>
On peut ainsi tracer une courbe pour le <span class="math inline">\(\mathsf{SPRSQ}\)</span> en fonction du nombre de groupes, comme dans le graphique <a href="analyse-regroupements.html#fig:fig4-e5">3.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig4-e5"></span>
<img src="figures/04-clustering-e5.png" alt="Courbe du $R^2$ semi-partiel en fonction des groupements hiérarchiques" width="80%" />
<p class="caption">
Figure 3.3: Courbe du <span class="math inline">\(R^2\)</span> semi-partiel en fonction des groupements hiérarchiques
</p>
</div>
<p>La procédure <strong>SAS</strong> qui permet d’effectuer une analyse de regroupements hiérarchique est <code>cluster</code>. Le fichier <code>cluster2_complet.sas</code> explique les différentes options disponibles.</p>
<pre class="sas"><code>proc cluster data=temp method=ward outtree=temp1 nonorm rsquare ccc;
var x1-x6;
copy id cluster_vrai x1-x6;
ods output stat.cluster.ClusterHistory=criteres;
run;</code></pre>
<p>On peut représenter graphique le R carré (Figure <a href="analyse-regroupements.html#fig:fig4-e7">3.4</a>), le Rcarré semi partiel (Figure <a href="analyse-regroupements.html#fig:fig4-e8">3.5</a>) et le critère de classification cubique (Figure <a href="analyse-regroupements.html#fig:fig4-e9">3.6</a>) en fonction du nombre de groupes. Ce derniers est plus technique mais son interprétation est simple, plus sa valeur est élevée, mieux c’est. Par contre, il peut avoir plusieurs maximums locaux et, de ce fait, ce n’est donc pas toujours évident de choisir le nombre de groupes avec ce critère. Il semble encore une fois que choisir trois groupes semble raisonnable.</p>
<div class="figure" style="text-align: center"><span id="fig:fig4-e7"></span>
<img src="figures/04-clustering-e8.png" alt="R carré en fonction des regroupements hiérarchiques" width="80%" />
<p class="caption">
Figure 3.4: R carré en fonction des regroupements hiérarchiques
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fig4-e8"></span>
<img src="figures/04-clustering-e9.png" alt="R carré semi-partiel en fonction des regroupements hiérarchiques" width="80%" />
<p class="caption">
Figure 3.5: R carré semi-partiel en fonction des regroupements hiérarchiques
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fig4-e9"></span>
<img src="figures/04-clustering-e10.png" alt="Critère de classification cubique en fonction des regroupements hiérarchiques" width="80%" />
<p class="caption">
Figure 3.6: Critère de classification cubique en fonction des regroupements hiérarchiques
</p>
</div>
<p>Parfois, l’information est présentée sous forme de dendogramme, qui trace l’arbre et la fusion des groupes. On peut ainsi retracer l’historique de la procédure hiérarchique. Celui produit par <strong>SAS</strong> donne, à un facteur multiplicatif près, le <span class="math inline">\(\mathsf{SPRSQ}\)</span>. Il n’y a donc pas de nouvelles informations ici. On voit que c’est lorsqu’on passe de trois à deux groupes, qu’il y a une bonne perte d’homogénéité.</p>
<p>En pratique, on ne peut jamais savoir si on a bel et bien regroupé ensemble les bons sujets. Mais ici, comme il s’agit de données artificielles qui ont été générées, nous connaissons la composition des vrais groupes. Il s’avère qu’il y en a effectivement trois. De plus, la solution à trois groupes obtenue avec la méthode de Ward a bien réussi à retrouver les groupes car il y a 146 sujets sur 150 (97,3%) qui sont bien regroupés, et quatre qui font partie d’un mauvais groupe. Ceci est un exemple facile où les observations sont bien séparées. Ce n’est pas toujours aussi simple en pratique.</p>
<p><strong>Interprétation des groupes</strong>: la méthode la plus simple consiste à inspecter les moyennes des variables de chaque groupe et de voir s’il en découle une interprétation raisonnable. La procédure <code>tree</code> permet d’extraire la solution avec un nombre spécifié de groupes et il est ensuite facile (avec la procédure <code>means</code>) d’obtenir ces moyennes (voir le fichier <code>cluster2_complet.sas</code>).</p>
<p><img src="figures/04-clustering-e11.png" width="55%" style="display: block; margin: auto;" /></p>
<p><img src="figures/04-clustering-e12.png" width="55%" style="display: block; margin: auto;" /></p>
<p><img src="figures/04-clustering-e13.png" width="55%" style="display: block; margin: auto;" /></p>
<p>Le groupe 1 est le groupe où les sujets ont les valeurs, en moyenne, les plus faibles pour les six variables. Le groupe 2 est celui où les sujets ont les valeurs, en moyenne, les plus élevées pour les 6 variables sauf pour la variable <span class="math inline">\(X_1\)</span> (activité sociale). Le groupe 3 est celui où les sujets ont, en moyenne, la valeur la plus élevée de la variable <span class="math inline">\(X_1\)</span> et des valeurs moyennes inférieures au groupe 3 mais supérieures au groupe 2 pour les cinq autres variables.</p>
<p>Dans l’article, les auteurs ont baptisé les sujets du groupe 1, les « indépendants », ceux du groupe 2, les « dépendants » et ceux du groupe 3, les « sociables ».
Notez qu’on <strong>ne peut pas</strong> tester l’égalité des moyennes des variables pour les différents groupes avec une ANOVA; la sélection des groupes est faite à l’aide d’un algorithme glouton pour maximiser la distance entre les groupes, aussi cela invalide l’inférence. On peut aussi explorer les groupes en modélisant les effets des variables en ce qui a trait à l’appartenance aux groupes. Traditionnellement, l’analyse discriminante est utilisée à cette fin. Il est aussi possible d’utiliser un arbre de classification ou une autre méthode prévisionnelle, telle la régression multinomiale logistique. La variable identifiant le groupe d’appartenance obtenu avec l’analyse de regroupement sert alors de variable dépendante <span class="math inline">\(Y\)</span>. Ce type d’analyse permet de creuser un peu plus pour essayer de comprendre la structure des groupes formés.</p>
</div>
</div>
<div id="calcul-alternatif-des-distances-pour-le-regroupement-hiérarchique" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Calcul alternatif des distances pour le regroupement hiérarchique</h2>
<p>Nous avons utilisé la méthode de Ward afin de calculer la distance entre les groupes et procéder au passage de <span class="math inline">\(n\)</span> groupes à un groupe, avec l’approche hiérarchique. Supposons que nous avons choisi une mesure de dissemblance <span class="math inline">\(d(S_i, S_j)\)</span> quelconque (distance euclidienne par exemple) pour mesurer la distance entre deux sujets. D’autres méthodes sont disponibles dans la procédure <code>cluster</code>, et les regroupements utilisent
Voici comment sont choisis les regroupements avec ces méthodes.</p>
<ul>
<li>Méthode du plus proche voisin ou méthode de liaison simple (<em>nearest neighbor</em>, <em>single linkage</em>): utilise la distance minimale entre chaque paire de sujets (un pour chaque groupe) provenant des deux groupes. Cette méthode fonctionne bien si l’écart entre deux regroupements est suffisamment grand. À l’inverse, s’il y a des observations bruitées entre deux regroupements, la qualité des regroupements en sera affectée.</li>
<li>Méthode du voisin le plus éloigné ou méthode de liaison complète (<em>complete linkage</em>): utilise la distance maximale entre toutes les paires de sujets (un pour chaque groupe) provenant des deux groupes. Cette méthode est moins sensible au bruit et aux faibles écarts entre regroupements, mais a tendance à casser les regroupements globulaires.</li>
<li>Méthode de liaison moyenne (<em>average linkage</em>): utilise la moyenne des distances entre toutes les paires de sujets (un pour chaque groupe) provenant des deux groupes.</li>
<li>Méthode du barycentre (<em>centroid</em>): utilise la distance entre les représentants moyens de chaque groupe où le représentant moyen d’un groupe est le barycentre, soit la moyenne variable par variable, des sujets formant le groupe.</li>
</ul>
<p>Le fichier <code>cluster3_voisin_eloigne.sas</code> contient les commandes pour utiliser la méthode du voisin le plus éloigné (avec l’option <code>method=complete</code>). Le graphe plus bas donne cette distance pour les deux groupes qui viennent d’être fusionnés. Il s’agit donc du maximum des distances entre chaque paire de sujets (un pour chaque groupe) provenant des deux groupes fusionnés.</p>
<div class="figure" style="text-align: center"><span id="fig:fig4-e21"></span>
<img src="figures/04-clustering-e21.png" alt="Distance maximale entre groupes en fonction des regroupements hiérarchiques pour la méthode du voisin le plus éloigné." width="80%" />
<p class="caption">
Figure 3.7: Distance maximale entre groupes en fonction des regroupements hiérarchiques pour la méthode du voisin le plus éloigné.
</p>
</div>
<p>Comme on veut que cette distance soit petite pour les groupes fusionnés, on pourrait être tenté d’arrêter à trois groupes ici sur la base de la Figure <a href="analyse-regroupements.html#fig:fig4-e20">3.8</a>. L’interprétation des groupes ne change pas comparativement aux analyses précédentes. La taille des groupes, (44, 71, 35) change un peu par rapport à la solution avec la méthode de Ward qui donnait des tailles de (43, 75, 32). En fait, avec la méthode du voisin le plus éloigné, 141 des 150 sujets sont dans le bon groupe (comparativement à 146 avec la méthode de Ward). Elle fait donc un peu moins bien pour cet exemple.</p>
<p>On peut comparer les performances des regroupements hiérarchiques selon la méthode de groupement. La <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html">page web de scikit-learn developers</a> montre la performance sur des exemples jouets, qui montre que selon les hypothèses et la structure, aucune ne performe mieux que les autres dans tous les exemples.</p>
<div class="figure" style="text-align: center"><span id="fig:fig4-e20"></span>
<img src="figures/04-clustering-e20.png" alt="Comparaison des méthodes de groupement sur des données test." width="80%" />
<p class="caption">
Figure 3.8: Comparaison des méthodes de groupement sur des données test.
</p>
</div>
</div>
<div id="standardisation-des-variables" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Standardisation des variables</h2>
<p>Dans les exemples précédents, nous avons utilisé les variables <span class="math inline">\(X_1\)</span> à <span class="math inline">\(X_6\)</span> telles quelles. En général, plus une variable a une grande variance, plus elle aura de l’influence sur le calcul de la distance euclidienne. Ainsi, en utilisant les variables telles quelles, nous accordons plus de poids aux variables avec de grandes variances, ce qui peut être bon ou mauvais selon la structure des groupes. Règle générale, il est préférable d’éviter qu’une variable domine dans la segmentation.</p>
<p>Avec la procédure <code>stdize</code>, on peut standardiser au préalable les variables avant de faire l’analyse. Par défaut, les variables seront standardisées afin d’avoir une moyenne de zéro et une variance de un. On peut ensuite faire les analyses comme précédemment. Le fichier <code>cluster4_standardisation.sas</code> contient les commandes pour standardiser les variables et ensuite refaire l’analyse de regroupements avec la méthode de Ward. Notez que les six variables ont des variances semblables, donc les résultats ne devraient donc pas être trop affectés en standardisant les variables. Il s’avère effectivement que les résultats changent très peu si on standardise les variables. Les tailles des groupes de la solution à trois groupes sont (44, 75, 31) (comparativement à (43, 75, 32) sans la standardisation). Il s’avère que 147 des 150 sujets sont bien regroupés (comparativement à 146 sans la standardisation). Il s’agit d’une très légère amélioration.</p>
</div>
<div id="autres-mesures-de-dissemblance" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Autres mesures de dissemblance</h2>
<p>Nous avons déjà mentionné que lorsque toutes les variables sont continues, la mesure de dissemblance la plus utilisée est la distance euclidienne. Dans la procédure <code>cluster</code>, <strong>SAS</strong> utilise la distance euclidienne au carré par défaut. Pour utiliser la distance euclidienne elle-même, il faut mettre le mot clé <code>nosquare</code> dans les options suivant l’appel à <code>proc cluster</code> (première ligne). Pour utiliser une mesure de dissemblance autre que la distance euclidienne (au carré ou pas), on peut utiliser la procédure <code>distance</code> au préalable pour calculer les distances, et ensuite fournir la matrice des distances directement à <code>cluster</code> en lieu et place des observations.</p>
<p>Il existe un très grand nombre d’autres mesures de dissemblance pour variables quantitatives, ordinales, nominales et binaires. Voici une brève description de certaines d’entre elles, qui sont disponibles dans <code>proc distance</code>.</p>
<p>Mesures de dissemblance pour variables quantitatives :</p>
<ol style="list-style-type: decimal">
<li>Distance euclidienne ou distance euclidienne au carré</li>
<li>Distance de Manhattan, ou taxi-distance:
<span class="math display">\[\begin{align*}
d(S_i, S_j) = |X_{i1}-X_{j1}| + \cdots + |X_{ip}- X_{jp}|
\end{align*}\]</span></li>
</ol>
<p>Mesure de dissemblance pour variables nominales:</p>
<p>Le plus simple est d’utiliser la mesure d’appariement simple (<em>simple matching</em>). Cette mesure est simplement de la proportion des variables pour lesquelles les deux sujets ont des valeurs différentes.</p>
<p>Mesures de dissemblances pour variables ordinales:</p>
<ol style="list-style-type: decimal">
<li>Une manière simple consiste à les traiter comme des variables continues, et utiliser la distance euclidienne ou la distance de Manhattan. On peut aussi les standardiser au préalable si elles ne sont pas sur la même échelle (par exemple, si certaines vont de 1 à 5 et d’autres de 1 à 7).</li>
<li>On peut aussi les traiter comme des variables nominales avec la mesure d’appariement simple; ce faisant, on n’utilise pas l’ordre entre les modalités.</li>
</ol>
<p>Le fichier <code>cluster4_cityblock.sas</code> contient les commandes pour refaire l’analyse des regroupements du voyage organisé avec la taxi-distance et la méthode de liaison moyenne (<code>method=average</code>) dans <code>proc cluster</code>.</p>
<p>Encore une fois, l’interprétation des groupes ne change pas comparativement aux analyses précédentes. La taille des groupes, (45, 75, 30) change un peu par rapport à la solution avec la méthode de Ward qui donnait des tailles de (43, 75, 32). Avec la distance de Manhattan et la méthode de liaison moyenne, 148 des 150 sujets sont dans le bon groupe (comparativement à 146 avec la méthode de Ward). Cette combinaison fait donc mieux pour cet exemple.</p>
<p>Règle générale, les différentes étapes des méthodes agglomératives hiérarchiques nécessitent <span class="math inline">\(\mathrm{O}(n^3)\)</span> opérations, bien qu’une version plus parsimonieuse existe avec complexité <span class="math inline">\(\mathrm{O}(n^2\ln n)\)</span> ou <span class="math inline">\(\mathrm{O}(n^2)\)</span> pour les méthodes de liaison simple et complexe. La formule de Lance–Williams permet de mettre à jour récursivement les distances entre regroupements pour la plupart des méthodes considérées. Le coût élevé de la méthode de regroupement hiérarchique, qui dépend de la taille de l’échantillon, devient prohibitif avec des mégadonnées. Il nécessite aussi le calcul d’une mesure de dissemblance et l’évaluation de la qualité de l’agglomération, autre que graphique, n’est pas évidente. Ces méthodes sont largement discontinuées de nos jours par des alternatives modernes (regroupement spectraux, mélanges de modèles, <span class="math inline">\(K\)</span>-médoïdes par itérations de Voronoï) qui ne sont pas implémentées en <strong>SAS</strong>.</p>
</div>
<div id="méthodes-non-hiérarchiques" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Méthodes non hiérarchiques</h2>
<p>Contrairement aux méthodes hiérarchiques, il faut spécifier le nombre de groupe désiré dès le départ pour les méthodes non hiérarchiques. La méthode des <span class="math inline">\(K\)</span> moyennes (<span class="math inline">\(K\)</span> <em>means</em>) sera la seule décrite ici. Cette méthode utilise la distance euclidienne et est donc seulement applicable avec des variables quantitatives. Supposons que l’on veuille <span class="math inline">\(K\)</span> groupes. La méthode des <span class="math inline">\(K\)</span> moyennes peut être décrite en trois étapes :</p>
<ol style="list-style-type: decimal">
<li>On sélectionne <span class="math inline">\(K\)</span> germes (<em>seeds</em>) qui vont agir comme centres préliminaires des groupes.
<ol style="list-style-type: lower-roman">
<li>Ces germes peuvent être les centres des groupes obtenus à partir d’une autre méthode comme une méthode hiérarchique.</li>
<li>Ces germes peuvent être choisis à même l’ensemble de données. Par exemple, on peut choisir <span class="math inline">\(K\)</span> sujets au hasard.</li>
</ol></li>
<li>On assigne dans l’ordre les observations au groupe le plus proche en utilisant la distance euclidienne par rapport au germe. Soit on assigne toutes les observations avant de mettre à jour les germes, soit on met à jour les germes après chaque assignation d’un sujet. Le nouveau germe d’un groupe est la moyenne des observations faisant partie du groupe.</li>
<li>On peut répéter le processus jusqu’à ce que les changements des germes des groupes deviennent négligeables ou nuls. Les groupes finaux sont formés en assignant les sujets au groupe le plus proche.</li>
</ol>
<p>À chaque étape, l’algorithme des <span class="math inline">\(K\)</span>-moyennes minimise le critère
<span class="math display">\[\begin{align*}
\sum_{i=1}^n \sum_{k=1}^K \mathrm{I}(x_i \in g_k)\|\boldsymbol{x}_i - \boldsymbol{\mu}_k\|^2 = \sum_{i=1}^n \sum_{k=1}^K \mathrm{I}(x_i \in g_k) \sum_{j=1}^p (x_{ij}-\mu_{kj})^2
\end{align*}\]</span>
où <span class="math inline">\(\mathrm{I}(\boldsymbol{x}_i \in g_k)\)</span> est une variable binaire qui vaut un si l’observation <span class="math inline">\(i\)</span> appartient au groupe <span class="math inline">\(g_k\)</span> et <span class="math inline">\(\boldsymbol{\mu}_k\)</span> est le centroïde du groupement <span class="math inline">\(k\)</span>. On minimise ainsi la variance intra-groupe. On voit ainsi dans la Figure <a href="analyse-regroupements.html#fig:kmoyperfo">3.9</a> que l’algorithme détecte bien les vrais regroupements si les groupes sont sphériques et de même variance, et que les regroupements sont bien séparés (c’est-à-dire, quand la distance euclidienne entre les regroupements est élevée).</p>
<div class="figure" style="text-align: center"><span id="fig:kmoyperfo"></span>
<img src="MATH60602_files/figure-html/kmoyperfo-1.png" alt="Performance de l'algorithme des $K$-moyennes en fonction de différents scénarios. En haut à gauche: nombre incorrect de classe et données normales de même variance, bien séparées. En haut à droite: spirales: les $K$-moyennes ignorent la topologie des regroupements, et ne segmente pas adéquatement les regroupements connectés. En bas à gauche: données elliptiques de même variance, mais fortement corrélées. Comme le critère minimise la distance intra-groupe sans pondération, les points regroupés appartiennent à différentes classes. En bas à droite: données sphériques de variances différentes. L'algorithme des $K$-moyennes réussit une bonne segmentation si les groupements sont compacts et bien séparés." width="70%" />
<p class="caption">
Figure 3.9: Performance de l’algorithme des <span class="math inline">\(K\)</span>-moyennes en fonction de différents scénarios. En haut à gauche: nombre incorrect de classe et données normales de même variance, bien séparées. En haut à droite: spirales: les <span class="math inline">\(K\)</span>-moyennes ignorent la topologie des regroupements, et ne segmente pas adéquatement les regroupements connectés. En bas à gauche: données elliptiques de même variance, mais fortement corrélées. Comme le critère minimise la distance intra-groupe sans pondération, les points regroupés appartiennent à différentes classes. En bas à droite: données sphériques de variances différentes. L’algorithme des <span class="math inline">\(K\)</span>-moyennes réussit une bonne segmentation si les groupements sont compacts et bien séparés.
</p>
</div>
<p>Nous allons utiliser cette procédure pour raffiner la solution obtenue précédemment avec la méthode de Ward en utilisant les moyennes des groupes comme centres préliminaires. Le fichier <code>cluster5_non-hierarchique.sas</code> explique les différentes options. La syntaxe de la procédure <strong>SAS</strong> <code>fastclus</code> est la suivante:</p>
<pre class="sas"><code>proc fastclus data=temp seed=initial distance maxclusters=3 out=temp3 maxiter=30;
var x1 x2 x3 x4 x5 x6;
run;</code></pre>
<p>Voici une partie de la sortie <strong>SAS</strong>:</p>
<p><img src="figures/04-clustering-e14.png" width="70%" style="display: block; margin: auto;" /></p>
<p><img src="figures/04-clustering-e15.png" width="90%" style="display: block; margin: auto;" /></p>
<p><img src="figures/04-clustering-e16.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Évidemment, comme la solution obtenue avec la méthode de Ward est déjà excellente (146 observations de bien regroupées sur 150), on ne pourra pas avoir une amélioration notable. Il y a peu de changements par rapport à la solution de la méthode de Ward. Les tailles des groupes étaient de (43, 75, 32) avant. Elles sont maintenant (45, 77, 28). Le R carré passe de 65,7% (avec Ward) à 66.2%.</p>
<p>L’interprétation des groupes est la même que précédemment.</p>
<p><img src="figures/04-clustering-e17.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Il y a maintenant 148 sujets sur 150 (98,7%) qui sont bien regroupés et 2 qui font partie d’un mauvais groupe. C’est une légère amélioration par rapport à la solution avec la méthode de Ward pour laquelle il y avait 146 sujets sur 150 qui étaient bien regroupés. Mais encore une fois, en pratique on n’aura pas accès à cette information.</p>
<p>Le champ des applications des <span class="math inline">\(K\)</span>-moyennes est parfois surprenant. On peut considérer, par exemple, la compression d’images. La Figure <a href="analyse-regroupements.html#fig:decelles">3.10</a> montre une image du bâtiment Decelles (coin supérieur gauche) et la reconstruction avec trois, quatre et 10 couleurs obtenues en appliquant l’algorithme des <span class="math inline">\(K\)</span>-moyennes sur la matrice formée par les valeurs des canaux (rouge, vert, bleu) de l’image.</p>
<div class="figure" style="text-align: center"><span id="fig:decelles"></span>
<img src="figures/kmoyennes_decelles.png" alt="Compression d'image avec l'algorithme des $K$-moyennes: image originale (en haut à gauche), compression avec trois (en haut à droite), quatre (en bas à gauche) et 10 (en bas à droite) couleurs." width="80%" />
<p class="caption">
Figure 3.10: Compression d’image avec l’algorithme des <span class="math inline">\(K\)</span>-moyennes: image originale (en haut à gauche), compression avec trois (en haut à droite), quatre (en bas à gauche) et 10 (en bas à droite) couleurs.
</p>
</div>
</div>
<div id="considérations-pratiques" class="section level2" number="3.9">
<h2><span class="header-section-number">3.9</span> Considérations pratiques</h2>
<p>Il peut être intéressant de comparer les résultats provenant d’une même méthode avec des nombres différents de groupes et aussi comparer ceux provenant de plusieurs méthodes (voir plus loin pour la description de certaines autres méthodes). Le choix de la méthode et du nombre de groupe n’est pas facile et devrait être basé sur des considérations pratiques et d’interprétation (comme en analyse factorielle). Il n’est pas rare qu’on obtienne des résultats très différents d’une méthode à l’autre pour un même ensemble de données.</p>
<p>Avec une méthode non hiérarchique, il est préférable de fournir des germes de départ « raisonnablement bon » (provenant d’une méthode hiérarchique par exemple) plutôt que de laisser l’algorithme les choisir au hasard.</p>
<p>Le choix des variables est important. En général on veut créer des groupes d’individus qui sont homogènes par rapport à un certain aspect de leur comportement ou de leur situation. On ne doit alors inclure que les variables pertinentes à cet aspect. Par exemple, si le but de l’analyse est de segmenter nos clients selon leurs habitudes de consommation (genre de boutiques fréquenté, fréquence, etc.), on ne voudra peut-être pas inclure des variables démographiques. En fait, souvent l’analyse de regroupements servira justement à créer des groupes qui seront comparés par rapport à d’autres variables qui n’ont pas été utilisées pour créer les groupes. Dans notre exemple sur les voyages organisés, on a segmenté les voyageurs en trois groupes (indépendants, dépendants et sociables). Les auteurs de l’article (voir page 369 de l’article) ont comparé les trois groupes selon l’expérience de voyage, la taille de la communauté où ils habitent (avec des ANOVA), selon leur âge, leur revenu et leur éducation (avec des tests d’indépendance du khi-deux). Notez que ces tests sont réalisés sur des variables qui ne sont pas utilisées lors de la segmentation.</p>
<p>Le problème majeur avec l’analyse de regroupements est qu’il n’y a pas de façon claire de quantifier la performance de notre analyse. Lorsqu’on développe un modèle de prédiction (régression linéaire ou logistique par exemple), on peut estimer la performance de notre modèle d’une manière objective à l’aide de l’erreur quadratique de généralisation (régression linéaire) ou du taux de bonne classification (régression logistique). Ces quantités peuvent être estimées d’une manière objective en utilisant une méthode telle la validation croisée ou la division de l’échantillon. On ne peut faire de même avec l’analyse de regroupements car on n’a pas de variable réponse à prédire. Tout comme pour l’analyse factorielle, les connaissances à priori, le jugement, et les considérations pratiques font partie d’une analyse de regroupements.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="analyse-factorielle.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="selection-modele.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH60602.pdf"],
"toc": {
"collapse": "section",
"split_by": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
