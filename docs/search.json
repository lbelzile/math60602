[
  {
    "objectID": "03-regroupements.html",
    "href": "03-regroupements.html",
    "title": "4  Analyse de regroupements",
    "section": "",
    "text": "On cherche à créer des regroupements (clusters) d’individus homogènes en utilisant \\(p\\) variables \\(X_1, \\ldots, X_p\\). Plus précisément, on veut combiner des sujets en groupes (interprétables) de telle sorte que les individus d’un même groupe soient le plus semblables possible par rapport à certaines caractéristiques et que les groupes soient le plus différent possible.\nNous disposons des observations pour \\(n\\) individus et \\(X_{ij}\\) dénote la valeur de la \\(j\\)e variable explicative pour le \\(i\\)e sujet: les variables correspondant au \\(i\\)e sujet \\(S_i\\) sont donc \\(X_{i1}, \\ldots, X_{ip}\\). Le résultat de l’analyse de regroupements sera une étiquette associée à chaque observation l’assignant à un regroupement ou l’identifiant comme aberrance.\nIl y a une certaine analogie avec l’analyse factorielle. En analyse factorielle, on cherche à déterminer s’il y a des groupes de variables corrélées entre elles. On cherche donc à regrouper des variables. En analyse de regroupements, on cherche plutôt à créer des groupes de sujets similaires. Les deux méthodes servent pour l’analyse exploratoire.\n\n\n\n\n\n\nÉtapes d’une analyse de regroupements\n\n\n\n\nChoisir les variables pertinentes à l’analyse.\nDécider comment seront mesurées les « distances » entre les sujets.\nDécider quel algorithme ou modèle sera utilisé.\nChoisir les hyperparamètres de l’algorithme (nombre de regroupements, rayon, etc.), qui déterminent la qualité de la segmentation.\nAssigner les étiquettes aux observations et calculer un représentant-type de groupe. Interpréter les regroupements obtenus.\nUtiliser ces groupes dans d’autres analyses, le cas échéant."
  },
  {
    "objectID": "03-regroupements.html#présentation-des-données",
    "href": "03-regroupements.html#présentation-des-données",
    "title": "4  Analyse de regroupements",
    "section": "4.2 Présentation des données",
    "text": "4.2 Présentation des données\nLes données sont inspirées d’un article traitant de la segmentation de seniors qui participent à des voyages organisés:\n\nHsu, C. H. C. et Lee E.-J. (2002). Segmentation of Senior Motorcoach Travelers. Journal of Travel Research, 40, 364-373.\n\nLes buts principaux de l’étude étaient\n\nRegrouper les gens de 55 et plus qui participent à des voyages organisés en autobus en groupes homogènes selon des caractéristiques reliées au choix de l’opérateur et du voyage.\nExaminer les caractéristiques démographiques de ces groupes.\n\nUn questionnaire a été élaboré afin d’évaluer l’importance de 55 caractéristiques des opérateurs de voyages organisés en autobus et des voyages eux-mêmes à l’aide d’une échelle de Likert à cinq points, allant de extrêmement important (\\(\\texttt{5}\\)) à pas important du tout (\\(\\texttt{1}\\)). Des données sont disponibles pour 150 sujets (il y en avait 817 dans l’article). Elles se trouvent dans la base de données cluster.\nAu lieu de faire une analyse de regroupements avec les 55 items du questionnaire, les auteurs ont choisi de faire une analyse factorielle avec rotation varimax au préalable afin de réduire le nombre de variables à six facteurs interprétables :\n\nActivités sociales (\\(X_1\\)): formé de cinq items\nPolitiques de l’opérateur et références (\\(X_2\\)) : formé de six items.\nHoraires flexibles (\\(X_3\\)): formé de trois items.\nSanté et sécurité (\\(X_4\\)) : formé de quatre items.\nMatériel publicitaire (\\(X_5\\)): formé de deux items.\nRéputation (\\(X_6\\)): formé de deux items.\n\nOn voit donc que 22 items, parmi les 55, sont utilisés dans la définition de ces six facteurs. Dans l’article, les auteurs ont décidé d’inclure ces 22 items dans l’analyse de regroupements. Pour notre part et afin de simplifier l’exemple, nous allons plutôt créer six nouvelles échelles en faisant la moyenne des items de chaque facteur ci-haut et utiliser seulement ces six échelles dans l’analyse de regroupements. Les valeurs de ces six variables pour les 150 sujets se trouvent dans le fichier cluster.sas7bdat et sont toutes dans l’intervalle [1, 5], puisqu’elles représentent la moyenne d’échelles de Likert. Le Tableau 4.1 présente les statistiques descriptives de la base de données.\n\n\n\nTableau 4.1:  Statistiques descriptives des six variables du jeu de données cluster. \n \n  \n    moyenne \n    écart-type \n    min \n    max \n    histogramme \n  \n \n\n  \n    2.93 \n    1.14 \n    1 \n    5 \n    ▆▅▇▆▇▅▃▅ \n  \n  \n    3.22 \n    1.16 \n    1 \n    5 \n    ▅▃▅▇▅▇▆▆ \n  \n  \n    2.96 \n    1.22 \n    1 \n    5 \n    ▇▅▆▆▆▇▃▆ \n  \n  \n    3.20 \n    1.05 \n    1 \n    5 \n    ▃▂▅▇▅▇▅▅ \n  \n  \n    3.32 \n    1.18 \n    1 \n    5 \n    ▃▅▅▂▇▆▆▇ \n  \n  \n    3.17 \n    1.36 \n    1 \n    5 \n    ▇▃▅▃▅▅▆▇"
  },
  {
    "objectID": "03-regroupements.html#analyse-exploratoire-graphique",
    "href": "03-regroupements.html#analyse-exploratoire-graphique",
    "title": "4  Analyse de regroupements",
    "section": "4.3 Analyse exploratoire graphique",
    "text": "4.3 Analyse exploratoire graphique\nComme c’est le cas avec n’importe quelle analyse statistique, il est utile d’explorer les données graphiquement. On peut parfois réussir à visualiser les groupes d’observations, ce qui nous permettra une fois l’analyse de regroupement complétée de vérifier la qualité de cette dernière.\nOn pourrait produire un nuage de points pour chaque paire de variables mais cette idée est problématique pour deux raisons:\n\nil y aura beaucoup de graphes si le nombre de variables est grand et\non examine seulement les relations bivariées.\n\n\n\n\n\n\nFigure 4.1: Nuage de point des paires d’observations des données cluster.\n\n\n\n\nIl n’est pas nécessairement évident de détecter des groupes d’observations ainsi, alors qu’on n’a déjà que six variables.\nOn peut utiliser les outils du chapitre précédent et réduire le nombre de variables en considérant plutôt les composantes principales. Dans notre exemple, on va seulement s’en servir comme outil graphique pour une analyse de regroupements en réduisant la dimension afin de permettre la visualisation des regroupements obtenus. Ces dernières pourraient aussi servir de variables pour l’analyse de regroupement: une fois les étiquettes obtenues, il suffirait de calculer les statistiques descriptives sur les variables originales.\n\n\n\n\n\nFigure 4.2: Diagramme d’éboulis de la matrice de corrélation des données cluster.\n\n\n\n\nLa proportion de la variance totale qui est expliquée par les deux premières composantes principales équivaut 76.7% de la variance totale originale. On ne retient que ces deux premières.\nMême en ne connaissant pas l’appartenance des observations au regroupement, on distingue environ trois groupes. Le panneau droit du graphique Figure 4.3 montre les deux composantes principales, mais avec l’identification des groupes obtenus suite à l’analyse de regroupement avec la méthode des \\(K\\)-moyennes couverte plus tard.\n\n\n\n\n\nFigure 4.3: Projection des observations sur les composantes principales avec les regroupements finaux créés à la fin du chapitre avec la méthode des \\(K\\)-moyennes."
  },
  {
    "objectID": "03-regroupements.html#choix-des-variables",
    "href": "03-regroupements.html#choix-des-variables",
    "title": "4  Analyse de regroupements",
    "section": "4.4 Choix des variables",
    "text": "4.4 Choix des variables\nLes algorithmes de segmentation utilisent une matrice de données et se base sur la distance entre observations. L’analyste est libre de choisir quelles variables seront incluses dans le modèle. Le choix des variables est important: en général on veut créer des groupes d’individus qui sont homogènes par rapport à certains aspects de leur comportement ou de leur situation. On ne doit alors inclure que les variables pertinentes à cet aspect. Inclure de nombreuses variables pour lesquelles il y a un fort similitude entre individus contribue également à diluer les différences.\nPar exemple, si le but de l’analyse est de segmenter nos clients selon leurs habitudes de consommation (genre de boutiques fréquenté, fréquence, etc.), on n’inclura pas des variables démographiques. En fait, souvent l’analyse de regroupements servira justement à créer des groupes qui seront comparés par rapport à d’autres variables qui n’ont pas été utilisées pour créer les groupes.\nLa compréhension de la base de données est cruciale pour comprendre le comportement. Par exemple, si on essaie de faire une segmentation du comportement d’utilisateurs et utilisatrices de transports en commun à partir d’informations auxiliaires comme le temps de passage, le nombre de correspondance et la fréquence d’utilisation, il peut être utile de créer de nouvelles variables (par exemple, une variable indicatrice qui indique si la personne voyage durant les heures de traffic entre 7h30 et 9h et 16h à 18h), si elle voyage cinq jours semaines, etc. L’inclusion des ces variables auxiliaires peut augmenter la qualité de la segmentation.\nPour voir si certaines variables sont inutiles, il peut être utile de comparer les représentants des groupes (par exemple, le barycentre ou une observation lambda du groupe) pour voir si les moyennes ou caractéristiques diffèrent. Si ce n’est pas le cas, on pourrait envisager de recommencer la procédure en enlevant cette variable.\nSi on a un nombre important de variables explicatives à disposition, il est parfois utile de réduire la dimension (par exemple, en effectuant une analyse en composantes principales) préalablement et à ne retenir que les premières composantes pour faciliter la tâche. Cette approche n’est pas la panacée: quelquefois, cette réduction de la dimension masque les différences entre groupe et mène à une segmentation inférieure à l’utilisation des variables originales.\nMalheureusement, il n’est pas évident de prime abord de déterminer quelles variables inclure dans la base de données pas plus qu’il n’est facile de juger de la qualité d’une segmentation ou du nombre de regroupements à effectuer. Les choix individuels auront un impact certains sur les regroupements obtenus: on recommande d’essayer plusieurs alternatives et de vérifier graphiquement ou à l’aide de critères d’ajustement si les regroupements obtenus sont homogènes, si la distance inter-groupe est élevée et la distance intra-groupe faible (groupements compacts).\n\n4.4.1 Standardisation\nEn général, plus une variable a une grande variance, plus elle aura de l’influence sur le calcul de la distance. Ainsi, en utilisant les variables telles quelles, nous accordons plus de poids aux variables avec de grandes variances, ce qui peut être bon ou mauvais selon la structure des groupes. Règle générale, il est préférable d’éviter qu’une variable domine dans la segmentation.\nOn peut standardiser au préalable les variables avant de faire l’analyse. Par défaut, les variables seront standardisées afin d’avoir une moyenne de zéro et une variance de un (scale). On peut ensuite faire les analyses comme précédemment. Si on a des valeurs aberrantes, cela peut impacter le calcul des moyennes et variances; d’autres estimateurs de localisation et d’échelles plus robustes, par exemple en soustrayant la médiane de chaque colonne et en divisant par la déviation absolue par rapport à la médiane (mad). Ces deux mesures pourraient être utilisées lors de la standardisation pour diminuer l’impact des valeurs aberrantes même si le coût de calcul associé est plus conséquent.\n\n\nCode\ndata(cluster, package = \"hecmulti\")\n# Standardisation usuelle \n# (soustraire moyenne, diviser par écart-type)\ncluster_std <- scale(cluster)\n# Standardisation robuste\ncluster_std_rob <- apply(cluster, \n                         MARGIN = 2, \n                         function(x){\n                           (x - median(x))/mad(x)})\n# apply permet d'appliquer une fonction\n# par ligne, colonne ou cellule\n# MARGIN = 2 indique colonne \n# (on centre chaque colonne tour à tour)"
  },
  {
    "objectID": "03-regroupements.html#mesures-de-dissemblance-et-de-similarité",
    "href": "03-regroupements.html#mesures-de-dissemblance-et-de-similarité",
    "title": "4  Analyse de regroupements",
    "section": "4.5 Mesures de dissemblance et de similarité",
    "text": "4.5 Mesures de dissemblance et de similarité\nComment mesurer si deux observations appartiennent à un même regroupement et sont similaires? Idéalement, on aimerait avoir une situation comme dans la Figure 4.4 où les regroupements sont clairement visibles. On aimerait que la similarité entre observations intra-groupe soit élevée et que la similarité entre groupe soit faible. Les regroupements devraient être éloignés les uns des autres, tandis que les observations au sein de ces regroupements devraient être proches. Dans la plupart des cas, il y aura des observations isolées qui n’appartiennent pas nécessairement logiquement à l’un ou l’autre des groupes. On appelle parfois ces observations aberrances.\n\n\n\n\n\nFigure 4.4: Données simulées avec deux regroupements hypothétiques.\n\n\n\n\n\n4.5.1 Mesure de dissemblance\nUne mesure de dissemblance sert à quantifier la distance entre deux sujets \\(S_i\\) et \\(S_j\\) en se basant sur les \\(p\\) variables \\(X_1, \\ldots, X_p\\). Plus cette mesure est petite, plus les sujets \\(S_i\\) et \\(S_j\\) sont similaires. Même s’il y a des exceptions, la plupart des mesures de dissemblances \\(d\\) ont les propriétés suivantes :\n\n\\(d(S_i, S_j) \\geq 0\\) (positivité);\n\\(d(S_i, S_i)=0\\);\n\\(d(S_i, S_j)=d(S_j, S_i)\\) (symmétrie);\n\\(d(S_i, S_j)\\) augmente au fur et à mesure que les deux sujets deviennent plus différents.\n\nLorsque toutes les variables sont continues, la mesure de dissemblance la plus utilisée est la distance euclidienne entre sujets, soit \\[\\begin{align*}\nd(S_i, S_j; l_2) = \\sqrt{(X_{i1}-X_{j1})^2 + \\cdots + (X_{ip}-X_{jp})^2}.\n\\end{align*}\\] La distance euclidienne est tout simplement la longueur du segment qui relie les deux points dans l’espace.\nIl existe un très grand nombre d’autres mesures de dissemblance pour variables quantitatives, ordinales, nominales et binaires. Voici une brève description de certaines d’entre elles, qui sont disponibles dans proc distance.\nMesures de dissemblance pour variables quantitatives:\nOn peut considérer d’autres mesures de distance. Par exemple, la distance \\(l_1\\), ou distance de Manhattan, est la somme des valeurs absolues entre chaque composante:1 \\[\\begin{align*}\nd(S_i, S_j; l_1) = |X_{i1}-X_{j1}| + \\cdots + |X_{ip}-X_{jp}|\n\\end{align*}\\] Plus généralement, la distance de Minkowski ou distance \\(l_q\\) \\[\\begin{align*}\nd(S_i, S_j; l_q) = \\left\\{(X_{i1}-X_{j1})^q + \\cdots + (X_{ip}-X_{jp})^q\\right\\}^{1/q}\n\\end{align*}\\]\nMesure de dissemblance pour variables catégorielles:\nLe plus simple est d’utiliser la mesure d’appariement simple (simple matching). Cette mesure est simplement de la proportion des variables pour lesquelles les deux sujets ont des valeurs différentes.\nOn peut traiter les variables ordinales comme continues ou les traiter comme des variables nominales avec la mesure d’appariement simple; ce faisant, on n’utilise pas l’ordre entre les modalités.\n\n\n4.5.2 Mesures de similarité\nIl existe d’autres façons de définir la similarité entre observations. Par exemple, si on considère la dissemblance Euclidienne, on pourrait créer une matrice de similarité \\(\\mathbf{S}\\) \\(n \\times n\\) où les entrées indiquent si les observations sont à distance \\(\\varepsilon\\) les unes des autres (si oui, en assignant \\(1\\) et autrement \\(0\\)) ou encore avec \\(1\\) pour les \\(m\\) plus proches voisins et \\(0\\) autrement. Cette approche sera utilisée lors de la présentation des regroupement spectraux."
  },
  {
    "objectID": "03-regroupements.html#méthodes-de-regroupement",
    "href": "03-regroupements.html#méthodes-de-regroupement",
    "title": "4  Analyse de regroupements",
    "section": "4.6 Méthodes de regroupement",
    "text": "4.6 Méthodes de regroupement\n\n4.6.1 K-moyennes\nL’algorithme de \\(K\\)-moyennes est un des plus populaires pour la segmentation en raison de son faible coût de calcul et de sa simplicité.\n\n\n\n\n\nFigure 4.5: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.6: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.7: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.8: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.9: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.10: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.11: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.12: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.13: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.14: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.15: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.16: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.17: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n\nFigure 4.18: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\n\n4.6.2 Algorithm DBSCAN\n\n\n4.6.3 Mélanges de modèles\n\n\n4.6.4 Regroupements spectraux"
  },
  {
    "objectID": "03-regroupements.html#mesure-de-la-qualité-des-regroupements",
    "href": "03-regroupements.html#mesure-de-la-qualité-des-regroupements",
    "title": "4  Analyse de regroupements",
    "section": "4.7 Mesure de la qualité des regroupements",
    "text": "4.7 Mesure de la qualité des regroupements\n\n4.7.1 Mesures d’adéquation\n\n\n4.7.2 Choix des hyperparamètres\n\n\n4.7.3 Méthodes hiérarchiques\nLes méthodes hiérarchiques agglomératives assignent les individus aux groupes à l’aide d’un algorithme glouton en partant du cas à \\(n\\) groupes où chaque sujet est un groupe à part entière. La distance entre chaque paire de groupe est calculée. Les deux groupes ayant la distance la plus petite sont regroupés pour ne laisser que \\(n-1\\) groupes. La distance entre chaque paire de groupe est à nouveau calculée (pour les groupes). Les deux groupes ayant la distance la plus petite sont regroupés pour ne former qu’un seul groupe et ainsi de suite. Le processus se continue ainsi jusqu’à ce que tous les sujets soient regroupés en un seul groupe.\nAvec une méthode hiérarchique, on n’a pas besoin de spécifier le nombre de groupes à priori. Cependant, une fois qu’un sujet est assigné à un groupe, il ne peut le quitter pour être réassigné à un autre groupe plus tard. Ce qui différencie les différentes méthodes hiérarchiques est la manière dont est calculée la distance entre deux groupes.\nPour les méthodes non hiérarchiques, le nombre de groupe est spécifié au départ et un algorithme cherche, à partir d’une solution initiale, la meilleure distribution des sujets à travers ce nombre de groupe d’une manière itérative. Avec ces méthodes, l’assignation d’un sujet peut être modifiée d’une itération à l’autre. Il faut cependant spécifier le nombre de groupe et les « centres » de ces groupes au départ. La solution peut être très sensible au choix des centres initiaux."
  },
  {
    "objectID": "03-regroupements.html#méthodes-hiérarchiques-1",
    "href": "03-regroupements.html#méthodes-hiérarchiques-1",
    "title": "4  Analyse de regroupements",
    "section": "4.8 Méthodes hiérarchiques",
    "text": "4.8 Méthodes hiérarchiques\nCette méthode débute avec \\(n\\) groupes, un par sujet, et procède en regroupant des groupes formés au préalable d’une manière hiérarchique jusqu’à ce que tous les sujets ne forment qu’un seul groupe. Le nombre de groupe retenu pourra être sélectionné à l’aide de certains critères que nous verrons plus tard.\nÀ une étape donnée, il faut choisir quels groupes seront combinés. Les deux groupes dont la distance est la plus faible seront combinés. Il faut donc être en mesure de calculer la distance entre deux groupes. Nous allons décrire la méthode de Ward, qui compte parmi les plus populaires. Nous reviendrons brièvement sur d’autres méthodes plus loin.\n\n4.8.1 Méthode de Ward\nCette méthode est basée sur un critère d’homogénéité global des groupes. Pour un groupe donné, cette homogénéité est mesurée par la somme des carrés des observations par rapport à la moyenne du groupe. L’homogénéité globale est alors la somme des homogénéités de tous les groupes. Plus l’homogénéité globale est petite, plus les groupes sont homogènes. À une étape donnée, les deux groupes qui causent la plus petite hausse de l’homogénéité globale (la plus petite perte d’information) sont regroupés. La méthode de Ward donne des groupes compacts d’apparence sphérique.\nPlus précisément, supposons qu’à une étape du processus hiérarchique, nous avons \\(M\\) groupes et que nous voulons passer à \\(M-1\\) groupes. Pour un groupe \\(K\\) (parmi \\(1, 2, \\ldots, M\\)), définissons la somme des carrés des distances par rapport à la moyenne du groupe, \\(\\mathsf{SCD}_k\\). Plus \\(\\mathsf{SCD}_k\\) est petite, plus le groupe est compact et homogène.\nOn peut calculer cette distance pour tous les \\(M\\) groupes et définir l’homogénéité globale comme la somme de l’homogénéité de tous les groupes, \\[\\begin{align*}\n\\mathsf{SCD}_G = \\mathsf{SCD}_1 + \\cdots + \\mathsf{SCD}_M.\n\\end{align*}\\] Plus l’homogénéité globale \\(\\mathsf{SCD}_G\\) est petite, mieux c’est. Pour passer de \\(M\\) à \\(M-1\\) groupes, la méthode de Ward va regrouper les deux groupes qui feront que \\(\\mathsf{SCD}_G\\) sera la plus petite possible.\nOn procède à une analyse simplifiée des données pour le voyage organisé avec deux variables et vingt observations afin d’être en mesure de visualiser l’algorithme de groupement.\n\n\n\n\n\n\n\n\nFigure 4.19: Regroupements hiérarchiques sur le sous-ensemble des données, étape par étape\n\n\n\n\nLa première analyse utilise la méthode de Ward. Les commandes SAS se trouvent dans cluster1_simplifie.sas; la présentation de la procédure et de la syntaxe est différée. L’historique de regroupement est décrit dans la sortie SAS. La première colonne donne le nombre de groupes. Au départ, les observations 16 et 19 sont regroupées, il y a maintenant 19 groupes. Ensuite, les observations 11 et 13 sont regroupées, il y a maintenant 18 groupes. Au moment de passer de 14 à 13 groupes, c’est le groupe formé à l’étape 16 qui est fusionné avec l’observation 2 et ainsi de suite. La colonne Fréq donne le nombre d’observations dans le groupe qui vient d’être formé.\n\n\n\n\n\nFigure 4.20: ?(caption)\n\n\n\n\nLes quantités sprsq et rsq sont des statistiques qui peuvent servir de guide pour choisir le nombre de groupes. Le \\(\\mathsf{RSQ}\\) est une mesure similaire au \\(R^2\\) régression linéaire qui mesure globalement à quel point les groupes sont homogènes. Elle prend une valeur entre 0 et 1 où 0 et plus le \\(\\mathsf{RSQ}\\) est élevé, meilleur le regroupement. On définit le \\(\\mathsf{RSQ}\\) comme la proportion de la variabilité expliquée par les groupes. C’est une version standardisée de la somme des homogénéités, \\(\\mathsf{SCD}_G\\), \\[\\begin{align*}\n\\mathsf{RSQ} = 1-\\frac{\\mathsf{SCD}_G}{\\mathsf{SCD}_T},\n\\end{align*}\\] où \\(\\mathsf{SCD}_T\\) est la somme des carrés des distances par rapport à la moyenne lorsque toutes les observations sont dans un même groupe. Le graphique Figure 4.21 montre l’évolution du \\(\\mathsf{RSQ}\\) en fonction du nombre de groupes.\n\n\n\n\n\nFigure 4.21: Critère du R carré en fonction du nombre de groupes.\n\n\n\n\nL’idée est généralement de choisir un petit nombre de groupe avec un \\(\\mathsf{RSQ}\\) assez élevé. Ici, on voit que le \\(\\mathsf{RSQ}\\) chute brutalement en passant de trois à deux groupes (il passe de 78.2% de variabilité expliquée à 48.6%). Ainsi, choisir trois groupes semble raisonnable.\nL’autre mesure, le \\(\\mathsf{SPRSQ}\\) ou \\(R\\) carré semi-partiel, mesure la perte d’homogénéité résultant du fait que l’on vient de former un nouveau groupe. Comme on veut des groupes homogènes, on veut qu’elle soit petite. Plus précisément, supposons que les groupes \\(k_1\\) et \\(k_2\\) viennent d’être regroupés à une étape donnée. Soient \\(\\mathsf{SCD}_{k_1}\\) et \\(\\mathsf{SCD}_{k_2}\\) les homogénéités de ces deux groupes et \\(\\mathsf{SCD}_{k}\\) l’homogénéité du nouveau groupe formé en fusionnant les deux. On définit la perte d’homogénéité (relative) en combinant ces deux groupes \\[\\begin{align*}\n\\mathsf{SPRSQ} = \\frac{\\mathsf{SCD}_k - \\mathsf{SCD}_{k_1} - \\mathsf{SCD}_{k_2}}{\\mathsf{SCD}_T}\n\\end{align*}\\] On peut ainsi tracer une courbe pour le \\(\\mathsf{SPRSQ}\\) en fonction du nombre de groupes, comme dans le graphique Figure 4.22.\n\n\n\n\n\nFigure 4.22: Courbe du \\(R^2\\) semi-partiel en fonction du nombre de regroupements hiérarchiques.\n\n\n\n\nLa procédure SAS qui permet d’effectuer une analyse de regroupements hiérarchique est cluster. Le fichier cluster2_complet.sas explique les différentes options disponibles.\n\n\nCode\nproc cluster data=temp method=ward outtree=temp1 nonorm rsquare;\nvar x1-x6;\ncopy id cluster_vrai x1-x6;\nods output stat.cluster.ClusterHistory=criteres;\nrun;\n\n\nOn peut représenter graphique le R carré (Figure Figure 4.23), le R carré semi partiel (Figure Figure 4.24) en fonction du nombre de groupes.\n\n\n\n\n\nFigure 4.23: R carré en fonction des regroupements hiérarchiques\n\n\n\n\n\n\n\n\n\nFigure 4.24: R carré semi-partiel en fonction des regroupements hiérarchiques\n\n\n\n\nParfois, l’information est présentée sous forme de dendogramme, qui trace l’arbre et la fusion des groupes. On peut ainsi retracer l’historique de la procédure hiérarchique. Celui produit par SAS donne, à un facteur multiplicatif près, le \\(\\mathsf{SPRSQ}\\). Il n’y a donc pas de nouvelles informations ici. On voit que c’est lorsqu’on passe de trois à deux groupes, qu’il y a une bonne perte d’homogénéité.\nEn pratique, on ne peut jamais savoir si on a bel et bien regroupé ensemble les bons sujets. Mais ici, comme il s’agit de données artificielles qui ont été générées, nous connaissons la composition des vrais groupes. Il s’avère qu’il y en a effectivement trois. De plus, la solution à trois groupes obtenue avec la méthode de Ward a bien réussi à retrouver les groupes. Ceci est un exemple facile où les observations sont bien séparées: ce ne sera pas toujours aussi simple en pratique.\nInterprétation des groupes: la méthode la plus simple consiste à inspecter les moyennes des variables de chaque groupe et de voir s’il en découle une interprétation raisonnable. La procédure tree permet d’extraire la solution avec un nombre spécifié de groupes et il est ensuite facile (avec la procédure means) d’obtenir ces moyennes (voir le fichier cluster2_complet.sas).\n\n\n\n\n\nFigure 4.25: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 4.26: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 4.27: ?(caption)\n\n\n\n\nLe groupe 1 est le groupe où les sujets ont les valeurs, en moyenne, les plus faibles pour les six variables. Le groupe 2 est celui où les sujets ont les valeurs, en moyenne, les plus élevées pour les 6 variables sauf pour la variable \\(X_1\\) (activité sociale). Le groupe 3 est celui où les sujets ont, en moyenne, la valeur la plus élevée de la variable \\(X_1\\) et des valeurs moyennes inférieures au groupe 3 mais supérieures au groupe 2 pour les cinq autres variables.\nDans l’article, les auteurs ont baptisé les sujets du groupe 1, les « indépendants », ceux du groupe 2, les « dépendants » et ceux du groupe 3, les « sociables ». Notez qu’on ne peut pas tester l’égalité des moyennes des variables pour les différents groupes avec une ANOVA; la sélection des groupes est faite à l’aide d’un algorithme glouton pour maximiser la distance entre les groupes, aussi cela invalide l’inférence. On peut aussi explorer les groupes en modélisant les effets des variables en ce qui a trait à l’appartenance aux groupes. Traditionnellement, l’analyse discriminante est utilisée à cette fin. Il est aussi possible d’utiliser un arbre de classification ou une autre méthode prévisionnelle, telle la régression multinomiale logistique. La variable identifiant le groupe d’appartenance obtenu avec l’analyse de regroupement sert alors de variable dépendante \\(Y\\). Ce type d’analyse permet de creuser un peu plus pour essayer de comprendre la structure des groupes formés."
  },
  {
    "objectID": "03-regroupements.html#calcul-alternatif-des-distances-pour-le-regroupement-hiérarchique",
    "href": "03-regroupements.html#calcul-alternatif-des-distances-pour-le-regroupement-hiérarchique",
    "title": "4  Analyse de regroupements",
    "section": "4.9 Calcul alternatif des distances pour le regroupement hiérarchique",
    "text": "4.9 Calcul alternatif des distances pour le regroupement hiérarchique\nNous avons utilisé la méthode de Ward afin de calculer la distance entre les groupes et procéder au passage de \\(n\\) groupes à un groupe, avec l’approche hiérarchique. Supposons que nous avons choisi une mesure de dissemblance \\(d(S_i, S_j)\\) quelconque (distance euclidienne par exemple) pour mesurer la distance entre deux sujets. Voici comment sont choisis les regroupements avec ces méthodes.\n\nMéthode du plus proche voisin ou méthode de liaison simple (nearest neighbor, single linkage): utilise la distance minimale entre chaque paire de sujets (un pour chaque groupe) provenant des deux groupes. Cette méthode fonctionne bien si l’écart entre deux regroupements est suffisamment grand. À l’inverse, s’il y a des observations bruitées entre deux regroupements, la qualité des regroupements en sera affectée.\nMéthode du voisin le plus éloigné ou méthode de liaison complète (complete linkage): utilise la distance maximale entre toutes les paires de sujets (un pour chaque groupe) provenant des deux groupes. Cette méthode est moins sensible au bruit et aux faibles écarts entre regroupements, mais a tendance à casser les regroupements globulaires.\nMéthode de liaison moyenne (average linkage): utilise la moyenne des distances entre toutes les paires de sujets (un pour chaque groupe) provenant des deux groupes.\nMéthode du barycentre (centroid): utilise la distance entre les représentants moyens de chaque groupe où le représentant moyen d’un groupe est le barycentre, soit la moyenne variable par variable, des sujets formant le groupe.\n\nLe fichier cluster3_voisin_eloigne.sas contient les commandes pour utiliser la méthode du voisin le plus éloigné (avec l’option method=complete). Le graphe plus bas donne cette distance pour les deux groupes qui viennent d’être fusionnés. Il s’agit donc du maximum des distances entre chaque paire de sujets (un pour chaque groupe) provenant des deux groupes fusionnés.\n\n\n\n\n\nFigure 4.28: Distance maximale entre groupes en fonction des regroupements hiérarchiques pour la méthode du voisin le plus éloigné.\n\n\n\n\nComme on veut que cette distance soit petite pour les groupes fusionnés, on pourrait être tenté d’arrêter à trois groupes ici sur la base de la Figure Figure 4.29. L’interprétation des groupes ne change pas comparativement aux analyses précédentes. La taille des groupes, (44, 71, 35), change un peu par rapport à la solution avec la méthode de Ward qui donnait des tailles de (43, 75, 32).\nOn peut comparer les performances des regroupements hiérarchiques selon la méthode de groupement. La page web de scikit-learn developers montre la performance sur des exemples jouets, qui montre que selon les hypothèses et la structure, aucune ne performe mieux que les autres dans tous les exemples.\n\n\n\n\n\nFigure 4.29: Comparaison des méthodes de groupement sur des données test\n\n\n\n\n\n\nCode\nlibrary(dbscan)\n# Trois plus proches voisins\neps <- quantile(dbscan::kNNdist(cluster, k = 3), \n                probs = 0.9)\nregroup_dbscan <- \n  dbscan::dbscan(x = cluster, \n                 eps = eps, \n                 minPts = 3)\npairs(cluster, col = regroup_dbscan$cluster + 1L)\n\n\n\n\n\nCode\n# Regroupements spectraux\nclust <- FCPS::SpectralClustering(\n  Data = cluster,\n  ClusterNo = 3)\n\n\nWarning in doTryCatch(return(expr), name, parentenv, handler): ClusterRename:\nDataOrDistances number of rows does not equal length of Cls. Nothing is done\n\n\nCode\npairs(cluster, col = clust$Cls + 1)\n\n\n\n\n\nEncore une fois, l’interprétation des groupes ne change pas comparativement aux analyses précédentes. La taille des groupes, (45, 75, 30) change un peu par rapport à la solution avec la méthode de Ward qui donnait des tailles de (43, 75, 32).\nRègle générale, les différentes étapes des méthodes agglomératives hiérarchiques nécessitent \\(\\mathrm{O}(n^3)\\) opérations, bien qu’une version plus parsimonieuse existe avec complexité \\(\\mathrm{O}(n^2\\ln n)\\) ou \\(\\mathrm{O}(n^2)\\) pour les méthodes de liaison simple et complexe. La formule de Lance–Williams permet de mettre à jour récursivement les distances entre regroupements pour la plupart des méthodes considérées. Le coût élevé de la méthode de regroupement hiérarchique, qui dépend de la taille de l’échantillon, devient prohibitif avec des mégadonnées. Il nécessite aussi le calcul d’une mesure de dissemblance et l’évaluation de la qualité de l’agglomération, autre que graphique, n’est pas évidente. Ces méthodes sont largement discontinuées de nos jours par des alternatives modernes (regroupement spectraux, mélanges de modèles, \\(K\\)-médoïdes par itérations de Voronoï)."
  },
  {
    "objectID": "03-regroupements.html#méthodes-non-hiérarchiques",
    "href": "03-regroupements.html#méthodes-non-hiérarchiques",
    "title": "4  Analyse de regroupements",
    "section": "4.10 Méthodes non hiérarchiques",
    "text": "4.10 Méthodes non hiérarchiques\nContrairement aux méthodes hiérarchiques, il faut spécifier le nombre de groupe désiré dès le départ pour les méthodes non hiérarchiques.\nNous allons utiliser cette procédure pour raffiner la solution obtenue précédemment avec la méthode de Ward en utilisant les moyennes des groupes comme centres préliminaires. Le fichier cluster5_non-hierarchique.sas explique les différentes options. La syntaxe de la procédure SAS fastclus est la suivante:\n\n\nCode\nproc fastclus data=temp seed=initial distance maxclusters=3 out=temp3 maxiter=30;\nvar x1 x2 x3 x4 x5 x6;\nrun;\n\n\nVoici une partie de la sortie SAS:\n\n\n\n\n\nFigure 4.30: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 4.31: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 4.32: ?(caption)\n\n\n\n\nÉvidemment, comme la solution obtenue avec la méthode de Ward est déjà excellente, on ne pourra pas avoir une amélioration notable. Il y a peu de changements par rapport à la solution de la méthode de Ward. Les tailles des groupes étaient de (43, 75, 32) avant. Elles sont maintenant (45, 77, 28). Le \\(R^2\\) passe de 65,7% (avec Ward) à 66.2%.\nL’interprétation des groupes est la même que précédemment.\n\n\n\n\n\nFigure 4.33: ?(caption)\n\n\n\n\nLe champ des applications des \\(K\\)-moyennes est parfois surprenant. Par exemple, cet article de FiveThirtyEight propose une segmentation des électeurs démocrates new-yorkais. Un autre exemple incongru est la compression d’images: la Figure Figure 4.34 montre une image du bâtiment Decelles (coin supérieur gauche) et la reconstruction avec trois, quatre et 10 couleurs obtenues en appliquant l’algorithme des \\(K\\)-moyennes sur la matrice formée par les valeurs des canaux (rouge, vert, bleu) de l’image.\n\n\n\n\n\nFigure 4.34: Compression d’image avec l’algorithme des \\(K\\)-moyennes: image originale (en haut à gauche), compression avec trois (en haut à droite), quatre (en bas à gauche) et 10 (en bas à droite) couleurs."
  },
  {
    "objectID": "03-regroupements.html#considérations-pratiques",
    "href": "03-regroupements.html#considérations-pratiques",
    "title": "4  Analyse de regroupements",
    "section": "4.11 Considérations pratiques",
    "text": "4.11 Considérations pratiques\nIl peut être intéressant de comparer les résultats provenant d’une même méthode avec des nombres différents de groupes et aussi comparer ceux provenant de plusieurs méthodes (voir plus loin pour la description de certaines autres méthodes). Le choix de la méthode et du nombre de groupe n’est pas facile et devrait être basé sur des considérations pratiques et d’interprétation (comme en analyse factorielle). Il n’est pas rare qu’on obtienne des résultats très différents d’une méthode à l’autre pour un même ensemble de données.\nAvec une méthode non hiérarchique, il est préférable de fournir des germes de départ « raisonnablement bon » (provenant d’une méthode hiérarchique par exemple) plutôt que de laisser l’algorithme les choisir au hasard.\nDans notre exemple sur les voyages organisés, on a segmenté les voyageurs en trois groupes (indépendants, dépendants et sociables). Les auteurs de l’article (voir page 369 de l’article) ont comparé les trois groupes selon l’expérience de voyage, la taille de la communauté où ils habitent (avec des ANOVA), selon leur âge, leur revenu et leur éducation (avec des tests d’indépendance du khi-deux). Notez que ces tests sont réalisés sur des variables qui ne sont pas utilisées lors de la segmentation: ils sont invalides si les données sont corrélées avec celle utilisées pour la segmentation. La logique est qu’on choisit les groupes pour maximiser les distances inter-groupes, donc forcément les tests d’hypothèse auront tendance à trouver des différences de moyenne quand ces différences sont trompeuses.\nLe problème majeur avec l’analyse de regroupements est qu’il n’y a pas de façon claire de quantifier la performance de notre analyse. Lorsqu’on développe un modèle de prédiction (régression linéaire ou logistique par exemple), on peut estimer la performance de notre modèle d’une manière objective à l’aide de l’erreur quadratique de généralisation (régression linéaire) ou du taux de bonne classification (régression logistique). Ces quantités peuvent être estimées d’une manière objective en utilisant une méthode telle la validation croisée ou la division de l’échantillon. On ne peut faire de même avec l’analyse de regroupements car on n’a pas de variable réponse à prédire. Tout comme pour l’analyse factorielle, les connaissances à priori, le jugement, et les considérations pratiques font partie d’une analyse de regroupements."
  },
  {
    "objectID": "04-selectionmodeles.html",
    "href": "04-selectionmodeles.html",
    "title": "5  Sélection de variables et de modèles",
    "section": "",
    "text": "Ce chapitre présente des principes, outils et méthodes très généraux pour choisir un « bon » modèle. Nous allons principalement utiliser la régression linéaire pour illustrer les méthodes en supposant que tout le monde connaît ce modèle de base. Les méthodes présentées sont en revanche très générales et peuvent être appliquées avec n’importe quel autre modèle (régression logistique, arbres de classification et régression, réseaux de neurones, analyse de survie, etc.)\nL’expression « sélection de variables » fait référence à la situation où l’on cherche à sélectionner un sous-ensemble de variables à inclure dans notre modèle à partir d’un ensemble de variables \\(X_1, \\ldots, X_p\\). Le terme variable ici inclut autant des variables distinctes que des transformations d’une ou plusieurs variables.\nPar exemple, supposons que les variables \\(\\texttt{age}\\), \\(\\texttt{sexe}\\) et \\(\\texttt{revenu}\\) soient trois variables explicatives disponibles. Nous pourrions alors considérer choisir entre ces trois variables. Mais aussi, nous pourrions considérer inclure \\(\\texttt{age}^2\\), \\(\\texttt{age}^3\\), \\(\\log(\\texttt{age})\\), etc. Nous pourrions aussi considérer des termes d’interactions entre les variables, comme \\(\\texttt{age} \\cdot \\texttt{revenu}\\) ou \\(\\texttt{age}\\cdot\\texttt{revenu}\\cdot\\texttt{sexe}\\). Le problème est alors de trouver un bon sous-ensemble de variables parmi toutes celles considérées.\nL’expression « sélection de modèle » est un peu plus générale. D’une part, elle inclut la sélection de variables car, pour une famille de modèles spécifiques (régression linéaire par exemple), choisir un sous-ensemble de variables revient à choisir un modèle. D’autre part, elle fait référence à la situation où l’on cherche à trouver le meilleur modèle parmi des modèles de natures différentes. Par exemple, on pourrait choisir entre une régression linéaire, un arbre de régression, une forêt aléatoire, un réseau de neurones, etc."
  },
  {
    "objectID": "04-selectionmodeles.html#sélection-de-variables-et-de-modèles-selon-les-buts-de-létude",
    "href": "04-selectionmodeles.html#sélection-de-variables-et-de-modèles-selon-les-buts-de-létude",
    "title": "5  Sélection de variables et de modèles",
    "section": "5.2 Sélection de variables et de modèles selon les buts de l’étude",
    "text": "5.2 Sélection de variables et de modèles selon les buts de l’étude\nNous disposons d’une variable réponse \\(Y\\) et d’un ensemble de variables explicatives \\(X_1, \\ldots, X_p\\). L’attitude à adopter dépend des buts de l’étude.\n\n1e situation: On veut développer un modèle pour faire des prédictions sans qu’il soit important de tester formellement les effets des paramètres individuels.\n\nDans ce cas, on désire seulement que notre modèle soit performant pour prédire des valeurs futures de \\(Y\\). On peut alors baser notre choix de variable (et de modèle) en utilisant des outils qui nous guiderons quant aux performances prédictives futures du modèle (voir \\(\\mathsf{AIC}\\), \\(\\mathsf{BIC}\\) et validation croisée plus loin). On pourra enlever ou rajouter des variables et des transformations de variables au besoin afin d’améliorer les performances prédictives. Les méthodes que nous allons voir concernent essentiellement ce contexte.\n\n2e situation: On veut développer un modèle pour estimer les effets de certaines variables sur notre \\(Y\\) et tester des hypothèses de recherche spécifiques concernant certaines variables.\n\nDans ce cas, il est préférable de spécifier le modèle dès le départ selon des considérations scientifiques et de s’en tenir à lui. Faire une sélection de variables dans ce cas est dangereux car on ne peut pas utiliser directement les valeurs-p des tests d’hypothèses (ou les intervalles de confiance sur les paramètres) concernant les paramètres du modèle final car elles ne tiennent pas compte de la variabilité due au processus de sélection de variables.\nUne bonne planification de l’étude est alors cruciale afin de collecter les bonnes variables, de spécifier le ou les bons modèles, et de s’assurer d’avoir suffisamment d’observations pour ajuster le ou les modèles désirés.\nSi procéder à une sélection de variables est quand même nécessaire dans ce contexte, il est quand même possible de le faire en divisant l’échantillon en deux. La sélection de variables pourrait être alors effectuée avec le premier échantillon. Une fois qu’un modèle est retenu, on pourrait alors réajuster ce modèle avec le deuxième échantillon (sans faire de sélection de variables cette fois-ci). L’inférence sur les paramètres (valeurs-p, etc.) sera alors valide. Le désavantage ici qu’il faut avoir une très grande taille d’échantillon au départ afin d’être en mesure de le diviser en deux."
  },
  {
    "objectID": "04-selectionmodeles.html#estimation-de-la-performance",
    "href": "04-selectionmodeles.html#estimation-de-la-performance",
    "title": "5  Sélection de variables et de modèles",
    "section": "5.3 Estimation de la performance",
    "text": "5.3 Estimation de la performance\nIl est préférable d’avoir un modèle un peu trop complexe qu’un modèle trop simple. Plaçons-nous dans le contexte de la régression linéaire et supposons que le vrai modèle est inclus dans le modèle qui a été ajusté. Il y a donc des variables en trop dans le modèle qui a été ajusté: ce dernier est dit surspécifié.\nPar exemple, supposons que le vrai modèle est \\(Y=\\beta_0+\\beta_1X_1+\\varepsilon\\) mais que c’est le modèle \\(Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\varepsilon\\) qui a été ajusté. Dans ce cas, règle générale, les estimateurs des paramètres et les prédictions provenant du modèle sont sans biais. Mais leurs variances estimées seront un peu plus élevées car on estime des paramètres pour des variables superflues.\nSupposons à l’inverse qu’il manque des variables dans le modèle ajusté et que le modèle ajusté est sous-spécifié. Par exemple, supposons que le vrai modèle est \\(Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\varepsilon\\), mais que c’est le modèle \\(Y=\\beta_0+\\beta_1X_1+\\varepsilon\\) qui est ajusté. Dans ce cas, généralement, les estimateurs des paramètres et les prédictions sont biaisés.\n\n\nCode\n# Simuler des observations d'un modèle linéaire\nn <- 200L\n# Variables explicatives bidons\nX1 <- rpois(n = n, lambda = 3)\nX2 <- rbinom(n = n, size = 1, prob = c(0.7, 0.3))\n# Modèle de régression X*beta + aléas\ny_a <- 20 + 2*X1 + 5*X2 + rnorm(n)\ny_b <- 20 + 2*X1 + rnorm(n)\n# Ajuster un modèle linéaire\n# lm(réponse ~ variables explicatives)\nmod_1a <- lm(y_a ~ X1) # sous-spécifié\nmod_2a <- lm(y_a ~ X1 + X2) # correct\n\nmod_1b <- lm(y_b ~ X1) # correct\nmod_2b <- lm(y_b ~ X1 + X2) # surspécifié\n\n# Comparer l'estimation des coefficients\n#  aux vraies valeurs\n# Vérifier si ces dernières se trouvent\n#  dans l'intervalle de confiance\ncoef(mod_1a) #coefficients\n## (Intercept)          X1 \n##        22.6         2.0\nconfint(mod_1a) #intervalles de confiance\n##             2.5 % 97.5 %\n## (Intercept)  21.9  23.36\n## X1            1.8   2.21\ncoef(mod_2a)\n## (Intercept)          X1          X2 \n##       19.98        2.03        5.01\nconfint(mod_2a)\n##             2.5 % 97.5 %\n## (Intercept) 19.66   20.3\n## X1           1.95    2.1\n## X2           4.73    5.3\n\n\nAinsi, il est généralement préférable d’avoir un modèle légèrement surspécifié qu’un modèle sous-spécifié. Plus généralement, il est préférable d’avoir un peu trop de variables dans le modèle que de prendre le risque d’omettre une ou plusieurs variables importantes. Il faut faire attention et ne pas tomber dans l’excès et avoir un modèle trop complexe (avec trop de variables inutiles) car il pourrait souffrir de surajustement (over-fitting). Les exemples qui suivent illustreront ce fait.\n\n5.3.1 Surajustement\nCette section traite de l’optimisme de l’évaluation d’un modèle (trop beau pour être vrai) lorsqu’on utilise les mêmes données qui ont servies à l’ajuster pour évaluer sa performance. Un principe fondamental lorsque vient le temps d’évaluer la performance prédictive d’un modèle est le suivant : si on utilise les mêmes observations pour évaluer la performance d’un modèle que celles qui ont servi à l’ajuster (à estimer le modèle et ses paramètres), on va surestimer sa performance. Autrement dit, notre estimation de l’erreur que fera le modèle pour prédire des observations futures sera biaisée à la baisse. Ainsi, il aura l’air meilleur que ce qu’il est en réalité. C’est comme si on demandait à un cinéaste d’évaluer son dernier film. Comme c’est son film, il n’aura généralement pas un regard objectif. C’est pourquoi on aura tendance à se fier à l’opinion d’un critique.\nOn cherchera donc à utiliser des outils et méthodes qui nous donneront l’heure juste (une évaluation objective) quant à la performance prédictive d’un modèle.\n\n\n5.3.2 Principes généraux\nLes idées présentées ici seront illustrées à l’aide de la régression linéaire. Par contre, elles sont valides dans à peu près n’importe quel contexte de modélisation.\nPlaçons-nous d’abord dans un contexte plus général que celui de la régression linéaire. Supposons que l’on dispose de \\(n\\) observations indépendantes sur (\\(Y, X_1, \\ldots, X_p\\)) et que l’on a ajusté un modèle \\(\\widehat{f}(X_1, \\ldots, X_p)\\), avec ces données, pour prédire une variable continue \\(Y\\).\nCe modèle peut être un modèle de régression linéaire, \\[\\begin{align*}\n\\widehat{f}(X_1, \\ldots, X_p) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X_1 + \\cdots + \\widehat{\\beta}_pX_p\n\\end{align*}\\] mais il pourrait aussi avoir été construit selon d’autres méthodes (réseau de neurones, arbre de régression, forêt aléatoire, etc.) Une manière de quantifier la performance prédictive du modèle est l’erreur quadratique moyenne (mean squared error), \\[\\begin{align*}\n\\mathsf{EQM}=\\mathsf{E}\\left[\\left\\{(Y-\\widehat{f}(X_1, \\ldots, X_p)\\right\\}^2\\right]\n\\end{align*}\\] lorsque (\\(Y, X_1, \\ldots, X_p\\)) est choisi au hasard dans la population. Cette quantité mesure l’erreur théorique (la différence au carré entre la vraie valeur de \\(Y\\) et la valeur prédite par le modèle) que fait le modèle en moyenne pour l’ensemble de la population. Plus cette quantité est petite, meilleur est le modèle. Le problème est que l’on ne peut pas la calculer car on n’a pas accès à toute la population. Tout au plus peut-on essayer de l’estimer ou bien d’estimer une fonction qui, sans l’estimer directement, classifiera les modèles dans le même ordre qu’elle.\nUne première idée est d’estimer l’erreur quadratique moyenne de l’échantillon d’apprentissage (training mean squared error), \\[\\begin{align*}\n\\widehat{\\mathsf{EQM}}_a= \\frac{1}{n}\\sum_{i=1}^n \\left\\{Y_i-\\widehat{f}(X_{i1}, \\ldots, X_{ip})\\right\\}^2.\n\\end{align*}\\]\nMalheureusement, selon le principe fondamental de la section précédente, cette quantité n’est pas un bon estimateur de l’\\(\\mathsf{EQM}\\). En effet, comme on utilise les mêmes observations que celles qui ont estimé le modèle, l’\\(\\widehat{\\mathsf{EQM}}_a\\) aura tendance à toujours diminuer lorsqu’on augmente la complexité du modèle (par exemple, lorsqu’on augmente le nombre de paramètres). L’\\(\\widehat{\\mathsf{EQM}}_a\\) tend à surestimer la qualité du modèle en sous-estimant l’\\(\\mathsf{EQM}\\) et le modèle a l’air meilleur qu’il ne l’est en réalité.\n\n\n5.3.3 Présentation de l’exemple\nCet exemple simple sur le choix d’un modèle polynomial en régression linéaire servira à illustrer le fait qu’on ne peut utiliser directement les mêmes données qui ont servi à ajuster un modèle pour évaluer sa performance.\nNous disposons de 100 observations sur une variable cible \\(Y\\) et d’une seule variable explicative \\(X\\) dans la base de données selection1_train. Nous voulons considérer des modèles polynomiaux (en \\(X\\)) afin d’en trouver un bon pour prédire \\(Y\\). Un modèle polynomial est un modèle de la forme \\(Y=\\beta_0 + \\beta_1X+\\cdots+\\beta_kX^k+\\varepsilon\\). Le cas \\(k=1\\) correspond à un modèle linéaire simple, \\(k=2\\) à un modèle cubique, \\(k=3\\) à un modèle cubique, etc. Notre but est de déterminer l’ordre (\\(k\\)) du polynôme qui nous donnera un bon modèle. Voici d’abord le graphe de ces 100 observations de l’échantillon d’apprentissage.\n\n\n\n\n\nFigure 5.1: Nuage de points de 100 observations simulées d’un modèle polynomial de degré inconnu.\n\n\n\n\nCes données ont été obtenues par simulation et le vrai modèle sous-jacent (celui qui a généré les données) est le modèle cubique, c’est-à-dire le modèle d’ordre \\(k=3\\).\nJ’ai ajusté tour à tour à tour les modèles polynomiaux jusqu’à l’ordre 10, avec l’échantillon d’apprentissage de taille 100. C’est-à-dire, le modèle linéaire avec un polynôme d’ordre \\(k=1\\) (linéaire), \\(k=2\\) (quadratique), etc., jusqu’à \\(k=10\\). J’ai ensuite obtenu la valeur de l’erreur quadratique moyenne d’apprentissage pour chacun de ces modèles. En pratique, on ne pourrait pas calculer l’erreur quadratique moyenne de généralization, mais j’ai approximé cette dernière en simulant 100 000 observations du vrai modèle (selection1_test), en obtenant la prédiction pour chacune de ces 100 000 observations en utilisant le modèle d’ordre \\(k\\) ajusté sur les données d’apprentissage et en calculant l’erreur quadratique moyenne par la suite. En pratique, on ne peut réaliser cette opération car on ne connaît pas le vrai modèle.\n\n\n\n\n\n\n\n\nFigure 5.2: erreur quadratique moyenne d’apprentissage (\\(\\widehat{\\mathsf{EQM}}_a\\)) et erreur quadratique moyenne théorique (\\(\\mathsf{EQM}\\)) en fonction de l’ordre (\\(k\\)) du polynôme ajusté.\n\n\n\n\nOn voit clairement dans la Figure 5.2 que l’\\(\\widehat{\\mathsf{EQM}}_a\\) diminue en fonction de l’ordre sur l’échantillon d’apprentissage: plus le modèle est complexe, plus l’erreur observée sur l’échantillon d’apprentissage est petite. La courbe \\(\\mathsf{EQM}\\) donne l’heure juste, car il s’agit d’une estimation de la performance réelle des modèles sur de nouvelles données. On voit que le meilleur modèle est donc le modèle cubique (\\(k=3\\)), ce qui n’est pas surprenant puisqu’il s’agit du modèle que utilisé pour générer les données. On peut aussi remarquer d’autres éléments intéressants. Premièrement, on obtient un bon gain en performance (\\(\\mathsf{EQM}\\)) en passant de l’ordre \\(2\\) à l’ordre \\(3\\). Ensuite, la perte de performance en passant de l’ordre \\(3\\) à \\(4\\), et ensuite à des ordres supérieurs n’est pas si sévère, même si elle est présente. Cela illustre empiriquement qu’il est préférable d’avoir un modèle un peu trop complexe que d’avoir un modèle trop simple. Il serait beaucoup plus grave pour la performance de choisir le modèle avec \\(k=2\\) que celui avec \\(k=4\\).\nEn pratique par contre, on n’a pas accès à la population : les 100 000 observations qui ont servi à estimer l’\\(\\mathsf{EQM}\\) théorique ne seront pas disponible. Si on a seulement l’échantillon d’apprentissage, soit 100 observations dans notre exemple, comment faire alors pour choisir le bon modèle? C’est ce que nous verrons à partir de la section suivante.\nMais avant cela, nous allons discuter un peu plus en détail au sujet de la régression linéaire et d’une mesure très connue, le coefficient de détermination (\\(R^2\\)). Supposons que l’on a ajusté un modèle de régression linéaire \\[\\begin{align*}\n\\widehat{f}(X_1, \\ldots, X_p) = \\widehat{Y}=\\widehat{\\beta}_0 + \\widehat{\\beta}_1X_1+ \\cdots + \\widehat{\\beta}_p X_p.\n\\end{align*}\\] La somme du carré des erreurs (\\(\\mathsf{SCE}\\)) pour notre échantillon est \\[\\begin{align*}\n\\mathsf{SCE}=\\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1X_1 - \\cdots - \\widehat{\\beta}_p X_p)^2 = \\sum_{i=1}^n (Y_i-\\widehat{Y}_i)^2.\n\\end{align*}\\] On peut démontrer que si on ajoute une variable quelconque au modèle, la valeur de la somme du carré des erreurs va nécessairement baisser. Il est facile de se convaincre de cela. En régression linéaire, les estimations sont obtenues par la méthode des moindres carrés qui consiste justement à minimiser la \\(\\mathsf{SCE}\\). Ainsi, en ajoutant une variable \\(X_{p+1}\\) au modèle, la \\(\\mathsf{SCE}\\) ne peut que baisser car, dans le pire des cas, le paramètre de la nouvelle variable sera \\(\\widehat{\\beta}_{p+1}=0\\) et on retombera sur le modèle sans cette variable. C’est pourquoi, la quantité \\(\\widehat{\\mathsf{EQM}}_a=\\mathsf{SCE}/n\\) ne peut être utilisée comme outil de sélection de modèles en régression linéaire.\nNous venons d’ailleurs d’illustrer cela avec notre exemple sur les modèles polynomiaux. En effet, augmenter l’ordre du polynôme de \\(1\\) revient à ajouter une variable. Le coefficient de détermination (\\(R^2\\)) est souvent utilisé, à tort, comme mesure de qualité du modèle. Il peut s’interpréter comme étant la proportion de la variance de \\(Y\\) qui est expliquée par le modèle.\nLe coefficient de détermination est \\[\\begin{align*}\nR^2=\\{\\mathsf{cor}(\\boldsymbol{y}, \\widehat{\\boldsymbol{y}})\\}^2 = 1-\\frac{\\mathsf{SCE}}{\\mathsf{SCT}},\n\\end{align*}\\] où \\(\\mathsf{SCT}=\\sum_{i=1}^n (Y_i-\\overline{Y})^2\\) est la somme des carrés totale calculée en centrant les observations. La somme des carrés totale, \\(\\mathsf{SCT}\\), ne varie pas en fonction du modèle. Ainsi, on voit que le \\(R^2\\) va méchaniquement augmenter lorsqu’on ajoute une variable au modèle (car la \\(\\mathsf{SCE}\\) diminue). C’est pourquoi on ne peut pas l’utiliser comme outil de sélection de variables.\nLe problème principal que nous avons identifié jusqu’à présent afin d’être en mesure de bien estimer la performance d’un modèle est le suivant : si on utilise les mêmes observations pour évaluer la performance d’un modèle que celles qui ont servi à l’ajuster, on va surestimer sa performance.\nIl existe deux grandes approches pour contourner ce problème lorsque le but est de faire de la sélection de variables ou de modèle :\n\nutiliser les données de l’échantillon d’apprentissage (en échantillon) et pénaliser la mesure d’ajustement (ici \\(\\widehat{\\mathsf{EQM}}_a\\)) pour tenir compte de la complexité du modèle (par exemple, à l’aide de critères d’informations).\ntenter d’estimer l’\\(\\mathsf{EQM}\\) directement sur d’autres données (hors échantillon) en utilisant des méthodes de rééchantillonnage, notamment la validation croisée ou la validation externe (division de l’échantillon).\n\n\n\n5.3.4 Pénalisation et critères d’information\nPlaçons-nous dans le contexte de la régression linéaire pour l’instant. Nous avons déjà utilisé les critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) en analyse factorielle. Il s’agit de mesures qui découlent d’une méthode d’estimation des paramètres, la méthode du maximum de vraisemblance.\nIl s’avère que les estimateurs des paramètres obtenus par la méthode des moindres carrés en régression linéaire sont équivalents à ceux provenant de la méthode du maximum de vraisemblance si on suppose la normalité des termes d’erreurs du modèle. Ainsi, dans ce cas, nous avons accès aux \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\), deux critères d’information définis pour les modèles dont la fonction objective est la vraisemblance (qui mesure la probabilité des observations sous le modèle postulé suivant une loi choisie par l’utilisateur). La fonction de vraisemblance \\(\\mathcal{L}\\) et la log-vraisemblance \\(\\ell\\) mesurent l’adéquation du modèle.\nSupposons que nous avons ajusté un modèle avec \\(p\\) paramètres en tout (incluant l’ordonnée à l’origine). En régression linéaire, le critère d’information d’Akaike, \\(\\mathsf{AIC}\\), est \\[\\begin{align*}\n\\mathsf{AIC} &=-2 \\ell(\\widehat{\\boldsymbol{\\beta}}, \\widehat{\\sigma}^2) +2p = n \\ln (\\mathsf{EQM}) + 2p + \\text{constante},\n\\end{align*}\\] tandis que le critère d’information bayésien de Schwartz, \\(\\mathsf{BIC}\\), est défini par \\[\\begin{align*}\n\\mathsf{BIC} &=-2 \\ell(\\widehat{\\boldsymbol{\\beta}}, \\widehat{\\sigma}^2) + p\\ln(n)=n \\ln (\\mathsf{EQM}) + p\\ln(n) + \\text{constante}.\n\\end{align*}\\] Plus la valeur du \\(\\mathsf{AIC}\\) (ou du \\(\\mathsf{BIC}\\)) est petite, meilleur est l’adéquation. Que se passe-t-il lorsqu’on ajoute un paramètre à un modèle? D’une part, la somme du carré des erreurs va méchaniquement diminuer tout comme l’erreur quadratique moyenne \\(\\textsf{eqm} = \\textsf{SCE}/n\\), donc la quantité \\(n \\ln (\\mathsf{EQM})\\) va diminuer. D’autre part, la valeur de \\(p\\) augmente de \\(1\\). Ainsi, le \\(\\mathsf{AIC}\\) peut soit augmenter, soit diminuer, lorsqu’on ajoute un paramètre; idem pour le \\(\\mathsf{BIC}\\). Par exemple, le \\(\\mathsf{AIC}\\) va diminuer seulement si la baisse de la somme du carré des erreurs est suffisante pour compenser le fait que le terme \\(2p\\) augmente à \\(2 (p+1)\\).\nCes critères pénalisent l’ajout de variables afin de se prémunir contre le surajustement. De plus, le \\(\\mathsf{BIC}\\) pénalise plus que le \\(\\mathsf{AIC}\\). Par conséquent, le critère \\(\\mathsf{BIC}\\) va choisir des modèles contenant soit le même nombre, soit moins de paramètres que le \\(\\mathsf{AIC}\\).\nLes critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) peuvent être utilisés comme outils de sélection de variables en régression linéaire mais aussi beaucoup plus généralement avec d’autres méthodes basées sur la vraisemblance (analyse factorielle, régression logistique, etc.) En fait, n’importe quel modèle dont les estimateurs proviennent de la méthode du maximum de vraisemblance produira ces quantités. Nous donnerons des formules générales pour le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\) dans le chapitre sur la régression logistique.\nLe critère \\(\\mathsf{BIC}\\) est le seul de ces critères qui est convergent. Cela veut dire que si l’ensemble des modèles que l’on considère contient le vrai modèle, alors la probabilité que le critère \\(\\mathsf{BIC}\\) choisissent le bon modèle tend vers 1 lorsque \\(n\\) tend vers l’infini. Il faut mettre cela en perspective : il est peu vraisemblable que \\(Y\\) ait été généré exactement selon un modèle de régression linéaire, car le modèle de régression n’est qu’une approximation de la réalité. Certains auteurs trouvent que le \\(\\mathsf{BIC}\\) est quelquefois trop sévère (il choisit des modèles trop simples) pour les tailles d’échantillons finies. Dans certaines applications, cette parcimonie est utile, mais il n’est pas possible de savoir d’avance lequel de ces deux critères (\\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\)) sera préférable pour un problème donné.\n\nIl est facile d’obtenir le \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) avec les méthodes AIC et BIC. On illustre ceci avec le modèle cubique:\n\n\nCode\ndata(selection1_train, package = \"hecmulti\")\n# Ajuster un polynôme de degré trois (modèle cubique)\nmod_cub <- lm(y ~ poly(x, 3),\n              data = selection1_train)\nsummary(mod_cub) # Tableau résumé des coefficients\n## \n## Call:\n## lm(formula = y ~ poly(x, 3), data = selection1_train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -111.91  -34.11    2.08   36.69  142.48 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    -9.77       5.22   -1.87   0.0645 .  \n## poly(x, 3)1   824.81      52.24   15.79  < 2e-16 ***\n## poly(x, 3)2  -281.95      52.24   -5.40  4.9e-07 ***\n## poly(x, 3)3   160.99      52.24    3.08   0.0027 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 52.2 on 96 degrees of freedom\n## Multiple R-squared:  0.75,   Adjusted R-squared:  0.742 \n## F-statistic:   96 on 3 and 96 DF,  p-value: <2e-16\nAIC(mod_cub)\n## [1] 1081\nBIC(mod_cub)\n## [1] 1094\n\n\nLe Tableau 5.1 résume ces quantités pour tous les modèles de l’ordre 1 à l’ordre 10.\n\n\n\nTableau 5.1:  Mesures de la qualité de l’ajustement d’un modèle polynomial aux données en fonction de l’ordre du polynôme. \n \n  \n      \n    \\(\\mathsf{EQM}\\) \n    \\(\\widehat{\\mathsf{EQM}}_a\\) \n    \\(R^2\\) \n    \\(\\mathsf{AIC}\\) \n    \\(\\mathsf{BIC}\\) \n    \\(\\mathsf{VC}_{10}\\) \n  \n \n\n  \n    1 \n    3191 \n    3674 \n    0.65 \n    1111 \n    1119 \n    3675 \n  \n  \n    2 \n    3133 \n    2879 \n    0.73 \n    1088 \n    1099 \n    2898 \n  \n  \n    3 \n    2697 \n    2620 \n    0.75 \n    1081 \n    1094 \n    2676 \n  \n  \n    4 \n    2767 \n    2582 \n    0.75 \n    1081 \n    1097 \n    2666 \n  \n  \n    5 \n    2771 \n    2581 \n    0.75 \n    1083 \n    1102 \n    2711 \n  \n  \n    6 \n    2780 \n    2578 \n    0.75 \n    1085 \n    1106 \n    2757 \n  \n  \n    7 \n    2780 \n    2577 \n    0.75 \n    1087 \n    1111 \n    2788 \n  \n  \n    8 \n    2797 \n    2531 \n    0.76 \n    1087 \n    1113 \n    2846 \n  \n  \n    9 \n    2811 \n    2528 \n    0.76 \n    1089 \n    1118 \n    2896 \n  \n  \n    10 \n    2849 \n    2519 \n    0.76 \n    1091 \n    1122 \n    2976 \n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: Critères d’information en fonction de l’ordre du polynôme.\n\n\n\n\nOn voit dans le Tableau 5.1 que l’erreur quadratique moyenne des données d’apprentissage, \\(\\widehat{\\mathsf{EQM}}_a\\), diminue toujours à mesure qu’on ajoute des variables (c’est-à-dire, qu’on augmente l’ordre du polynôme); ces valeurs sont représentées dans la Figure 5.2. Les critères d’information, \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\), ne sont pas sur la même échelle, mais le graphique de la Figure 5.3 illustre un comportement semblable à la vraie courbe de l’erreur quadratique moyenne théorique et suggèrent que le meilleur modèle est le modèle cubique (\\(k=3\\)), c’est-à-dire le vrai modèle.  N’oubliez pas que ces critères sont calculés avec l’échantillon d’apprentissage (\\(n=100\\)), mais en pénalisant l’ajout de variables. On est ainsi en mesure de contrecarrer le problème provenant du fait qu’on ne peut pas utiliser directement le \\(\\widehat{\\mathsf{EQM}}_a\\).\nLe \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\) sont des critères très utilisés et très généraux. Ils sont disponibles dès qu’on utilise la méthode du maximum de vraisemblance est utilisée comme méthode d’estimation. \n\n\n5.3.5 Validation externe\nLa deuxième grande approche après celle consistant à pénaliser le \\(\\widehat{\\mathsf{EQM}}_a\\) consiste à tenter d’estimer le \\(\\mathsf{EQM}\\) directement sans utiliser deux fois les mêmes données. Nous allons voir deux telles méthodes ici, la validation externe (division de l’échantillon) et la validation croisée (cross-validation).\nCes deux méthodes s’attaquent directement au problème qu’on ne peut utiliser (sans ajustement) les mêmes données qui ont servi à estimer les paramètres d’un modèle pour estimer sa performance. Pour ce faire, l’échantillon de départ est divisé en deux, ou plusieurs parties, qui vont jouer des rôles différents.\nL’idée de la validation externe est simple. Nous avons un échantillon de taille \\(n\\) que nous pouvons diviser au hasard en deux parties de tailles respectives \\(n_1\\) et \\(n_2\\) (\\(n_1+n_2=n\\)), soit\n\nun échantillon d’apprentissage (training) de taille \\(n_1\\) et\nun échantillon de validation (test) de taille \\(n_2\\).\n\nL’échantillon d’apprentissage servira à estimer les paramètres du modèle. L’échantillon de validation servira à estimer la performance prédictive (par exemple estimer l’\\(\\mathsf{EQM}\\)) du modèle. Comme cet échantillon n’a pas servi à estimer le modèle lui-même, il est formé de « nouvelles » observations qui permettent d’évaluer d’une manière réaliste la performance du modèle. Comme il s’agit de nouvelles observations, on n’a pas à pénaliser la complexité du modèle et on peut directement utiliser le critère de performance choisi, par exemple, l’erreur quadratique moyenne, c’est-à-dire, la moyenne des erreurs au carré pour l’échantillon de validation. Cette quantité est une estimation valable de l’\\(\\mathsf{EQM}\\) de ce modèle. On peut faire la même chose pour tous les modèles en compétition et choisir celui qui a la meilleure performance sur l’échantillon de validation.\nCette approche possède plusieurs avantages. Elle est facile à implanter. Elle est encore plus générale que les critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\). En effet, ces critères découlent de la méthode d’estimation du maximum de vraisemblance. Plusieurs autres types de modèles ne sont pas estimés par la méthode du maximum de vraisemblance (par exemple, les arbres, les forêts aléatoires, les réseaux de neurones, etc.) La performance de ces modèles peut toujours être estimée en divisant l’échantillon. Cette méthode peut donc servir à comparer des modèles de familles différentes. Par exemple, choisit-on un modèle de régression linéaire, une forêt aléatoire ou bien un réseau de neurones?\nCette approche possède tout de même un désavantage. Elle nécessite une grande taille d’échantillon au départ. En effet, comme on divise l’échantillon, on doit en avoir assez pour bien estimer les paramètres du modèle (l’échantillon d’apprentissage) et assez pour bien estimer sa performance (l’échantillon de validation).\nLa méthode consistant à diviser l’échantillon en deux (apprentissage et validation) afin de sélectionner un modèle est valide. Par contre, si on veut une estimation sans biais de la performance du modèle choisi (celui qui est le meilleur sur l’échantillon de validation), on ne peut pas utiliser directement la valeur observée de l’erreur de ce modèle sur l’échantillon de validation car elle risque de sous-évaluer l’erreur. En effet, supposons qu’on a 10 échantillons et qu’on ajuste 10 fois le même modèle séparément sur les 10 échantillons. Nous aurons alors 10 estimations différentes de l’erreur du modèle. Il est alors évident que de choisir la plus petite d’entre elles sous-estimerait la vraie erreur du modèle. C’est un peu ce qui se passe lorsqu’on choisit le modèle qui minimise l’erreur sur l’échantillon de validation. Le modèle lui-même est un bon choix, mais l’estimation de son erreur risque d’être sous-évaluée.\nUne manière d’avoir une estimation de l’erreur du modèle retenu consiste à diviser l’échantillon de départ en trois (plutôt que deux). Aux échantillons d’apprentissage et de validation, s’ajoute un échantillon « test ». Cet échantillon est laissé de côté durant tout le processus de sélection du modèle qui est effectué avec les deux premiers échantillons tel qu’expliqué plus haut. Une fois un modèle retenu (par exemple celui qui minimise l’erreur sur l’échantillon de validation), on peut alors évaluer sa performance sur l’échantillon test qui n’a pas encore été utilisé jusque là. L’estimation de l’erreur du modèle retenu sera ainsi valide. Il est évident que pour procéder ainsi, on doit avoir une très grande taille d’échantillon au départ.\n\n\n5.3.6 Validation croisée\nSi la taille d’échantillon n’est pas suffisante pour diviser l’échantillon en deux et procéder comme nous venons de l’expliquer, la validation croisée est une bonne alternative. Cette méthode permet d’imiter le processus de division de l’échantillon.\nVoici les étapes à suivre pour faire une validation croisée à \\(K\\) groupes (\\(K\\)-fold cross-validation) :\n\nDiviser l’échantillon au hasard en \\(K\\) parties \\(P_1, P_2, \\ldots, P_K\\) contenant toutes à peu près le même nombre d’observations.\nPour \\(j = 1\\) à \\(K\\),\n\nEnlever la partie \\(j\\).\nEstimer les paramètres du modèle en utilisant les observations des \\(K-1\\) autres parties combinées.\nCalculer la mesure de performance (par exemple la somme du carré des erreurs) de ce modèle pour le groupe \\(P_j\\).\n\nCombiner les \\(K\\) estimations de performance pour obtenir une mesure de performance finale.\n\nPour l’erreur quadratique moyenne, cette dernière étape revient à additionner la somme du carré des erreurs avant de diviser par la taille de l’échantillon totale.\n\n\n\n\n\nFigure 5.4: Illustration de la validation croisée: on scinde l’échantillon d’apprentissage en cinq groupes (abcisse) et à chaque étape, une portion différente des données est mise de côté et ne sert que pour la validation.\n\n\n\n\nOn recommande habituellement de prendre entre \\(K=5\\) et \\(10\\) groupes (le choix de 10 groupes est celui qui revient le plus souvent en pratique). Si on prend \\(K=10\\) groupes, alors chaque modèle est estimé avec 90% des données et on prédit ensuite le 10% restant. Comme on passe en boucle les 10 parties, chaque observation est prédite une et une seule fois à la fin. Il est important de souligner que les groupes sont formés de façon aléatoire et donc que l’estimé que l’on obtient peut être très variable, surtout si la taille de l’échantillon d’apprentissage est petite. Il arrive également que le modèle ajusté sur un groupe ne puisse pas être utilisé pour prédire les observations mises de côté, notamment si des variables catégorielles sont présentes mais qu’une modalité n’est présente que dans un des groupes; ce problème se présente en pratique si certaines classes ont peu d’observations. Un échantillonnage stratifié permet de pallier à cette lacune et de s’assurer d’une répartition plus homogène des variables catégorielles.\n\n\nCode\n# Validation croisée avec k groupes\nlmkfold <- function(formula, data, k, ...){\n   # Créer un accumulateur pour le calcul de l'EQM\n   accu <- 0\n   k <- as.integer(k) # nombre de groupes\n   n <- nrow(data) # nombre d'observations\n   # Permuter les indices des observatoins\n   gp <- sample.int(n, n, replace = FALSE)\n   # Créer une liste de k éléments avec les nos d'observations\n   folds <- split(gp, cut(seq_along(gp), k, labels = FALSE))\n   for(i in seq_len(k)){\n      # Extraire les indices des observations de la portion validation\n      g <- as.integer(unlist(folds[i]))\n      # Ajuster le modèles à toutes les données, moins celles de la portion validation\n      fitlm <- lm(formula, data = data[-g,])\n      # ajouter l'erreur quadratique du pli de validation\n      accu <- accu + sum((data[g, all.vars(formula)[1]] -predict(fitlm, newdata=data[g,]))^2)\n   }\n   # Diviser par la taille de l'échantillon \n   # pour obtenir la moyenne\n   return(accu/n)\n}\n\n# Le paquet 'caret' a une fonction \n# pour faire la validation croisée\ncv_caret <- \n  caret::train(form = formula, \n             data = data, \n             method = \"lm\",\n             trControl = caret::trainControl(\n               method = \"cv\",\n               number = 10))\neqm_cv <- cv_caret$results$RMSE^2\n\n\nLe cas particulier \\(K=n\\) (en anglais leave-one-out cross validation, ou \\(\\mathsf{LOOCV}\\)) consiste à enlever une seule observation, à estimer le modèle avec les \\(n-1\\) autres et à valider à l’aide de l’observation laissée de côté: on répète cette procédure pour chaque observation. Pour les modèles linéaires, il existe des formules explicites qui nous permettent d’éviter d’ajuster \\(n\\) régressions par moindre carrés. Cette forme de validation croisée tend à être trop optimiste.\nRevenons à notre exemple où une seule variable explicative est disponible et où l’on cherche à déterminer un bon modèle polynomial. La dernière colonne de Tableau 5.1, \\(\\mathsf{VC}_{10}\\), donne les moyennes de 100 réplications de estimations de l’\\(\\mathsf{EQM}\\) obtenues avec la validation croisée à 10 groupes. Notez que si vous exécutez le programme, vous n’obtiendrez pas les mêmes valeurs car il y a un élément aléatoire dans ce processus.\nLe modèle cubique (ordre 3) est aussi choisi par la validation croisée, en moyenne (comme il l’était par le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\)). Le graphe qui suit trace les valeurs de l’estimation par validation croisée (courbe de validation croisée) et aussi le \\(\\mathsf{EQM}\\). On voit que l’estimation par validation croisée suit assez bien la forme du \\(\\mathsf{EQM}\\) (qu’il est supposé estimer). Les boîtes à moustache permettent d’apprécier la variabilité des estimés de l’erreur quadratique moyenne telles qu’estimée par validation croisée avec 10 groupes.\n\n\n\n\n\nFigure 5.5: Boîtes-à-moustaches des 100 réplications des valeurs de l’erreur quadratique moyenne estimées par validation croisée à 10 plis pour chaque ordre du polynôme."
  },
  {
    "objectID": "04-selectionmodeles.html#présentation-des-données",
    "href": "04-selectionmodeles.html#présentation-des-données",
    "title": "5  Sélection de variables et de modèles",
    "section": "5.4 Présentation des données",
    "text": "5.4 Présentation des données\nNous allons présenter un exemple classique de commercialisation de bases de données qui nous servira à illustrer la sélection de modèles, la régression logistique et la gestion de données manquantes. Le but est de cibler les clients pour l’envoi d’un catalogue.\nLe contexte est le suivant : une entreprise possède une grande base de données client. Elle désire envoyer un catalogue à ses clients mais souhaite maximiser les revenus d’une telle initiative. Il est évidemment possible d’envoyer le catalogue à tous les clients mais ce n’est possiblement pas optimal. La stratégie envisagée est la suivante :\n\nEnvoyer le catalogue à un échantillon de clients et attendre les réponses. Le coût de l’envoi d’un catalogue est de 10$.\nConstruire un modèle avec cet échantillon afin de décider à quels clients (parmi les autres) le catalogue devrait être envoyé, afin de maximiser les revenus.\n\nPlus précisément, on s’intéresse aux clients de 18 ans et plus qui ont au moins un an d’historique avec l’entreprise et qui ont effectué au moins un achat au cours de la dernière année. Dans un premier lieu, on a envoyé le catalogue à un échantillon de 1000 clients. Un modèle sera construit avec ces 1000 clients afin de cibler lesquels des clients restants seront choisis pour recevoir le catalogue.\nPour les 1000 clients de l’échantillon d’apprentissage, les deux variables cibles suivantes sont disponibles :\n\nyachat, une variable binaire qui indique si le client a acheté quelque chose dans le catalogue égale à 1 si oui et 0 sinon.\nymontant, le montant de l’achat si le client a acheté quelque chose.\n\nLes 10 variables suivantes sont disponibles pour tous les clients et serviront de variables explicatives pour les deux variables cibles. Il s’agit de :\n\nx1: sexe de l’individu, soit homme (0) ou femme (1);\nx2: l’âge (en année);\nx3: variable catégorielle indiquant le revenu, soit moins de 35 000$ (1), entre 35 000$ et 75 000$ (2) ou plus de 75 000$ (3);\nx4: variable catégorielle indiquant la région où habite le client (de 1 à 5);\nx5: couple : la personne est elle en couple (0=non, 1=oui);\nx6: nombre d’année depuis que le client est avec la compagnie;\nx7: nombre de semaines depuis le dernier achat;\nx8: montant (en dollars) du dernier achat;\nx9: montant total (en dollars) dépensé depuis un an;\nx10: nombre d’achats différents depuis un an.\n\nLes données se trouvent dans le fichier dbm. Voici d’abord des statistiques descriptives pour l’échantillon d’apprentissage.\n\n\nCode\ndata(dbm, package = \"hecmulti\")\nstr(dbm)\n## tibble [101,000 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ x1      : int [1:101000] 1 1 0 0 1 1 0 0 0 1 ...\n##  $ x2      : num [1:101000] 42 59 52 32 38 63 35 32 26 32 ...\n##  $ x3      : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 3 1 2 2 2 1 3 1 ...\n##  $ x4      : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 3 5 1 5 5 1 3 1 5 ...\n##  $ x5      : int [1:101000] 1 1 1 0 0 1 1 0 0 0 ...\n##  $ x6      : num [1:101000] 8.6 8.6 1.4 10.7 9.1 9.4 10.6 4.8 4 10.3 ...\n##  $ x7      : num [1:101000] 8 9 9 42 5 1 6 5 48 9 ...\n##  $ x8      : num [1:101000] 49 70 120 31 30 28 59 70 73 55 ...\n##  $ x9      : num [1:101000] 159 123 434 110 55 102 593 298 83 90 ...\n##  $ x10     : num [1:101000] 5 5 8 3 3 8 10 6 2 3 ...\n##  $ yachat  : int [1:101000] 0 0 0 0 0 0 0 1 1 1 ...\n##  $ ymontant: num [1:101000] NA NA NA NA NA NA NA 52 79 77 ...\n##  $ test    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\n\n\n\nTableau 5.2:  Tableaux de fréquence pour les variables catégorielles de la base de données marketing. \n\n  \n    \n\n\n \n  \n    sexe \n    décompte \n  \n \n\n  \n    0 \n    534 \n  \n  \n    1 \n    466 \n  \n\n\n\n \n    \n\n\n \n  \n    revenu \n    décompte \n  \n \n\n  \n    1 \n    397 \n  \n  \n    2 \n    337 \n  \n  \n    3 \n    266 \n  \n\n\n\n \n    \n\n\n \n  \n    couple \n    décompte \n  \n \n\n  \n    0 \n    575 \n  \n  \n    1 \n    425 \n  \n\n\n\n \n    \n\n\n \n  \n    région \n    décompte \n  \n \n\n  \n    1 \n    216 \n  \n  \n    2 \n    185 \n  \n  \n    3 \n    216 \n  \n  \n    4 \n    191 \n  \n  \n    5 \n    192 \n  \n\n\n\n \n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 5.6: Histogrammes des variables continues de la base de données dbm pour les 1000 clients par intention d’achat.\n\n\n\n\nIl y a 46.6% de femmes parmi les 1000 clients de l’échantillon. De plus, 39.7% ont un revenu de moins de 35 000$, 33.7% sont entre 35 000$ et 75 000$ et 26.6% ont plus de 75 000$. 42.5% de ces clients qui ont un conjoint.\n\n\n\nTableau 5.3:  Statistiques descriptives des variables numériques de la base de données marketing. \n \n  \n    variable \n    description \n    moyenne \n    écart-type \n    min \n    max \n  \n \n\n  \n    x2 \n    âge \n    37.06 \n    9.27 \n    20 \n    70 \n  \n  \n    x6 \n    nombre d’année comme client \n    6.01 \n    2.92 \n    1 \n    11 \n  \n  \n    x7 \n    nombre de semaines depuis le dernier achat \n    9.97 \n    9.34 \n    1 \n    52 \n  \n  \n    x8 \n    montant du dernier achat \n    48.41 \n    28.27 \n    20 \n    252 \n  \n  \n    x9 \n    montant total dépensé sur un an \n    229.27 \n    173.97 \n    22 \n    1407 \n  \n  \n    x10 \n    nombre d'achats différents sur un an \n    5.64 \n    2.31 \n    1 \n    14 \n  \n\n\n\n\n\n\nLe nombre d’achats différents depuis un an par ces clients varie entre 1 et 14. Un peu plus de la moitié (51.4%) ont fait cinq achats ou moins. Parmi les 1000 clients de l’échantillon d’apprentissage, 210 ont acheté quelque chose dans le catalogue. La variable yachat sera l’une des variables que nous allons chercher à modéliser en vue d’obtenir des prédictions.\nL’âge des 1000 clients de l’échantillon d’apprentissage varie entre 20 et 70 avec une moyenne de 37.1 ans. En moyenne, ces clients ont acheté pour 229.30$ depuis un an. Le dernier achat de ces clients remonte, en moyenne, à 10 semaines.\nDans cette section, nous modéliserons le montant d’achat, ymontant. Seuls 210 clients ont acheté quelque chose dans le catalogue et les statistiques rapportées correspondent seulement à ces derniers, car la variable ymontant est manquante si le client n’a rien acheté dans le catalogue. On pourrait également remplacer ces valeurs par des zéros et les modéliser, mais nous aborderons cet aspect ultérieurement. Les clients qui ont acheté quelque chose ont dépensé en moyenne 67.3$, et au minimum 25$. La Figure 5.6 présente les histogrammes de quelques unes de ces variables.\nIl y a plusieurs façons d’utiliser l’échantillon d’apprentissage afin de mieux cibler les clients à qui envoyer le catalogue et maximiser les revenus. En voici quelques unes.\n\nOn pourrait développer un modèle afin d’estimer la probabilité qu’un client achète quelque chose si on lui envoie un catalogue. Plus précisément, on peut développer un modèle pour \\(\\Pr(\\texttt{yachat}=1)\\). Comme la variable yachat est binaire, un modèle possible est la régression logistique, que nous décrirons au chapitre suivant. Ainsi, en appliquant le modèle aux 100 000 clients restant, on pourra cibler les clients susceptibles d’acheter (ceux avec une probabilité élevée).\nUne autre façon serait de tenter de prévoir le montant d’argent dépensé. Nous venons de voir la distribution de la variable ymontant. Il y a deux situations, ceux qui ont acheté et ceux qui n’ont pas achetés. En conditionnant sur le fait d’avoir acheté quelque chose, il est possible de décomposer le problème de la manière suivante :\n\n\\[\\begin{align*}\n\\mathsf{E}(\\texttt{ymontant}) &= \\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1) \\mathsf{P}(\\texttt{yachat}=1) \\\\& \\quad +\n\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=0) \\mathsf{P}(\\texttt{yachat}=0) \\\\\n&= \\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1) \\mathsf{P}(\\texttt{yachat}=1),\n\\end{align*}\\] puisque le terme \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=0)\\) est zéro: les gens qui n’ont pas acheté n’ont rien dépensé.\nOn peut donc estimer \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) et \\(\\mathsf{P}(\\texttt{yachat}=1)\\), pour ensuite les combiner et avoir une estimation de \\(\\mathsf{E}(\\texttt{ymontant})\\). Le développement du modèle pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) peut se faire avec la régression linéaire, en utilisant seulement les clients qui ont acheté dans l’échantillon d’apprentissage, car \\(\\texttt{ymontant}\\) est une variable continue dans ce cas. Le développement du modèle pour \\(\\mathsf{P}(\\texttt{yachat}=1)\\) peut se faire avec la régression logistique, tel que mentionné plus haut, en utilisant tous les 1000 clients de l’échantillon d’apprentissage. En fait, nous verrons plus loin qu’il est possible d’estimer conjointement les deux modèles avec un modèle Tobit. En appliquant le modèle aux 100 000 clients restants, on pourra cibler les clients qui risquent de dépenser un assez grand montant.\nComme nous n’avons pas encore vu la régression logistique, nous allons nous limiter à illustrer les méthodes qui restent à voir dans ce chapitre avec la régression linéaire en cherchant à développer un modèle pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\), le montant d’argent dépensé par les clients qui ont acheté quelque chose.\nLa base de donnée contient deux variables explicatives catégorielles. Il s’agit de revenu (x3) et région (x4). Il faut coder d’une manière appropriée afin de pouvoir les incorporer dans les modèles. La manière habituelle est de créer des variables indicatrices (binaires) qui indiquent si la variable prend ou non une valeur particulière dans R est de transformer la variable en facteur (factor). En général, si une variable catégorielle possède \\(K\\) valeurs possibles, il est suffisant de créer \\(K-1\\) indicatrices, en laissant une modalité comme référence. Par exemple, pour x3, nous allons créer deux variables,\n\nx31: variable binaire égale à 1 si x3 égale 1 et 0 sinon,\nx32: variable binaire égale à 1 si x3 égale 2 et 0 sinon.\n\nAinsi, la valeur 3 est celle de référence. Ces deux indicatrices sont suffisantes pour récupérer toute l’information comme le démontre le Tableau 5.4.\n\n\nTableau 5.4: Valeur des indicateurs en fonction du niveau de la variable catégorielle\n\n\nx3\nx31\nx32\n\n\n\n\n1\n1\n0\n\n\n2\n0\n1\n\n\n3\n0\n0\n\n\n\n\nIl est important de noter que, si le modèle qui inclut toutes les modalités (ordonnée à l’origine, x31 et x32) possibles ne dépend pas de la catégorie de référence, ce ne sera plus le cas si on permet lors de la sélection de variables de ne conserver que certains niveaux de la variable catégorielle. Par exemple, si on inclut uniquement x31 comme variable explicative, l’ordonnée à l’origine englobera toutes les autres valeurs de x3, à savoir \\(\\{2, 3\\}\\).1\n\n\n\n\n\n\nDanger de surajustement\n\n\n\nLa principale cause de mauvaise performance est le surajustement sélectif. Dans l’exemple que l’on considère avec la base de données marketing, la plupart des modalités des variables catégorielles semblent à première vues suffisantes pour estimer des coefficients. Si on s’intéresse par contre aux interactions, on se rendra rapidement compte qu’il y a trop peu de valeurs pour certaines combinaisons (par exemple, x3*x5) pour estimer de manière fiable l’effet combiné. Il suffit d’une valeurs aberrante pour fausser la sélection et donner une grande erreur quadratique moyenne de validation."
  },
  {
    "objectID": "04-selectionmodeles.html#sélection-de-variables",
    "href": "04-selectionmodeles.html#sélection-de-variables",
    "title": "5  Sélection de variables et de modèles",
    "section": "5.5 Sélection de variables",
    "text": "5.5 Sélection de variables\n\n5.5.1 Recherche exhaustive (meilleurs sous-ensembles)\nLorsque nous voulons comparer un petit nombre de modèles, il est relativement aisé d’obtenir les critères (\\(\\mathsf{AIC}\\), \\(\\mathsf{BIC}\\) ou autre) pour tous les modèles et de choisir le meilleur. C’était le cas dans l’exemple du choix de l’ordre du polynôme où il y avait seulement 10 modèles en compétitions. Mais lorsqu’il y a plusieurs variables en jeu, le nombre de modèles potentiel augmente très rapidement.\nEn fait, supposons qu’on a \\(p\\) variables distinctes disponibles. Avant même de considérer les transformations des variables et les interactions entre elles, il y a déjà trop de modèles possibles. En effet, chaque variable est soit incluse ou pas (deux possibilités) et donc il y a \\(2^p=2\\times 2 \\times \\cdots \\times 2\\) (\\(p\\) fois) modèles en tout à considérer. Ce nombre augmente très rapidement comme en témoigne le Tableau 5.5.\n\n\n\nTableau 5.5:  Nombres de modèles en fonction du nombre de paramètres. \n \n  \n    \\(p\\) \n    nombre de paramètres \n  \n \n\n  \n    5 \n    32 \n  \n  \n    10 \n    1024 \n  \n  \n    15 \n    32768 \n  \n  \n    20 \n    1048576 \n  \n  \n    25 \n    33554432 \n  \n  \n    30 \n    1073741824 \n  \n\n\n\n\n\n\nAinsi, si le nombre de variables est restreint, il est possible de comparer tous les modèles potentiels et de choisir le meilleur (selon un critère). II existe même des algorithmes très efficaces qui permettent de trouver le meilleur modèle sans devoir examiner tous les modèles possibles. Le nombre de variables qu’il est possible d’avoir dépend de la puissance de calcul et augmente d’année en année. Par contre, dans plusieurs applications, il ne sera pas possible de comparer tous les modèles et il faudra effectuer une recherche limitée. Faire une recherche exhaustive parmi tous les modèles possibles s’appelle sélection de tous les sous-ensembles (best subsets).\nOn veut trouver un bon modèle pour prévoir la valeur de ymontant des clients qui ont acheté quelque chose. On a vu qu’il y a 210 clients qui ont acheté dans l’échantillon d’apprentissage. Nous allons chercher à développer un « bon » modèle avec ces 210 clients. Dans ce premier exemple, nous allons seulement utiliser les 10 variables explicatives de base (14 variables avec les indicatrices).\nPour un nombre de variables fixé, le meilleur modèle selon le \\(R^2\\) est aussi le meilleur selon les critères d’information \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\), pour ce nombre fixé de variables. Pour vous convaincre de cette affirmation, fixons le nombre de variables et restreignons-nous seulement aux modèles avec ce nombre de variables. Comme \\(R^2=1 - \\mathsf{SCE}/\\mathsf{SCT}\\) et que \\(\\mathsf{SCT}\\) est une constante indépendante du modèle, le modèle avec le plus grand coefficient de détermination, \\(R^2\\), est aussi celui avec la plus petite somme du carré des erreurs (\\(\\mathsf{SCE}\\)). Comme \\(\\mathsf{AIC}=n \\ln (\\mathsf{EQM}) + 2p\\), ce sera aussi celui avec le plus petit \\(\\mathsf{AIC}\\) car la pénalité \\(2p\\) est la même si on fixe le nombre de variables; la même remarque est valide pour le \\(\\mathsf{BIC}\\).\nAinsi, pour trouver le meilleur modèle globalement (sans fixer le nombre de variables), il suffit de trouver le modèle à \\(k\\) variables explicatives ayant le coefficient de détermination le plus élevé pour tous les nombres de variables fixés et d’ensuite de trouver celui qui minimise le \\(\\mathsf{AIC}\\) (ou le \\(\\mathsf{BIC}\\)) parmi ces modèles. Ainsi, le modèle linéaire simple qui a le plus grand \\(R^2\\) est celui qui inclut l’indicateur de couple (x5). Le meilleur modèle (selon le \\(R^2\\)) parmi tous les modèles avec deux variables est celui avec x5 et x6.\n\n\n\nTableau 5.6:  Modèle parmi les candidats ayant la plus grande valeur de coefficient de détermination selon le nombre de régresseurs, avec valeurs des critères d’informations. \n \n  \n    variables \n    BIC \n    AIC \n  \n \n\n  \n    x5 \n    -96 \n    -103 \n  \n  \n    x5 x6 \n    -196 \n    -206 \n  \n  \n    x31 x5 x6 \n    -311 \n    -324 \n  \n  \n    x31 x5 x6 x10 \n    -351 \n    -368 \n  \n  \n    x1 x31 x5 x6 x10 \n    -369 \n    -389 \n  \n  \n    x1 x31 x5 x6 x7 x10 \n    -387 \n    -411 \n  \n  \n    x1 x31 x5 x6 x7 x8 x10 \n    -392 \n    -419 \n  \n  \n    x1 x31 x44 x5 x6 x7 x8 x10 \n    -391 \n    -421 \n  \n  \n    x1 x2 x31 x44 x5 x6 x7 x8 x10 \n    -390 \n    -424 \n  \n  \n    x1 x2 x31 x44 x5 x6 x7 x8 x9 x10 \n    -387 \n    -424 \n  \n  \n    x1 x2 x31 x43 x44 x5 x6 x7 x8 x9 x10 \n    -383 \n    -423 \n  \n  \n    x1 x2 x31 x41 x42 x43 x44 x5 x6 x7 x8 x10 \n    -378 \n    -422 \n  \n  \n    x1 x2 x31 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10 \n    -375 \n    -422 \n  \n\n\n\n\n\n\nUn algorithme par séparation et évaluation permet d’effectuer cette recherche de manière efficace sans essayer tous les candidats pour ces sous-ensembles. Dans l’exemple, on voit que le modèle avec les variables x1 x2 x31 x44 x5 x6 x7 x8 x9 et x10 est celui qui minimise le \\(\\mathsf{AIC}\\) globalement (\\(\\mathsf{AIC}\\) de -423.754). Le modèle choisi par le \\(\\mathsf{BIC}\\) contient seulement sept variables explicatives (plutôt que 10), soit x1 x31 x5 x6 x7 x8 x10.\n\n\nCode\ndata(dbm, package = \"hecmulti\")\ndbm_a <- dbm |>\n  dplyr::filter(test == 0,\n                !is.na(ymontant))\n# Conserver données d'entraînement (test == 0)\n# des personnes qui ont acheté ymontant > 0\n   \nrec_ex <- leaps::regsubsets(\n  x = ymontant ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, \n  nvmax = 13L,\n  method = \"exhaustive\",\n  data = dbm_a)\nresume_rec_ex <- summary(rec_ex,\n                         matrix.logical = TRUE)\n# Trouver le modèle avec le plus petit BIC\nmin_BIC <- which.min(resume_rec_ex$bic)\n# Nom des variables dans le modèle retenu\nrec_ex$xnames[resume_rec_ex$which[min_BIC,]]\n# Coefficients\n# coef(rec_ex, id = min_BIC)\n\n\nNous avons seulement inclus les variables de base pour ce premier essai. Il est possible qu’ajouter des variables supplémentaires améliore la performance du modèle. Pour cet exemple, nous allons considérer les variables suivantes2:\n\nles variables continues au carré, comme \\(\\texttt{age}^2\\).\ntoutes les interactions d’ordre deux entre les variables de base, comme \\(\\texttt{sexe}\\cdot\\texttt{age}\\).\n\nAux variables de base (10 variables explicatives, mais 14 avec les indicatrices pour les variables catégorielles), s’ajoutent ainsi 90 autres variables. Il y a donc 104 variables explicatives potentielles si on inclut les interactions et les termes quadratiques. Notez qu’il y a des interactions entre chacune des variables indicatrices et chacune des autres variables, mais il ne sert à rien de calculer une interaction entre deux indicatrices d’une même variable (car une telle variable est zéro pour tous les individus). De même, il ne sert à rien de calculer le carré d’une variable binaire codée \\(\\{0, 1\\}\\).\nDans la mesure où on aura un ratio d’environ un paramètre pour deux observations,Le modèle à 104 variables servira uniquement à illustrer le surajustement. Pensez à la taille de votre échantillon comme à un budget et aux paramètres comme à un nombre d’items: plus vous achetez d’items, moins votre budget est élevé pour chacun et leur qualité en pâtira. Réalistement, un modèle avec plus d’une vingtaine de variables ici serait difficilement estimable de manière fiable et l’inclusion d’interactions et de termes quadratiques sert surtout à augmenter la flexibilité et les possibilités lors de la sélection de variables.\n\n\nCode\n# (...)^2 crée toutes les interactions d'ordre deux\n# I(x^2) permet de créer les termes quadratiques\nformule <- \n  formula(ymontant ~ \n          (x1 + x2 + x3 + x4 + x5 + \n             x6 + x7 + x8 + x9 + x10)^2 + \n            I(x2^2) + I(x6^2) + I(x7^2) +\n            I(x8^2) + I(x9^2) + I(x10^2))\nmod_complet <- lm(formule, data = dbm_a)\n\n\nLancer une sélection exhaustive de tous les sous-modèles avec 104 variables risque de prendre un temps énorme. Que faire alors? Il y a plusieurs possibilités. Nous pourrions faire une recherche limitée avec les méthodes que nous allons voir à partir de la section suivante. Nous pourrions aussi combiner les deux approches. Supposons que notre ordinateur permet de faire une recherche exhaustive de tous les sous-modèles avec 40 variables. Nous pourrions alors commencer avec une recherche limitée pour trouver un sous-ensemble de 40 « bonnes » variables et faire une recherche exhaustive, mais en se restraignant à ces 40 variables.\n\n\n5.5.2 Méthodes séquentielles de sélection\nLes méthodes de sélection ascendante, descendante et séquentielle sont des algorithmes gloutons. Elles ont été développées à une époque où la puissance de calcul était bien moindre, et où il était impossible de faire une recherche exhaustive des sous-modèles. La procédure leaps::regsubsets permet une sélection de modèle avec une approche séquentielle, ascendante ou descendante en choisissant le meilleur modèle (côté ajustement) avec \\(k\\) variables \\((k=1, \\ldots, k_{\\text{max}})\\). La procédure MASS::stepAIC permet de faire cette sélection en utilisant un critère d’information.\nL’idée de la sélection ascendante est d’ajuster à chaque étape au modèle précédent la variable qui améliore le plus l’ajustement. Le modèle de départ est celui qui n’inclut que l’ordonnée à l’origine (aucune variable explicative). À chaque étape, on ajoute la variable qui améliore le plus le critère d’ajustement jusqu’à ce qu’aucune amélioration ne soit résultante.\nUn algorithme glouton résoud un problème d’optimisation étape par étape: après \\(k\\) étapes, le modèle construit par la procédure n’est pas nécessairement le meilleur modèle (si on essayait toutes les combinaisons). Si on commence avec \\(p\\) variables, on regarde \\(p\\) choix à la première étape de la procédure ascendante, puis on choisit nue variable parmi les \\(p-1\\) restantes à la deuxième étape, etc. La procédure exhaustive essaiera toutes les \\(\\binom{p}{2}\\) combinaisons possibles3: puisque plus de modèles sont essayés, la solution finale est nécessairement meilleur pour l’échantillon d’apprentissage.\nLa sélection descendante est similaire, sauf qu’on part avec le modèle qui inclut toutes les variables explicatives. À chaque étape, on retire la variable qui contribue le moins à l’ajustement jusqu’à ce que le critère d’ajustement ne puisse plus être amélioré ou jusqu’à ce qu’on recouvre le modèle sans variables explicatives, selon le scénario. C’est l’inverse de la méthode ascendante: on va tester le retrait de chaque variable individuellement et retirer celle qui est la moins significative.\nLa méthode de sélection séquentielle est un hybride entre les méthodes de sélection ascendantes et descendante. On débute la recherche à partir du modèle ne contenant que l’ordonnée à l’origine. À chaque étape, on fait une étape ascendante suivie de une (ou plusieurs) étapes descendantes. On continue ainsi tant que le modèle retourné par l’algorithme n’est pas identique à celui de l’étape précédente (dépendant de notre critère). Le dernier modèle est celui retenu.\nAvec la méthode séquentielle, une fois qu’on entre une variable (étape ascendante), on fait autant d’étapes descendante afin de retirer toutes les variables qui satisfont le critère de sortie (il peut ne pas y en avoir). Une fois cela effectué, on refait une étape ascendante pour voir si on peut ajouter une nouvelle variable.\nAvec la méthode ascendante, une fois qu’une variable est dans le modèle, elle y reste. Avec la méthode descendante, une fois qu’une variable est sortie du modèle, elle ne peut plus y entrer. Avec la méthode séquentielle, une variable peut entrer dans le modèle et sortir plus tard dans le processus. Par conséquent, parmi les trois, la méthode séquentielle est généralement préférable aux méthodes ascendante et descendante, car elle inspecte potentiellement un plus grand nombre de modèles.\n\n\nCode\n# Cette procédure séquentielle retourne\n# la liste de modèles de 1 variables à\n# nvmax variables.\nrec_seq <- \n  leaps::regsubsets(\n    x = formule, \n    data = dbm_a,\n    method = \"seqrep\", \n    nvmax = length(coef(mod_complet)))\nwhich.min(summary(rec_seq)$bic)\n\n# Alternative avec procédure séquentielle\n# qui utilise le critère AIC pour déterminer \n# l'inclusion ou l'exclusion de variables\n# \n# Procédure plus longue à rouler\n# (car les modèles linéaires sont ajustés)\n# \n# On ajoute ou retire la variable qui\n# améliore le plus le critère de sélection\n# à chaque étape.\nseq_AIC <- MASS::stepAIC(\n  lm(ymontant ~ 1, data = dbm_a), \n  # modèle initial sans variables explicative\n    scope = formule, # modèle maximal possible\n    direction = \"both\", #séquentielle\n    trace = FALSE, # ne pas imprimer le suivi\n    keep = function(mod, AIC, ...){ \n      # autres sorties des modèles à conserver\n      list(bic = BIC(mod), \n           coef = coef(mod))},\n    k = 2) #\n# Remplacer k=2 par k = log(nrow(dbm_a)) pour BIC\n\n# L'historique des étapes est disponible via\n# seq_AIC$anova\n\n\nLa procédure exhaustive est préférable aux méthodes séquentielles si le nombre de variables n’est pas trop élevé. S’il y a trop de variables, rien ne nous empêche de combiner plusieurs méthodes: on pourrait par exemple faire une procédure descendante pour ne conserver que 40 variables. En utilisant seulement ce sous-ensemble de variables, on choisit le meilleur modèle selon le \\(\\mathsf{AIC}\\) ou le \\(\\mathsf{BIC}\\) en faisant une recherche exhaustive de tous les sous-modèles. On pourrait également faire une recherche séquentielle avec le \\(\\mathsf{AIC}\\) et choisir le modèle parmi l’historique avec le plus petit \\(\\mathsf{BIC}\\).\n\n\n\n\n\n\n\n\nFigure 5.7: Critères d’information et estimation de l’erreur quadratique moyenne de validation externe et de validation croisée (10 groupes) pour les 40 premiers modèles de la procédure descendante, selon le nombre de termes inclus dans la régression linéaire. Les traitillés verticaux indiquent le nombre de terme du modèle avec la meilleure valeur du critère pour chaque méthode.\n\n\n\n\nOn peut voir sur la Figure 5.7 l’historique des valeurs de AIC et BIC à mesure qu’on augmente le nombre de variables dans le modèle obtenu par une procédure séquentielle: les mêmes variables sont enlevées à chaque étape, mais la valeur optimale du critère est différente pour la sélection finale. Sur l’axe des abscisses, j’ai ajouté l’erreur quadratique moyenne de l’échantillon de validation pour les clients avec ymontant positif. Cet exemple n’est pas réaliste puisqu’on regarde la solution, mais il permet de nous comparer et de voir à quel point ici le critère d’information bayésien suit la même tendance que l’erreur quadratique moyenne de validation. L’erreur quadratique moyenne obtenue par validation croisée est trop optimiste (mais aléatoire!), comme le AIC. Pour éviter le surentraînement dans une région où le critère est quasi constant, on peut utiliser la règle d’une erreur-type. Puisque on a plusieurs réplications, on peut estimer ce dernier avec la validation croisée en même temps que l’EQM et choisir le modèle le plus simple à distance une erreur-type du modèle avec la plus petite erreur de validation croisée.\n\n\n5.5.3 Méthodes de régression avec régularisation\nUne façon d’éviter le surajustement est d’ajouter une pénalité sur les coefficients: ce faisant, on introduit un biais dans nos estimés, mais dans l’espoir de réduire leur variabilité et ainsi d’obtenir une meilleur erreur quadratique moyenne.\nL’avantage des moindres carrés est que les valeurs ajustées et les prédictions ne changent pas si on fait une transformation affine (de type \\(Z = aX+b\\)). Peu importe le choix d’unité (par exemple, exprimer une distance en centimètres plutôt qu’en mètres, ou la température en Farenheit plutôt qu’en Celcius), on obtient le même ajustement. En revanche, une fois qu’on introduit un terme de pénalité, notre solution dépendra de l’unité de mesure, d’où l’importance d’utiliser les données centrées et réduites pour que la solution reste la même.\nLes estimateurs des moindres carrés ordinaires pour la régression linéaire représentent la combinaison qui minimise la somme du carré des erreurs, \\[\\begin{align*}\n\\mathsf{SCE} = \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^pX_{ij}\\beta_{j}\\right)^2.\n\\end{align*}\\] On peut ajouter à cette fonction objective \\(\\mathsf{SCE}\\) un terme additionnel de pénalité qui va contraindre les paramètres à ne pas être trop grand. On considère une pénalité additionnelle pour la valeur absolue des coefficients, [ q_1() = _{j=1}^p |_j|. ] Pour chaque valeur de \\(\\lambda\\) donnée, on obtiendra une solution différente pour les estimés car on minimisera désormais \\(\\mathsf{SCE} + q_1(\\lambda)\\). On ne pénalise pas l’ordonnée à l’origine \\(\\beta_0\\), parce que ce coefficient sert à recentrer les observations et a une signification particulière: si on standardise les données, de manière à ce que leur moyenne empirique soit zéro et leur écart-type un, alors \\(\\widehat{\\beta}_0 = \\overline{y}\\).\nLa pénalité \\(q_1(\\lambda)\\) a un rôle particulier parce qu’elle a deux effets: elle réduit la taille des paramètres, mais elle force également certains paramètres très proches de zéro à être exactement égaux à zéro, ce qui fait que la régression pénalité agit également comme outil de sélection de variables. Des algorithmes efficaces permettent de trouver la solution du problème d’optimisation \\[\\begin{align*}\n\\min_{\\boldsymbol{\\beta}} \\{\\mathsf{SCE} + q_1(\\lambda)\\} = \\min_{\\boldsymbol{\\beta}}  \\left\\{\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^pX_{ij}\\beta_{j}\\right)^2 +\n\\lambda \\sum_{j=1}^p |\\beta_j|\\right\\}\n\\end{align*}\\] laquelle est appelée LASSO. La Figure 5.8 montre la fonction objective dans le cas où on a deux paramètres, \\(\\beta_1\\) et \\(\\beta_2\\). La solution des moindres carrés ordinaires, qui minimisent l’erreur quadratique moyenne, est au centre des ellipses de contour et correspond à la solution du modèle avec \\(\\lambda=0\\). À mesure que l’on augmente la pénalité \\(\\lambda\\), les coefficients rétrécissent vers \\((0, 0)\\). On peut interpréter la pénalité \\(l_1\\) comme une contraire budgétaire: les coefficients estimés pour une valeur de \\(\\lambda\\) donnée sont ceux qui minimisent la somme du carré des erreurs, mais doivent être à l’intérieur d’un budget alloué (losange). La forme de la région fait en sorte que la solution, qui se trouve sur la bordure du losange, intervient dans un coin avec certaines coordonnées nulles.\n\n\n\n\n\nFigure 5.8: Courbes de contour du critère de l’erreur quadratique moyenne (ellipses) et fonction de pénalité (losanges) pour différentes valeurs de \\(\\lambda\\). Les points dénotent des solutions différentes et intersectent les contours du losange.\n\n\n\n\nPlusieurs variantes existent dans la littérature qui généralisent le modèle à des contextes plus compliqués. Le choix des variables à inclure dans la sélection dépend du choix de la pénalité \\(\\lambda\\), qui est règle générale estimée par validation croisée à cinq ou 10 groupes.\n\n\nCode\n# Sélection par LASSO\nlibrary(glmnet)\n# Paramètre de pénalité déterminé par \n# validation croisée à partir d'un vecteur\n# de valeurs candidates\nlambda_seq <- seq(0.1,2, by = 0.01)\n\ncv_output <- \n  glmnet::cv.glmnet(x = as.matrix(dbm_a[,1:10]), \n            y = dbm_a$ymontant, \n            alpha = 1, \n            lambda = lambda_seq)\nplot(cv.output)\n\n# On réestime le modèle avec la pénalité\nlambopt <- cv_output$lambda.min\nlasso_best <- \n  glmnet::glmnet(\n    x = as.matrix(dbm_a[,1:10]),\n    y = dbm_a$ymontant,\n    alpha = 1, \n    lambda = lambopt)\n# Prédictions et calcul de l'EQM\n# On pourrait remplacer `newx` par \n# d'autres données (validation externe)\npred <- predict(lasso_best, \n                s = lambopt, \n                newx = as.matrix(dbm_a[,-1]))\neqm_lasso <- mean((pred - dbm_a$ymontant)^2)\n\n\nLe graphique de la Figure 5.9 montre l’évolution de l’erreur quadratique moyenne estimée en fonction du logarithme naturel de la pénalité (axe des abscisses). Comme plusieurs pénalités sont dans la marge d’erreurs, on choisit le première modèle à un erreur-type de la valeur minimale\n\n\n\n\n\nFigure 5.9: Estimation de l’erreur quadratique moyenne (validation croisée à 10 groupes) pour les modèles avec pénalité LASSO en fonction de la pénalité (échelle log).\n\n\n\n\n\n\n5.5.4 Moyenne de modèles\nIl est souvent préférable de combiner plusieurs modèles plutôt que d’en choisir un seul. La technique des forêts aléatoires (random forests) est une des meilleures techniques de prédiction disponibles de nos jours. Elle est basée sur cette idée, en combinant plusieurs arbres de classification (ou de régression) individuels. C’est une des techniques de base en exploitation de données.\nIci, nous allons voir comment cette idée peut être appliquée à notre contexte. Toutes les méthodes que nous avons vues jusqu’à maintenant font une sélection « rigide » de variables, dans le sens que chaque variable est soit sélectionnée pour faire partie du modèle, soit elle ne l’est pas. C’est donc tout ou rien pour chaque variable. Il y a beaucoup de variabilité associée à une telle forme de sélection. Une variable peut avoir été très près d’être choisie, mais elle ne l’a pas été et est éliminée complètement. Construire plusieurs modèles et en faire la moyenne permet d’adoucir le processus de sélection car une variable peut alors être partiellement sélectionnée.\nSupposons qu’on dispose de deux échantillons et qu’on fasse une sélection de variables séparément pour les deux échantillons, avec l’une des approches que nous avons vues jusqu’à maintenant. Il est alors très probable qu’on ne va pas avoir exactement les mêmes variables sélectionnées pour les deux échantillons. Supposons ensuite qu’on fasse la moyenne des coefficients pour les deux modèles. Si une variable, disons \\(X_1\\), a été choisie les deux fois, alors la moyenne des deux coefficients devrait estimer en quelque sorte un effet global pour cette variable. Si une autre variable, disons \\(X_2\\), n’a pas été choisie du tout pour les deux échantillons, alors la moyenne de ses deux coefficients est nulle. Mais si une variable, disons, \\(X_3\\), a été choisie pour seulement l’un des deux échantillons, alors la moyenne de ses deux coefficients est la moitié du coefficient pour le modèle dans lequel elle a été choisie (car l’autre est zéro). Ainsi, cette variable est donc représentée par une « moitié » d’effet dans la moyenne des modèles. Donc au lieu d’être totalement là ou totalement absente, elle est présente en fonction de sa probabilité d’être sélectionnée. Ceci diminue de beaucoup la variabilité engendrée par une sélection « rigide » de variables et permet souvent de produire un modèle fort raisonnable.\nLe problème est que l’on n’a pas plusieurs échantillons mais un seul. Une solution possible est de générer nous-mêmes des échantillons différents à partir de l‘échantillon original. Cela peut être fait avec l’autoamorçage (bootstrap). Un échantillon d’autoamorçage est tout simplement un échantillon choisi au hasard et avec remise dans l’échantillon original. Ainsi, une même observation peut être sélectionnée plus d’une fois tandis qu’une autre peut ne pas être sélectionnée du tout.\nL’idée est alors la suivante :\n\nGénérer plusieurs échantillons par autoamorçage nonparamétrique à partir de l‘échantillon original.\nFaire une sélection de variables pour chaque échantillon.\nFaire la moyenne des paramètres de ces modèles.\n\n\n\nCode\ndata(dbm, package = \"hecmulti\")\ndbm_a <- dbm |>\n  dplyr::filter(test == 0,\n                !is.na(ymontant)) |>\n  dplyr::mutate(x3 = factor(x3),\n                x4 = factor(x4))\n\n# Moyenne de modèles\nmoyenne_modeles <- function(\n    data, \n    form, \n    aic = FALSE, \n    B = 100L,\n    ks = 2){\n  B <- as.integer(B)\n  stopifnot(is.logical(aic),\n            length(aic) == 1L,\n            inherits(form, \"formula\"),\n            B > 1,\n            ks >= 0,\n            inherits(data, \"data.frame\"))\n  N <- nrow(data)\n  # Faire une expansion pour obtenir colonnes\n  matmod <- model.matrix(form, data = data)\n  # Nombre de variables explicatives\n  p <- ncol(matmod) - 1L\n  # Formule du modèle complet\n  fmod <- formula(paste0(\"y ~\", paste0(\"x\", seq_len(p), collapse = \"+\")))\n  # Sauvegarder les noms\n  noms <- colnames(matmod)\n  xnoms <- paste0(\"x\", seq_len(p))\n  # Extraire le nom de la variable réponse\n  nom_reponse <- all.vars(form)[attr(terms(form), \"response\")]\n  # Créer une base de données avec la réponse\n  # moins l'ordonnée à l'origine\n  matmod <- data.frame(cbind(\n    y = get(nom_reponse, data), \n    matmod[,-1]))\n  colnames(matmod) <- c(\"y\", xnoms)\n    # Contenant pour params/ nb de sélections\n  params <- nselect <- rep(0, p + 1)\n  names(params) <- \n    names(nselect) <- \n    c(\"(Intercept)\", xnoms)\n\n  # Boucle\n  for(b in seq_len(B)){\n    # Procédure de sélection avec AIC ou BIC\n  modselect <- MASS::stepAIC(\n    # Valeurs de départ\n    object = lm(formula = y ~ 1,\n   # Rééchantillonner données (avec remplacement)\n       data = matmod[sample.int(n = N, \n                             size = N, \n                             replace = TRUE),]),\n    # Modèle maximal additif considéré\n    scope = fmod, \n    # pénalité pour critère d'information\n    # k = ifelse(aic, 2, log(N)), \n    direction = \"both\", \n    trace = FALSE,\n   keep = function(mod, AIC, ...){ \n      # autres sorties des modèles à conserver\n      list(IC = AIC(mod, k = ifelse(aic, 2, log(N))),\n           coef = coef(mod))},\n   k = ks)\n  min_IC <- which.min(unlist(modselect$keep['IC',]))\n  coefsv <- modselect$keep[2,min_IC]$coef\n  # Trouver quelles colonnes représentent\n  #  un coefficient non-nul\n  colind <- match(names(coefsv),\n                  names(params))\n  # Incrémenter paramètres non-nuls\n  params[colind] <- params[colind] +\n    as.numeric(coefsv)\n  nselect[colind] <- nselect[colind] + 1L\n  }\n  \n  names(nselect) <- noms\n  names(params) <- noms\n  return(list(coefs = params / B,\n              nselect = nselect[-1] / B))\n}\n\n\n# Moyenne de modèles\n#  procédure séquentielle ascendante (AIC) \n#  sélection de modèle selon BIC\nmmodeles <- \n  moyenne_modeles(\n    data = dbm_a, \n    form = formule,\n    B = 10L,\n    aic = FALSE)\n\n# Proportion des variables \n# sélectionnées dans plus d'un modèle\nsum(mmodeles$nselect > 0)\n# Nombre moyen de coefficients\nsum(mmodeles$nselect)\n# variables retenues plus de 20% du temps\nnames(which(mmodeles$nselect > 0.2))\n# moyenne des coefficients\nmmodeles$coefs\n\n\nChaque modèle est construit à l’aide d’un échantillon aléatoire avec remise. Utilisez set.seed pour fixer le générateur de nombre aléatoire et permettre la reproductibilité\nToutes les méthodes employées jusqu’à maintenant utilise une méthode de pénalisation pour déterminer le meilleur modèle. Une alternative avec serait de répéter la sélection en utilisant directement l’erreur quadratique moyenne estimée à l’aide de la validation croisée comme critère de sélection: pour cela, il faudrait ajuster l’ensemble des modèles candidats retournés par une procédure exhaustive ou séquentielle."
  },
  {
    "objectID": "04-selectionmodeles.html#évaluation-de-la-performance",
    "href": "04-selectionmodeles.html#évaluation-de-la-performance",
    "title": "5  Sélection de variables et de modèles",
    "section": "5.6 Évaluation de la performance",
    "text": "5.6 Évaluation de la performance\nLa direction de la compagnie a décidé de passer outre vos recommandations et d’envoyer le catalogue aux 100 000 clients restants; nous pouvons donc faire un post-mortem afin de voir ce que chaque modèle aurait donné comme profit, comparativement à la stratégie de référence. Les 100 000 autres clients serviront d’échantillon de validation pour évaluer la performance des modèles et, plus précisément, afin d’évaluer les revenus (ou d’autres mesures de performance) si ces modèles avaient été utilisés. L’échantillon de validation nous donnera donc l’heure juste quant aux mérites des différentes approches que nous allons comparer. En pratique, nous ne pourrions pas faire cela car la valeur de la variable cible ne serait pas connue pour ces clients et nous utiliserions plutôt les modèles pour obtenir des prédictions pour déterminer quels clients cibler avec l’envoi. Parmi, les 100 000 clients restants, il y en a 23 179 qui auraient acheté quelque chose si on leur avait envoyé le catalogue. Ces 23 179 observations vont nous servir pour estimer l’erreur quadratique moyenne (théorique) des modèles retenus par nos critères.\nCommençons par l’estimation de l’erreur quadratique moyenne (moyenne des carrés des erreurs) pour les deux modèles retenus par le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\) avec les variables de base. Le Tableau 5.7 contient aussi l’estimation de l’erreur quadratique moyenne si on utilise toutes les variables (14 en incluant les indicatrices) sans faire de sélection. On voit que le modèle choisi par le \\(\\mathsf{BIC}\\) est le meilleur des trois. Ces deux méthodes font mieux que le modèle qui inclut toutes les variables sans faire de sélection, mais nous verrons que leur performance est exécrable: les variables de base ne permettent pas de capturer les effets présents dans les données et ce manque de flexibilité coûte cher.\n\n\nTableau 5.7: Estimation de l’erreur quadratique moyenne sur l’échantillon test avec les variables de base. Les meilleurs modèles selon les critères d’informations découlent d’une recherche exhaustive de tous les sous-ensembles.\n\n\nnombre de variables\n\\(\\mathsf{EQM}\\)\nméthode\n\n\n\n\n15\n25.69\ntoutes les variables\n\n\n12\n25.53\nexhaustive - \\(\\mathsf{AIC}\\)\n\n\n10\n25.04\nexhaustive - \\(\\mathsf{BIC}\\)\n\n\n\n\n\n\nTableau 5.8: Comparaison des méthodes selon l’erreur quadratique moyenne avec les variables de base, les interactions et les termes quadratiques.\n\n\nnombre de variables\n\\(\\mathsf{EQM}\\)\nméthode\n\n\n\n\n104\n19.63\ntoutes les variables\n\n\n21\n12\nséquentielle ascendante, choix selon \\(\\mathsf{BIC}\\)\n\n\n15\n12.31\nséquentielle ascendante, choix selon \\(\\mathsf{AIC}\\)\n\n\n23\n12.75\nséquentielle ascendante avec critère \\(\\mathsf{AIC}\\)\n\n\n20\n12.4\nséquentielle descendante avec critère \\(\\mathsf{BIC}\\)\n\n\n30\n12\nLASSO, validation croisée avec 10 groupes\n\n\n\n\nLe Tableau 5.8 présente la performance de toutes les méthodes avec les autres variables. On voit d’abord qu’utiliser toutes les 104 variables sans faire de sélection fait mieux (\\(\\mathsf{EQM}\\) de 19.63) que les modèles précédents basés sur les 10 variables originales. Mais faire une sélection permet une amélioration très importante de la performance (\\(\\mathsf{EQM}\\) jusqu’à 12 dans l’exemple). Utiliser les 104 variables mène à du surajustement (over-fitting).\nLes méthodes séquentielles avec un critère d’information (qui pénalisent davantage que les tests d’hypothèse classique) mènent à des modèles plus parcimonieux qui ont une erreur quadratique moyenne de validation plus faible. Le LASSO performe très bien dans ce cas de figure. Les coefficients sont tous rétrécis vers zéro (donc le nombre de coefficients non-nuls n’est pas évocateur), ce qui engendre du biais et peut affecter négativement la performance si le rapport signal-bruit est élevé.\nIl faut bien comprendre qu’il ne s’agit que d’un seul exemple: il ne faut surtout pas conclure que la méthode séquentielle sera toujours la meilleure. En fait, il est impossible de prévoir quelle méthode donnera les meilleurs résultats.\nIl y aurait plusieurs autres approches/combinaisons qui pourraient être testées. Le but de ce chapitre était simplement de présenter les principes de base en sélection de modèles et de variables ainsi que certaines approches pratiques. Il y a d’autres approches intéressantes, tels le filet élastique. Ces méthodes sont dans la même mouvance moderne que celle qui consiste à faire la moyenne de plusieurs modèles, en performant à la fois une sélection de variables et en permettant d’avoir des parties d’effet par le rétrécissement (shrinkage). De récents développements théoriques permettent aussi de corriger les valeurs-p pour faire de l’inférence post-sélection avec le LASSO.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nEn présence de nombreuses variables explicatives, choisir un modèle prédictif est compliqué: le nombre de modèles possibles augmente rapidement avec le nombre de prédicteurs, \\(p\\).\nSi un modèle est mal spécifié (variables importantes manquantes), alors les estimations sont biaisées. Si le modèle est surspécifié, les coefficients correspondants aux variables superflues incluses sont en moyenne nuls, mais contribuent à l’augmentation de la variance (compromis biais/variance).\nLa taille du modèle (\\(p\\), le nombre de variables explicatives) est restreinte par le nombre d’observations disponibles, \\(n\\).\n\nEn général, il faut s’assurer d’avoir suffisamment d’observations pour estimer de manière fiable les coefficients (le rapport \\(n/p\\) donne le budget moyen par paramètre).\nPorter une attention particulière aux variables binaires et aux interactions avec ces dernières: si les effectifs de certaines modalités sont faibles, il y a possible de surajustement.\n\nLe principal critère pour juger de la qualité d’un modèle linéaire est l’erreur quadratique moyenne.\n\nL’estimation de l’erreur quadratique moyenne obtenue à partir de l’échantillon d’apprentissage (qui sert à estimer les paramètres) est trompeuse et mène au surajustement:\nplus le modèle est compliqué, plus cette erreur décroît.\ncette performance n’est pas répétée sur de nouvelles données.\n\nCritères de sélection: Plusieurs stratégies existent pour pallier à cet excès d’optimisme\n\nvalidation externe: diviser le jeu de données aléatoirement au préalable en deux ou trois. Nécessite une grande base de données, potentiellement sous-optimal.\nvalidation croisée: diviser aléatoirement le jeu de données en plis et varier les échantillons d’apprentissage en conservant un pli en réserve à chaque fois comme validation. Plus coûteux en calcul (il faut réajuster plusieurs fois les modèles), applicable avec des petites bases de données.\npénalisation a posteriori: ajouter une pénalité fonction du nombre de paramètres qui compense pour l’augmentation constante de l’ajustement (par ex., critères d’information).\nrétrécissement des coefficients: inclure dans la fonction objective qui est maximisée une pénalité qui contraint les paramètres et les force à demeurer petit. Cela introduit du biais pour réduire la variance.\nUne pénalité particulière (LASSO) contraint certains paramètres à être exactement nuls, ce qui correspond implicitement à une sélection de variables.\n\nEn pratique, on cherche à essayer plusieurs modèle pour trouver un choix optimal de variables.\n\nUne recherche exhaustive garantie le survol du plus grand nombre de modèles possibles, mais est coûteuse et limitée à moins de 50 variables.\nLes algorithmes gloutons de recherche séquentielle sont sous-optimaux, mais rapides\n\nOn applique le critère de sélection sur la liste de modèles candidats pour retenir celui qui donne la meilleure performance.\nPour éviter une sélection rigide, on peut perturber les données et répéter la procédure pour calculer une moyenne de modèles. Cette approche est très coûteuse en calcul."
  },
  {
    "objectID": "05-reglogistique.html",
    "href": "05-reglogistique.html",
    "title": "6  Régression logistique",
    "section": "",
    "text": "\\[\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\eps}{\\varepsilon}\n\\newcommand{\\Rlang}{\\textsf{R}}\n\\newcommand{\\SAS}{\\textsf{SAS}}\n\\newcommand{\\Sp}{\\mathscr{S}}\n\\renewcommand{\\P}[1]{{\\mathsf P}\\left(#1\\right)}\n\\newcommand{\\E}[1]{{\\mathsf E}\\left(#1\\right)}\n\\newcommand{\\Va}[1]{{\\mathsf{Var}}\\left(#1\\right)}\n\\newcommand{\\Cor}[1]{{\\mathsf{Cor}}\\left(#1\\right)}\n\\newcommand{\\I}[1]{{\\mathbf 1}_{#1}}\n\\newcommand{\\expit}{\\mathrm{expit}}\n\\newcommand{\\logit}{\\mathrm{logit}}\n\\newcommand{\\code}[1]{\\texttt{#1}}\n\\newcommand{\\Hy}{\\mathcal{H}}\n\\renewcommand{\\d}{\\mathrm{d}}\n\\]"
  },
  {
    "objectID": "05-reglogistique.html#introduction",
    "href": "05-reglogistique.html#introduction",
    "title": "6  Régression logistique",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nEn régression linéaire, on cherche à expliquer le comportement d’une variable quantitative \\(Y\\) que l’on peut traiter comme étant continue (elle peut prendre suffisamment de valeurs différentes).\nSupposons à présent que l’on veut expliquer le comportement d’une variable \\(Y\\) prenant seulement deux valeurs que l’on va noter 0 et 1.\nExemples :\n\nEst-ce qu’un client potentiel va répondre favorablement à une offre promotionnelle?\nEst-ce qu’un client est satisfait du service après-vente?\nEst-ce qu’un client va faire faillite ou non au cours des trois prochaines années.\n\nEn général, on cherchera à expliquer le comportement d’une variable binaire \\(Y\\) en utilisant un modèle basé sur p variables quelconques \\(\\mathrm{X}_1, \\ldots, \\mathrm{X}_p\\).\nNotre but sera de faire de l’inférence, de la prédiction, ou les deux à la fois, soit\n\nComprendre comment et dans quelles mesures les variables \\(\\mathbf{X}\\) influencent \\(Y\\) (ou bien la probabilité que \\(Y=1\\)).\nPrédiction : développer un modèle pour prévoir des valeurs de \\(Y\\) futures à partir des variables \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "05-reglogistique.html#modèle-de-régression-logistique",
    "href": "05-reglogistique.html#modèle-de-régression-logistique",
    "title": "6  Régression logistique",
    "section": "6.2 Modèle de régression logistique",
    "text": "6.2 Modèle de régression logistique\nAvec une variable réponse continue, le modèle de régression linéaire, \\[\\begin{align*}\nY = \\beta_0 + \\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p \\mathrm{X}_p + \\varepsilon,\n\\end{align*}\\] avec \\(\\E{\\varepsilon \\mid \\mathbf{X}}=0\\) et \\(\\Va{\\varepsilon \\mid \\mathbf{X}}=\\sigma^2\\), peut être écrit de manière équivalente comme \\(\\E{Y \\mid \\mathbf{X}} = \\beta_0 + \\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p\\mathrm{X}_p\\) et \\(\\Va{Y \\mid \\mathbf{X}}=\\sigma^2.\\)\nSi \\(Y\\) est binaire (0/1), on peut facilement vérifier que \\[\\begin{align*}\n\\E{Y \\mid \\mathbf{X}} = \\P{Y=1 \\mid  \\mathbf{X}},\n\\end{align*}\\] soit la probabilité que \\(Y\\) égale 1 étant donné les valeurs des variables explicatives. Pour simplifier la notation, posons \\(p = \\P{Y=1 \\mid \\mathbf{X}}\\) en se rappelant que \\(p\\) est une fonction des variables explicatives.\nÀ première vue, on peut se demander pourquoi ne pas utiliser le même modèle que la régression linéaire, c’est-à-dire \\[\\begin{align*}\n\\eta=\\beta_0 + \\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p \\mathrm{X}_p.\n\\end{align*}\\]\n\n\n\n\n\nFigure 6.1: Données de la réserve de Boston sur l’approbation de prêts hypothécaires (1990); données tirées de Stock et Watson (2007).\n\n\n\n\nLa Figure 6.1 montre le modèle de régression linéaire (bleu) et le modèle logistique. La pente pour la ligne bleu correspond à l’augmentation (réputée constante) de la probabilité d’approbation de crédit, de l’ordre de 11% par augmentation de 0.1 du rapport paiements hypothécaires sur revenu.\nIl y a quelques problèmes avec le modèle linéaire. D’abord, les données binaires ne respectent pas le postulat d’égalité des variances, ce qui rend les tests d’hypothèses caducs. Le problème principal est que \\(p\\) est une probabilité. Par conséquent \\(p\\) prend seulement des valeurs entre 0 et 1 alors que rien n’empêche \\(\\eta\\) de prendre des valeurs dans \\(\\mathbb{R}=(-\\infty, \\infty)\\): par exemple, on voit que la droite de la Figure 6.1 retourne des prédictions négatives dès que le ratio paiements/revenus est en dessous de 0.094: on peut évidemment tronquer ces prédictions à zéro, mais cela sous-tend que la probabilité d’acceptation est nulle, alors que certaines personnes dans l’échantillon ont reçu un prêt.\nUne façon de résoudre ce problème consiste à appliquer une transformation à \\(p\\) de telle sorte que la quantité transformée puisse prendre toutes les valeurs entre \\(-\\infty\\) et \\(\\infty\\). Le modèle de régression logistique est défini à l’aide de la transformation \\(\\logit\\), \\[\\begin{align*}\n\\logit(p) = \\ln\\left( \\frac{p}{1-p}\\right)=\\eta=\\beta_0 + \\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p \\mathrm{X}_p,\n\\end{align*}\\] où \\(\\ln\\) est le logarithme naturel.\nEn régression linéaire, on suppose que l’espérance de \\(Y\\) étant donné les valeurs des variables explicatives est une combinaison linéaire de ces dernières. En régression logistique, on suppose que le logit de la probabilité que \\(Y=1\\) étant donné les valeurs des variables explicatives est une combinaison linéaire de ces dernières.\nUne simple manipulation algébrique permet d’exprimer ce modèle en terme de la probabilité \\(p\\), \\[\\begin{align*}\np &= \\expit(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\n= \\frac{1}{1+\\exp(-\\eta)}.\n\\end{align*}\\] On peut voir qu’à mesure que le prédicteur linéaire \\(\\eta=\\beta_0+\\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p\\mathrm{X}_p\\) augmente, la probabilité augmente. Si le coefficient \\(\\beta_j\\) est négatif, \\(p\\) diminuera à mesure que \\(\\mathrm{X}_j\\) augmente.\n\n\n\n\n\n\n\n\n\nPour une variable binaire \\(Y\\), le quotient \\(p/(1-p)\\) est appelé cote et représente le ratio de la probabilité de succès (\\(Y=1\\)) sur la probabilité d’échec (\\(Y=0\\)), \\[\\begin{align*}\n\\mathsf{cote}(p) = \\frac{p}{1-p} = \\frac{\\P{Y=1 \\mid \\mathbf{X}}}{\\P{Y=0 \\mid \\mathbf{X}}}.\n\\end{align*}\\]\nPar exemple, une cote de 4 veut dire qu’il y a 4 fois plus de chance que \\(Y\\) soit égale à \\(1\\) par rapport à \\(0\\). Une cote de 0.25 veut dire le contraire, il y a 4 fois moins de chance que \\(Y=1\\) par rapport à \\(0\\) ou bien, de manière équivalente, il y a 4 fois plus de chance que \\(Y=0\\) par rapport à \\(1\\). Le ?tbl-03-cotes donne un aperçu de cotes pour quelques probabilités \\(p\\).\n\n\n\nCote et probabilité de succès\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\P{Y=1}\\)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n\n\n\ncote\n0.11\n0.25\n0.43\n0.67\n1\n1.5\n2.3\n4\n9\n\n\n\n\\(\\frac{1}{9}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{3}{7}\\)\n\\(\\frac{2}{3}\\)\n\\(1\\)\n\\(\\frac{3}{2}\\)\n\\(\\frac{7}{3}\\)\n\\(4\\)\n\\(9\\)"
  },
  {
    "objectID": "05-reglogistique.html#estimation-des-paramètres",
    "href": "05-reglogistique.html#estimation-des-paramètres",
    "title": "6  Régression logistique",
    "section": "6.3 Estimation des paramètres",
    "text": "6.3 Estimation des paramètres\n\n6.3.1 Principes de base\nSupposons qu’on dispose d’un échantillon de taille \\(n\\) sur les variables \\((Y, \\mathrm{X}_1, \\ldots, \\mathrm{X}_p)\\). À l’aide de ces observations, on peut estimer les paramètres \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1 ,\\ldots, \\beta_p)\\) du modèle de régression logistique \\[\\begin{align*}\n\\logit(p) = \\ln \\left( \\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\mathrm{X}_1 + \\cdots + \\beta_p\\mathrm{X}_p.\n\\end{align*}\\] On obtient ainsi les estimés des paramètres \\(\\widehat{\\boldsymbol{\\beta}}\\), desquels découle une estimation de \\(\\P{Y=1}\\) pour les valeurs \\(\\mathrm{X}_1=x_1, \\ldots, \\mathrm{X}_p=x_p\\) d’un individu donné, \\[\\begin{align*}\n\\widehat{p} = \\expit(\\widehat{\\beta}_0 + \\cdots + \\widehat{\\beta}_p\\mathrm{X}_p).\n\\end{align*}\\]\nUn modèle ajusté peut ensuite être utilisé pour faire de la classification (prédiction) pour de nouveaux individus pour lesquels la variable réponse \\(Y\\) n’est pas observée. Pour ce faire, on choisit un point de coupure \\(c\\) (souvent \\(c=0.5\\) mais pas toujours) et on classifie les observations en deux groupes:\n\nSi \\(\\widehat{p}< c\\), alors \\(\\widehat{Y}=0\\) (c’est-à-dire, on assigne cette observation à la catégorie 0).\nSi \\(\\widehat{p} \\geq c\\), alors \\(\\widehat{Y}=1\\) (c’est-à-dire, on assigne cette observation à la catégorie 1).\n\nOn reviendra en détail sur cet aspect dans une section suivante.\nLa méthode d’estimation des paramètres habituellement utilisée est la méthode du maximum de vraisemblance. Pour les applications, il est suffisant de savoir manipuler trois quantités importantes: la log-vraisemblance, le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\). Les deux critères d’information, que nous avons couvert dans les chapitres précédents, servent à la sélection de modèles tandis que la log-vraisemblance \\(\\ell\\) servira à construire un test d’hypothèse.\n\n\n6.3.2 Méthode du maximum de vraisemblance\nCette sous-section est facultative. Elle donne plus de détails sur la méthode du maximum de vraisemblance et les quantités en découlant, soit \\(\\mathsf{AIC}\\), \\(\\mathsf{BIC}\\) et \\(\\ell(\\widehat{\\boldsymbol{\\beta}})\\).\nLa méthode du maximum de vraisemblance (maximum likelihood) est possiblement la méthode d’estimation la plus utilisée en statistique. En général, pour un échantillon donné et un modèle avec des paramètres inconnus \\(\\boldsymbol{\\theta}\\), on peut calculer la « probabilité » d’avoir obtenu les observations de notre échantillon selon les paramètre. Si on traite cette « probabilité » comme étant une fonction des paramètres du modèle, \\(\\boldsymbol{\\theta}\\), on l’appelle alors la vraisemblance (likelihood). La méthode du maximum de vraisemblance consiste à trouver les valeurs des paramètres qui maximisent la vraisemblance. On cherche donc les estimations qui sont les plus vraisemblables étant donné nos observations.\nEn pratique, il est habituellement plus simple de chercher à maximiser le log de la vraisemblance (ce qui revient au même car le log est une fonction croissante) et on nomme cette fonction la log-vraisemblance (log-likelihood).\nVous connaissez déjà des exemples d’estimateurs du maximum de vraisemblance. La moyenne d’un échantillon est l’estimateur du maximum de vraisemblance pour la moyenne de la population \\(\\mu\\) si les observations représentent un échantillon aléatoire simple tiré d’une loi normale.\nDans le cas d’un modèle de régression linéaire multiple \\(Y_i \\sim \\mathsf{No}(\\beta_0 + \\sum_{j=1}^p \\beta_j\\mathrm{X}_{ij, \\sigma^2)\\) des termes indépendants et de même loi, la log-vraisemblance du modèle pour un échantillon de taille \\(n\\) est \\[\\begin{align*}\n\\ell(\\boldsymbol{\\beta}, \\sigma^2) =- \\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (Y_i- \\beta_0 - \\beta_1 \\mathrm{X}_{1i} - \\cdots - \\beta_p\\mathrm{X}_{ip})^2.\n\\end{align*}\\] Puisque le premier terme ne dépend pas des paramètres \\(\\boldsymbol{\\beta}\\), il est clair que maximiser cette fonction de \\(\\boldsymbol{\\beta}\\) revient à minimiser \\(\\sum_{i=1}^n (Y_i- \\beta_0 - \\beta_1 \\mathrm{X}_{1i} - \\cdots - \\beta_p\\mathrm{X}_{ip})^2\\), et ce critère est exactement le même que celui des moindres carrés. Par conséquent, les estimations des paramètres \\(\\boldsymbol{\\beta}\\) provenant de la méthode des moindres carrés peuvent être vues comme étant des estimateurs du maximum de vraisemblance sous l’hypothèse de normalité des observations; il est même possible d’écrire une formule explicite pour ces estimations.\nDans le cas de la régression logistique, la fonction de log-vraisemblance s’écrit \\[\\begin{align*}\n\\ell(\\boldsymbol{\\beta}) &= \\sum_{i=1}^n Y_i ( \\beta_0 + \\beta_1 \\mathrm{X}_{i1} + \\cdots + \\beta_p \\mathrm{X}_{ip}) \\\\&- \\sum_{i=1}^n \\ln\\left\\{1+\\exp(\\beta_0 + \\cdots + \\beta_p\\mathrm{X}_{ip})\\right\\}\n\\end{align*}\\]\nContrairement au cas de la régression linéaire, on ne peut trouver une fonction explicite pour les valeurs des paramètres qui maximisent cette fonction. Des méthodes numériques doivent alors être utilisées pour l’optimisation. Une fois la maximisation accomplie, on obtient les estimés du maximum de vraisemblance, \\(\\widehat{\\boldsymbol{\\beta}}\\). On peut alors calculer la valeur maximale (numérique) de la log-vraisemblance, \\(\\ell(\\widehat{\\boldsymbol{\\beta}})\\). La quantité \\(-2\\ell(\\widehat{\\boldsymbol{\\beta}})\\) (-2 Log L) est rapportée dans les sorties SAS. Par analogie avec la régression linéaire la valeur de la log-vraisemblance évaluée à \\(\\widehat{\\boldsymbol{\\beta}}\\), \\(\\ell(\\widehat{\\boldsymbol{\\beta}})\\), augmente toujours lorsqu’on ajoute des régresseurs et c’est pourquoi on ne pourra pas l’utiliser comme outil de sélection de variables.\nLes critères d’information sont des fonctions de la log-vraisemblance, mais incluent une pénalité pour le nombre de coefficients \\(\\boldsymbol{\\beta}\\), \\[\\begin{align*}\n\\mathsf{AIC} & = -2 \\ell(\\widehat{\\boldsymbol{\\beta}}) + 2(p+1)\\\\\n\\mathsf{BIC} & = -2 \\ell(\\widehat{\\boldsymbol{\\beta}}) + \\ln(n)(p+1)\n\\end{align*}\\]\nCes définitions sont utilisables dans plusieurs situations lorsque le modèle est ajusté par la méthode du maximum de vraisemblance. En particulier, elles sont utilisées par SAS en régression logistique. Tout comme en régression linéaire et en analyse factorielle, ces deux critères pourront être utilisés pour faire de la sélection de modèles si on calcule les estimateurs du maximum de vraisemblance."
  },
  {
    "objectID": "05-reglogistique.html#cowboy",
    "href": "05-reglogistique.html#cowboy",
    "title": "6  Régression logistique",
    "section": "6.4 Exemple du Professional Rodeo Cowboys Association",
    "text": "6.4 Exemple du Professional Rodeo Cowboys Association\nL’exemple suivant est inspiré de l’article\n\nDaneshvary, R. et Schwer, R. K. (2000) The Association Endorsement and Consumers’ Intention to Purchase. Journal of Consumer Marketing 17, 203-213.\n\nDans cet article, les auteurs cherchent à voir si le fait qu’un produit soit recommandé par le Professional Rodeo Cowboys Association (PRCA) a un effet sur les intentions d’achats. On dispose de 500 observations sur les variables suivantes:\n\n\\(Y\\): seriez-vous intéressé à acheter un produit recommandé par le PRCA\n\n\\(\\texttt{0}\\): non\n\\(\\texttt{1}\\): oui\n\n\\(\\mathrm{X}_1\\): quel genre d’emploi occupez-vous?\n\n\\(\\texttt{1}\\): à la maison\n\\(\\texttt{2}\\): employé\n\\(\\texttt{3}\\): ventes/services\n\\(\\texttt{4}\\): professionnel\n\\(\\texttt{5}\\): agriculture/ferme\n\n\\(\\mathrm{X}_2\\): revenu familial annuel\n\n\\(\\texttt{1}\\): moins de 25 000\n\\(\\texttt{2}\\): 25 000 à 39 999\n\\(\\texttt{3}\\): 40 000 à 59 999\n\\(\\texttt{4}\\): 60 000 à 79 999\n\\(\\texttt{5}\\): 80 000 et plus\n\n\\(\\mathrm{X}_3\\): sexe\n\n\\(\\texttt{0}\\): homme\n\\(\\texttt{1}\\): femme\n\n\\(\\mathrm{X}_4\\): avez-vous déjà fréquenté une université?\n\n\\(\\texttt{0}\\): non\n\\(\\texttt{1}\\): oui\n\n\\(\\mathrm{X}_5\\): âge (en années)\n\\(\\mathrm{X}_6\\): combien de fois avez-vous assisté à un rodéo au cours de la dernière année?\n\n\\(\\texttt{1}\\): 10 fois ou plus\n\\(\\texttt{2}\\): entre six et neuf fois\n\\(\\texttt{3}\\): cinq fois ou moins\n\n\nLe but est d’examiner les effets de ces variables sur l’intentions d’achat (\\(Y\\)). Les données se trouvent dans le fichier logit1.sas7bdat.\n\n6.4.1 Modèle avec une seule variable explicative\nFaisons tout d’abord une analyse en utilisant seulement \\(\\mathrm{X}_5\\) (âge) comme variable explicative. L’ajustement du modèle de régression incluant uniquement \\(\\mathrm{X}_5\\) sera effectuée en exécutant le programme ::: {.cell}\n\nCode\nproc logistic data=multi.logit1 ;\nmodel y(ref='0') = x5 / clparm=pl clodds=pl expb;\nrun;\n\n::: La syntaxe y(ref='0') sert à spécifier la catégorie de référence, zéro, de la variable réponse \\(Y\\): le modèle décrit donc \\(\\P{y=1 \\mid \\mathrm{X}_5}\\).\nVoici une partie de la sortie\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn voit qu’il y a 272 personnes (\\(\\texttt{0}\\)) qui ne sont pas intéressées à acheter un produit recommandé par le PRCA et 228 personnes (\\(\\texttt{1}\\)) qui le sont.\nLes estimés des paramètres sont \\(\\widehat{\\beta}_0 = -3.05\\) et \\(\\widehat{\\beta}_{\\texttt{age}}=0.0749\\).\nUn intervalle de confiance de niveau 95% pour l’effet de l’âge est [\\(0.0465; 0.1043\\)].\nLe modèle ajusté est \\(\\logit\\{\\P{Y=1 \\mid \\mathrm{X}_5=x_5}\\} = -3.05 + 0.0749 x_5\\). On peut également exprimer ce modèle directement en terme de la probabilité de succès, \\[\\begin{align*}\n\\P{Y=1 \\mid \\mathrm{X}_5=x_5} &= \\expit(-3.05 + 0.0749 x_5) \\\\&= \\frac{1}{1+\\exp(-3.05 - 0.0749 x_5)}\n\\end{align*}\\] Le graphe de cette fonction pour \\(\\mathrm{X}_5\\) allant de 18 à 59 ans, respectivement les valeurs minimales et maximales observées dans l’échantillon, montre que le lien entre l’âge et \\(p\\) est presque linéaire entre 20 et 60 ans. On décèle tout de même la forme sigmoide de la fonction \\(\\logit\\) aux deux extrémités. ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\nLa valeur-\\(p\\) pour \\(\\widehat{\\beta}_{\\texttt{age}}\\) (Pr > khi-2), correspondant aux test des hypothèses \\(\\Hy_0: \\beta_{\\texttt{age}}=0\\) versus \\(\\Hy_1: \\beta_{\\texttt{age}} \\neq 0\\), est plus petite que \\(10^{-4}\\) et donc l’effet de la variable âge est statistiquement différent de zéro. Plus l’âge augmente, plus la probabilité d’être intéressé à acheter un produit recommandé par le PRCA augmente.\nLe tableau Test de l'hypothèse nulle globale : BETA=0 contient les résultats de trois tests pour l’hypothèse nulle que tous les paramètres sont nuls, contre l’alternative qu’au moins un des paramètres est différent de zéro. Comme il y a un seul paramètre ici, ces tests reviennent à tester l’effet de la variable âge. Le test de Wald est le même que celui que nous venons de voir dans le tableau des coefficients.\n\n\n\n6.4.2 Interprétation du paramètre\nSi une variable est modélisée à l’aide d’un seul paramètre (pas de terme quadratique et pas d’interaction avec d’autre covariables), une valeur positive du paramètre indique une association positive avec \\(p\\) alors qu’une valeur négative indique le contraire.\nAinsi, le signe du paramètre donne le sens de l’association. Si le coefficient \\(\\beta_j\\) de la variable \\(\\mathrm{X}_j\\) est positif, alors plus la variable augmente, plus \\(\\P{Y=1}\\) augmente. Inversement, Si le coefficient \\(\\beta_j\\) est négatif, plus la variable augmente, plus \\(\\P{Y=1}\\) diminue.\nEn régression linéaire, l’interprétation de coefficient \\(\\beta_j\\) est simple: lorsque la variable \\(\\mathrm{X}_j\\) augmente de un, la variable \\(Y\\) augmente en moyenne de \\(\\beta_j\\), toute chose étant égale par ailleurs. Cette interprétation ne dépend pas de la valeur de \\(\\mathrm{X}_j\\). En régression logistique, comme le modèle est nonlinéaire en fonction de \\(\\P{Y=1}\\) (courbe sigmoide), l’augmentation ou la dimininution de \\(\\P{Y=1\\mid \\mathbf{X}}\\) pour un changement d’une unité de \\(\\mathrm{X}_j\\) dépend de la valeur de cette dernière. C’est pourquoi il est parfois plus utile d’utiliser la cote pour interpréter globalement l’effet d’une variable.\nDans notre exemple, on peut exprimer le modèle ajusté en termes de cote, \\[\\begin{align*}\n\\frac{\\P{Y=1 \\mid \\mathrm{X}_5=x_5}}{\\P{Y=0 \\mid \\mathrm{X}_5=x_5}} = \\exp(-3.05)\\exp(0.0749x_5).\n\\end{align*}\\] Ainsi, lorsque \\(\\mathrm{X}_5\\) augmente d’une année, la cote est multipliée par \\(\\exp(0.0749) = 1.078\\) peut importe la valeur de \\(x_5\\). Pour deux personnes dont la différence d’âge est un an, la cote de la personne plus âgée est 7.8% plus élevée. On peut aussi quantifier l’effet d’une augmentation d’un nombre d’unités quelconque. Par exemple, pour chaque augmentation de 10 ans de \\(\\mathrm{X}_5\\), la cote est multiplié par \\(1.078^{10} = 2.12\\), soit une augmentation de 112%.\nLa cote est rapportée à la dernière colonne du tableau des coefficients. En général, si on veut une interprétation globale de l’effet d’une variable, il faudra baser l’interprétation sur l’exponentielle du coefficient, \\(\\exp(\\widehat{\\beta})\\). SAS dénomme cette quantité rapport de cote (odds ratio).\nUn des avantages d’utiliser la vraisemblance comme fonction objective est que les intervalles de confiance et les estimateurs basés sur la vraisemblance (profilée) sont invariant aux reparametrisations. Ainsi, l’intervalle de confiance à niveau 95% pour \\(\\exp(\\beta_{\\texttt{age}})\\) est obtenu en prenant l’exponentielle des bornes de l’intervalle pour \\(\\beta_{\\texttt{age}}\\), [\\(\\exp(0.0465); \\exp(0.1043)\\)], soit [\\(1.048; 1.110\\)] tel que rapporté dans la sortie. Ce n’est pas le cas des intervalles de Wald qui ont la forme \\(\\widehat{\\beta} \\pm 1.96 \\mathrm{se}(\\widehat{\\beta})\\). Comme l’exponentielle est une transformation monotone croissante, on a \\(\\beta>0\\) si et seulement si \\(\\exp(\\beta)>1\\), etc. On peut ainsi utiliser les intervalles de confiance pour tester l’hypothèse \\(\\mathscr{H}_0: \\beta_j=0\\) ou de façon équivalente \\(\\mathscr{H}_0: \\exp(\\beta_j)=1\\) à niveau 95%.\n\n\n6.4.3 Modèle avec toutes les variables explicatives\nAjustons à présent le modèle avec toutes les variables explicatives. Rappelez-vous que la variable \\(\\mathrm{X}_1\\) (quel genre d’emploi occupez-vous) a cinq catégories, \\(\\mathrm{X}_2\\) (revenu familial annuel) a cinq catégories, et \\(\\mathrm{X}_6\\) (combien de fois avez-vous assisté à un rodéo au cours de la dernière année) a trois catégories. Il faut donc spécifier à SAS de les traiter comme des variables catégorielles dans le modèle. Notez qu’on pourrait aussi traiter \\(\\mathrm{X}_2\\) comme continue car elle est ordinale et possède tout de même cinq modalités, mais on la traitera comme variable nominale.\n\n\nCode\nproc logistic data=multi.logit1 ;\nclass x1(ref=last) x2(ref=last) x6 / param=ref;\nmodel y(ref='0') =x1-x6 / clparm=pl clodds=pl expb;\nrun;\n\n\nDans SAS, les variables incluses dans la commande class sont modélisées à l’aide d’un ensemble de variables indicatrices. Cette commande nous évite de créer nous-même les indicatrices; cette option est disponible dans la plupart des procédures SAS, bien que la procédure reg est une exception notable.\nOn peut changer la catégorie de référence (ref=) qui est par défaut la dernière modalité (en ordre alphanumérique). L’option param=ref pour class permet d’imprimer un tableau indiquant le code pour les variables indicatrices. Les variables incluses dans la commande class sont modélisées à l’aide d’un ensemble de variables indicatrices. Prenons l’exemple de la variable \\(\\mathrm{X}_1\\): la modalité de référence est (), soit agriculture est spécifiée dans le tableau Informations sur les niveaux de classe.\n\n\n\n\n\n\n\n\n\nLe fichier logit1_intro.sas contient le code pour ajuster le même modèle sans la commande class, c’est-à-dire en créant nous-mêmes les variables indicatrices pour inclure les variables explicatives catégorielles. Vous pouvez l’exécuter afin de vous convaincre qu’il s’agit du même modèle. Les estimés seront les mêmes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe modèle ajusté est \\[\\begin{align*}\n\\logit\\{\\P{Y=1 \\mid \\mathbf{X}=\\boldsymbol{x}}\\} &= -6.89 + 0.36\\I{\\mathrm{X}_1=1} - 0.47\\I{\\mathrm{X}_1=2} - 0.31\\I{\\mathrm{X}_1=3} - 0.32\\I{\\mathrm{X}_1=4} \\\\& \\qquad\n+ 1.33\\I{\\mathrm{X}_2=1} + 1.15\\I{\\mathrm{X}_2=2} + 0.77\\I{\\mathrm{X}_2=3} - 1.11\\I{\\mathrm{X}_2=4} \\\\&\\qquad\n+ 1.35\\mathrm{X}_3+ 1.83\\mathrm{X}_4\n+ 0.11\\mathrm{X}_5\n+ 2.41\\I{\\mathrm{X}_6=1} + 1.04\\I{\\mathrm{X}_6=2}\n\\end{align*}\\]\nNotez que les variables \\(\\I{\\mathrm{X}_1=1}\\) (x11), \\(\\I{\\mathrm{X}_1=21}\\) (x12), \\(\\I{\\mathrm{X}_1=3}\\) (x13) et \\(\\I{\\mathrm{X}_1=4}\\) (x14) représentent les quatre indicatrices pour la variable \\(\\mathrm{X}_1\\) (et de même pour \\(\\mathrm{X}_2\\) et \\(\\mathrm{X}_6\\)). L’interprétation se fait comme en régression linéaire multiple. Ici, il n’y a pas de terme quadratique, ni d’interaction. Les paramètres estimés représentent donc l’effet de la variable correspondante sur le logit une fois que les autres variables sont dans le modèle, et demeurent fixes.\nPrenons le coefficient associé à l’âge (\\(\\mathrm{X}_5\\)) comme exmple. Le paramètre estimé est \\(\\widehat{\\beta}_{\\texttt{age}}=0.1095\\) et il est significativement différent de zéro. Ainsi, plus l’âge augmente, plus \\(\\P{Y=1\\mid \\mathbf{X}}\\) augmente, toutes autres choses étant égales par ailleurs. Pour chaque augmentation d’un an de \\(\\mathrm{X}_5\\), la cote est multipliée par \\(\\exp(0.1095)=1.116\\), lorsque les autres variables demeurent fixes.\nN’oubliez pas la nuance suivante concernant l’interprétation d’un test lorsque plusieurs variables explicatives font partie du modèle. Si un paramètre n’est pas significativement différent de zéro, cela ne veut pas dire qu’il n’y a pas de lien entre la variable correspondante et \\(Y\\). Cela veut seulement dire qu’il n’y a pas de lien significatif une fois que les autres variables sont dans le modèle.\nPrenons l’exemple de la variable \\(\\mathrm{X}_6\\), qui représente le nombre de fois où l’individu a assisté à un rodéo au cours de la dernière année. Cette variable est modélisée à l’aide de deux variables indicatrices, \\(\\I{\\mathrm{X}_6=1}\\) égale à un si \\(\\mathrm{X}_6=1\\) et zéro autrement, et \\(\\I{\\mathrm{X}_6=2}\\) égale à un si \\(\\mathrm{X}_6=2\\) et zéro sinon. La catégorie de référence est \\(\\mathrm{X}_6=3\\), c’est-à-dire les personnes ayant assisté cinq fois ou moins à un rodéo au cours de la dernière année. Pour tester la significativité globale d’une variable catégorielle qui est modélisée avec plusieurs indicatrices, il faut aller dans le tableau Analyse des effets Type 3. On voit que la statistique de test est \\(42.9364\\) et que la valeur-\\(p\\) associée est négligeable: la variable \\(\\mathrm{X}_6\\) est donc globalement significative. En fait, il s’agit du test conjoint sur toutes les indicatrices associées à cette variable. Plus précisément, il s’agit du test de l’hypothèse nulle \\(\\Hy_0: \\beta_{6_{\\texttt{1}}}=\\beta_{6_{\\texttt{2}}}=0\\) versus la contre-hypothèse qu’au moins un de ces deux paramètres est différent de zéro.\nL’interprétation des variables catégorielles est analogue à celle faite en régression linéaire. On peut aussi interpréter individuellement les paramètres des indicatrices: pour \\(\\I{\\mathrm{X}_6=1}\\), lorsque les autres variables demeurent fixes, les personnes ayant assisté 10 fois ou plus à un rodéo au cours de la dernière année voient leur cote multipliée par \\(\\exp(2.4122)=11.158\\) par rapport aux personnes ayant assisté cinq fois ou moins. Ce paramètre est significativement différent de zéro car sa valeur-\\(p\\) est négligeable (tableau Analyse des valeurs estimées du maximum de vraisemblance); l’intervalle de confiance à 95% pour le rapport de cotes, basé sur la vraisemblance profilée, est [\\(5.456; 23.882\\)] et un n’est pas dans l’intervalle. Ainsi, il y a une différence significative entre les gens qui ont assisté à 10 rodéos ou plus et les gens qui ont assisté à 5 rodéos ou moins, pour ce qui est de l’intérêt à acheter un produit recommandé par le PRCA.\nOn procède de la même façon pour \\(\\I{\\mathrm{X}_6=2}\\): lorsque les autres variables demeurent fixes, les personnes ayant assisté entre six et neuf fois à un rodéo au cours de la dernière année voient leur cote multipliée par \\(2,842\\) par rapport aux personnes ayant assisté cinq fois ou moins. Ce paramètre est aussi significativement différent de zéro. Il y a donc une progression. Plus une personne a assisté à un grand nombre de rodéo au cours de la dernière année, plus elle est intéressée à acheter un produit recommandé par la PRCA.\nSi on désire comparer les deux modalités \\(\\mathrm{X}_6=1\\) et \\(\\mathrm{X}_6=2\\), il suffit de changer la modalité de référence dans la commande class et d’exécuter le modèle à nouveau. Une alternative est de calculer le rapport (de rapport) de cotes pour ces deux modalités.\n\n\n6.4.4 Test du rapport de vraisemblance\nLes tests correspondants aux valeurs-\\(p\\) dans le tableau des paramètres sont des tests de Wald. Ces tests feront l’affaire dans la plupart des applications. Par contre, il existe un autre test qui est généralement plus puissant, c’est-à-dire qu’il sera meilleur pour détecter que \\(\\Hy_0\\) n’est pas vraie lorsque c’est effectivement le cas. Ce test est le test du rapport de vraisemblance (likelihood ratio test). Il découle de la méthode d’estimation du maximum de vraisemblance et est donc généralement applicable lorsqu’on estime les paramètres avec cette méthode. Il est basé sur la quantité \\(\\ell\\) que nous avons vue plus tôt.\nLa procédure consiste à ajuster deux modèles emboîtés:\n\nLe premier modèle, le modèle complet, contient tous les paramètres et l’estimateur du maximum de vraisemblance \\(\\widehat{\\boldsymbol{\\beta}})\\).\nLe deuxième modèle correspondant à l’hypothèse nulle \\(\\Hy_0\\), le modèle réduit, contient tous les paramètres avec les restrictions imposées sous \\(\\Hy_0\\); on dénote l’estimateur du maximum de vraisemblance \\(\\widehat{\\boldsymbol{\\beta}}_0\\)\n\nLe test est basé sur la statistique \\[\\begin{align*}\nD = -2\\{\\ell(\\widehat{\\boldsymbol{\\beta}}_0)-\\ell(\\widehat{\\boldsymbol{\\beta}})\\}\n\\end{align*}\\] ou la différence entre -2 Log L pour le modèle réduit et -2 Log L pour le modèle complet. Cette différence \\(D\\), lorsque l’hypothèse \\(\\Hy_0\\) est vraie suit approximativement une loi khi-deux avec un nombre de degrés de liberté égal au nombre de paramètre testé (le nombre de restrictions sous \\(\\Hy_0\\)). On peut donc calculer la valeur-\\(p\\) en utilisant la distribution du khi-deux.\nPrenons comme exemple le test de la significativité de \\(\\mathrm{X}_6\\), qui est modélisée à l’aide deux variables binaires \\(\\I{\\mathrm{X}_6=1}\\) et \\(\\I{\\mathrm{X}_6=2}\\) et dont les paramètres correspondants sont \\(\\beta_{6_{\\texttt{1}}}\\) et \\(\\beta_{6_{\\texttt{2}}}\\). Nous avons déjà étudié la sortie pour le test de Wald de significativité globale de \\(\\mathrm{X}_6\\), soit le test de l’hypothèse \\(\\Hy_0: \\beta_{6_{\\texttt{1}}}=\\beta_{6_{\\texttt{2}}}=0\\) versus l’alternative qu’au moins un de ces deux paramètres est différent de zéro. La statistique de test (de Wald) est \\(42.93\\) et la valeur-\\(p\\) est moins de \\(10^{-4}\\). Pour effectuer le test du rapport de vraisemblance, il suffit de retirer la variable \\(\\mathrm{X}_6\\) et de réajuster le modèle à nouveau avec toutes les autres variables; cette manipulation est effectuée dans logit1_intro.sas. On obtient donc -2 Log L de 516,196 pour le modèle complet sans contrainte et \\(566.447\\) pour le modèle excluant la variable \\(\\mathrm{X}_6\\).\nLa différence \\(D = 566.447 - 516.196 = 50.25\\). Il s’agit de la statistique du test de rapport de vraisemblance. La valeur-\\(p\\) peut-être obtenue de la loi du khi-deux avec 2 degrés de liberté via le code suivant permet d’imprimer la valeur-\\(p\\), qui est \\(1.22 \\times 10^{-11}\\).\n\n\nCode\ndata pval;\npval=1-CDF('CHISQ', 566.447 - 516.196, 2);\nrun;\nproc print data=pval;\nrun;\n\n\nComme la statistique du test de rapport de vraisemblance \\(D=50.25\\) est encore plus grande est encore plus grande que la statistique de Wald (\\(42.9364\\)), qui suit la même loi de probabilité sous \\(\\Hy_0\\), cela indique que le test du rapport de vraisemblance est encore plus significatif que le test de Wald. Cela ne fait pas de différence ici mais, dans certains cas, il est possible que le test de Wald ne soit pas significatif (valeur-\\(p\\) plus grande que \\(0.05\\)) tandis que le test du rapport de vraisemblance le soit (valeur-\\(p\\) inférieure à \\(0.05\\)).\n\n\n6.4.5 Multicolinéarité\nRappelez-vous que le terme multicolinéarité fait référence à la situation où les variables explicatives sont très corrélées entre elles ou bien, plus généralement, à la situation où une (ou plusieurs) variable(s) explicative(s) est (sont) très corrélée(s) à une combinaison linéaire des autres variables explicatives.\nL’effet potentiellement néfaste de la multicolinéarité est le même qu’en régression linéaire, c’est-à-dire, elle peut réduire la précision des estimations des paramètres (augmenter leurs écarts-types estimés).\nEn pratique, le problème est qu’il devient difficile de départager l’effet individuel d’une variable explicative lorsqu’elle est fortement corrélée avec d’autres variables explicatives.\nComme la multicolinéarité est une propriété des variables explicatives (le \\(Y\\) n’intervient pas) on peut utiliser les mêmes outils qu’en régression linéaire pour tenter de la détecter, par exemple, le facteur d’inflation de la variance (variance inflation factor). Cette quantité ne dépend que des variables explicatives \\(\\mathbf{X}\\), pas du modèle ou de la variable réponse.\nLa multicolinéarité est surtout un problème lorsque vient le temps d’interpréter et tester l’effet des paramètres individuels. Si le but est seulement de faire de la classification (prédiction) et que l’interprétation des paramètres individuels n’est pas cruciale alors il n’y a pas lieu de se soucier de la multicolinéarité. Il faut alors plutôt comparer correctement la performance de classification des modèles en utilisant des méthodes permettant d’obtenir un bon modèle tout en se protégeant contre le surajustement. Certaines de ces méthodes (division de l’échantillon, validation croisée) ont déjà été présentées."
  },
  {
    "objectID": "05-reglogistique.html#classification-et-prédiction-à-laide-de-la-régression-logistique",
    "href": "05-reglogistique.html#classification-et-prédiction-à-laide-de-la-régression-logistique",
    "title": "6  Régression logistique",
    "section": "6.5 Classification et prédiction à l’aide de la régression logistique",
    "text": "6.5 Classification et prédiction à l’aide de la régression logistique\nLa finalité du modèle de régression logistique est fréquemment l’obtention de prédictions. Une fois qu’on a ajusté un modèle, on peut l’utiliser pour prévoir la valeur de \\(Y\\) pour de nouvelles observations. Ceci consiste à assigner une classe (\\(0\\) ou \\(1\\)) à ces observations (pour lesquels \\(Y\\) est inconnue) à partir des valeurs prises par \\(\\mathrm{X}_1, \\ldots, \\mathrm{X}_p\\).\nLe modèle ajusté nous fournit une estimation de \\(\\P{Y=1 \\mid \\mathbf{X}=\\boldsymbol{x}}\\) pour des valeurs \\(\\mathrm{X}_1=x_1, \\ldots, \\mathrm{X}_p=x_p\\) données. Cet estimé est \\[\\begin{align*}\n\\widehat{p} = \\frac{1}{1+ \\exp\\{- ( \\widehat{\\beta}_0 + \\widehat{\\beta}_1x_1 + \\cdots + \\widehat{\\beta}_p x_p)\\}}.\n\\end{align*}\\]\nClassification de base: pour classifier des observations, il suffit de choisir un point de coupure \\(c\\), souvent \\(c=0.5\\), et de classifier une observation de la manière suivante:\n\nSi \\(\\widehat{p} < c\\), on assigne cette observation à la catégorie zéro et \\(\\widehat{Y}=0\\).\nSi \\(\\widehat{p} \\geq c\\), on assigne cette observation à la catégorie un et \\(\\widehat{Y}=1\\).\n\nSi on prend \\(c=0.5\\) comme point de coupure, cela revient à assigner l’observation à la classe (catégorie) la plus probable, un choix fort raisonnable. Nous verrons dans une section suivante que, lorsque les conséquences de faussement classifier une observation (succès, mais échec prédit et vice-versa) ne sont pas les mêmes, il peut être avantageux d’utiliser un autre point de coupure.\nDans un cadre de prédiction, il nous faudra un critère pour juger de la qualité de l’ajustement du modèle. Rappelez-vous que pour une réponse continue, nous avons utilisé l’erreur moyenne quadratique, \\(\\mathsf{EMQ} = \\mathsf{E}\\{(Y-\\widehat{Y})^2\\}\\), où \\(\\widehat{Y} = \\mathsf{E}(Y \\mid \\mathbf{X})\\), pour juger de la performance d’un modèle. Comme la réponse \\(Y\\) est binaire ici, nous allons utiliser des critères différents.\nVoyons d’abord un premier critère pour juger de la qualité d’un modèle de prédiction. Soit \\(Y\\) la vraie valeur de la réponse binaire et \\(\\widehat{Y}\\) (soit 0 ou 1) la valeur de \\(Y\\) prédite par un modèle pour une observation choisie au hasard dans la population. Un premier critère pour juger de la performance d’un modèle est le taux de mauvaise classification, un estimé de la probabilité de mal classifier une observation choisie au hasard dans la population, \\(\\P{Y \\neq\\widehat{Y}}\\). Plus \\(\\P{Y \\neq\\widehat{Y}}\\) est petite, meilleure est la capacité prédictive du modèle.\nTout comme l’erreur moyenne quadratique, on ne peut qu’estimer \\(\\P{Y \\neq\\widehat{Y}}\\). Pour les raisons vues au chapitre précédent, l’estimer en calculant le taux de mauvaise classification des observations ayant servi à l’ajustement du modèle sans aucune correction n’est pas une bonne approche. Les approches couvertes dans le dernier chapitre pour l’estimation de l’erreur moyenne quadratique, telles la validation-croisée et la division de l’échantillon, peuvent être utilisées pour estimer le taux de mauvaise classification \\(\\P{Y \\neq \\widehat{Y}}\\).\nCette utilisation d’un modèle de régression logistique sera illustrée avec l’exemple que nous avons traité au chapitre précédent: notre objectif final est de construire un modèle avec les 1000 clients de l’échantillon d’apprentissage et cibler ensuite lesquels des 100 000 clients restants seront choisis pour recevoir le catalogue. Les variables cibles sont:\n\nyachat: variable binaire égale à un si le client a acheté quelque chose dans le catalogue et zéro sinon.\nymontant: le montant de l’achat si le client a acheté quelque chose\n\nLes 10 variables suivantes sont disponibles pour tous les clients et serviront de variables explicatives,\n\nx1: sexe de l’individu, soit homme (0) ou femme (1);\nx2: l’âge (en année);\nx3: variable catégorielle indiquant le revenu, soit moins de 35 000$ (1), entre 35 000$ et 75 000$ (2) ou plus de 75 000$ (3);\nx4: variable catégorielle indiquant la région où habite le client (de 1 à 5);\nx5: conjoint : le client a-t-il un conjoint, soit oui (1) ou non (0);\nx6: nombre d’année depuis que le client est avec la compagnie;\nx7: nombre de semaines depuis le dernier achat;\nx8: montant (en dollars) du dernier achat;\nx9: montant total (en dollars) dépensé depuis un an;\nx10: nombre d’achats différents depuis un an.\n\nDans le chapitre précédent, nous avons cherché à développer un modèle pour prévoir ymontant, le montant dépensé, étant donné que le client achète quelque chose. Cette fois-ci, nous allons travailler avec la variable yachat, qui est binaire, à l’aide de la régression logistique.\nAfin d’introduire différentes notions, nous allons, dans un premier temps, utiliser les 10 variables de base. À partir de la section suivante, nous chercherons à optimiser le modèle en considérant les interactions d’ordre deux. Pour ce faire, nous utiliserons des méthodes de sélections de variables. Les commandes se trouvent dans le fichier logit2_classification_base.sas. Dans le code qui suit, le fichier train contient les 1000 clients de l’échantillon d’apprentissage et le fichier test contient les 100 000 clients pour lesquels on veut prédire l’intention d’achat.\n\n\nCode\nproc logistic data=train;\nmodel yachat(ref='0') = x1x2 x3 x32 x41-x44 x5-x10;\noutput out=pred predprobs=crossvalidate;\nrun;\n\n\nLe modèle utilise seulement les 10 variables de base (en fait 14 avec les indicatrices pour les variables catégorielles). Des prévisions pour les clients restants seront exportées dans le fichier pred, grâce à la commande score. L’option ctable permet d’obtenir la Table de classification (sic). Tel que nous l’avons vu au chapitre précédent, il y a 210 clients qui ont acheté quelque chose parmi les 1000.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoupe\nVP\nVN\nFP\nFN\ncorrect (%)\nsensibilité (%)\nspécificité (%)\nFP (%)\nFN (%)\n\n\n\n\n0.02\n210\n209\n581\n0\n41.9\n100.0\n26.5\n73.5\n0.0\n\n\n0.04\n207\n320\n470\n3\n52.7\n98.6\n40.5\n69.4\n0.9\n\n\n0.06\n201\n398\n392\n9\n59.9\n95.7\n50.4\n66.1\n2.2\n\n\n0.08\n199\n451\n339\n11\n65.0\n94.8\n57.1\n63.0\n2.4\n\n\n0.10\n193\n480\n310\n17\n67.3\n91.9\n60.8\n61.6\n3.4\n\n\n0.12\n191\n512\n278\n19\n70.3\n91.0\n64.8\n59.3\n3.6\n\n\n0.14\n184\n547\n243\n26\n73.1\n87.6\n69.2\n56.9\n4.5\n\n\n0.16\n176\n572\n218\n34\n74.8\n83.8\n72.4\n55.3\n5.6\n\n\n0.18\n172\n598\n192\n38\n77.0\n81.9\n75.7\n52.7\n6.0\n\n\n0.20\n164\n611\n179\n46\n77.5\n78.1\n77.3\n52.2\n7.0\n\n\n0.22\n162\n626\n164\n48\n78.8\n77.1\n79.2\n50.3\n7.1\n\n\n0.24\n158\n639\n151\n52\n79.7\n75.2\n80.9\n48.9\n7.5\n\n\n0.26\n153\n645\n145\n57\n79.8\n72.9\n81.6\n48.7\n8.1\n\n\n0.28\n150\n657\n133\n60\n80.7\n71.4\n83.2\n47.0\n8.4\n\n\n0.30\n143\n667\n123\n67\n81.0\n68.1\n84.4\n46.2\n9.1\n\n\n0.32\n138\n679\n111\n72\n81.7\n65.7\n85.9\n44.6\n9.6\n\n\n0.34\n134\n695\n95\n76\n82.9\n63.8\n88.0\n41.5\n9.9\n\n\n0.36\n130\n699\n91\n80\n82.9\n61.9\n88.5\n41.2\n10.3\n\n\n0.38\n126\n708\n82\n84\n83.4\n60.0\n89.6\n39.4\n10.6\n\n\n0.40\n120\n715\n75\n90\n83.5\n57.1\n90.5\n38.5\n11.2\n\n\n0.42\n115\n723\n67\n95\n83.8\n54.8\n91.5\n36.8\n11.6\n\n\n0.44\n112\n731\n59\n98\n84.3\n53.3\n92.5\n34.5\n11.8\n\n\n0.46\n109\n736\n54\n101\n84.5\n51.9\n93.2\n33.1\n12.1\n\n\n0.48\n106\n739\n51\n104\n84.5\n50.5\n93.5\n32.5\n12.3\n\n\n0.50\n100\n744\n46\n110\n84.4\n47.6\n94.2\n31.5\n12.9\n\n\n0.52\n98\n748\n42\n112\n84.6\n46.7\n94.7\n30.0\n13.0\n\n\n0.54\n92\n750\n40\n118\n84.2\n43.8\n94.9\n30.3\n13.6\n\n\n0.56\n87\n753\n37\n123\n84.0\n41.4\n95.3\n29.8\n14.0\n\n\n0.58\n83\n761\n29\n127\n84.4\n39.5\n96.3\n25.9\n14.3\n\n\n0.60\n80\n766\n24\n130\n84.6\n38.1\n97.0\n23.1\n14.5\n\n\n0.62\n77\n769\n21\n133\n84.6\n36.7\n97.3\n21.4\n14.7\n\n\n0.64\n74\n771\n19\n136\n84.5\n35.2\n97.6\n20.4\n15.0\n\n\n0.66\n68\n772\n18\n142\n84.0\n32.4\n97.7\n20.9\n15.5\n\n\n0.68\n62\n774\n16\n148\n83.6\n29.5\n98.0\n20.5\n16.1\n\n\n0.70\n54\n775\n15\n156\n82.9\n25.7\n98.1\n21.7\n16.8\n\n\n0.72\n51\n777\n13\n159\n82.8\n24.3\n98.4\n20.3\n17.0\n\n\n0.74\n49\n778\n12\n161\n82.7\n23.3\n98.5\n19.7\n17.1\n\n\n0.76\n46\n778\n12\n164\n82.4\n21.9\n98.5\n20.7\n17.4\n\n\n0.78\n41\n781\n9\n169\n82.2\n19.5\n98.9\n18.0\n17.8\n\n\n0.80\n35\n783\n7\n175\n81.8\n16.7\n99.1\n16.7\n18.3\n\n\n0.82\n33\n783\n7\n177\n81.6\n15.7\n99.1\n17.5\n18.4\n\n\n0.84\n32\n783\n7\n178\n81.5\n15.2\n99.1\n17.9\n18.5\n\n\n0.86\n28\n784\n6\n182\n81.2\n13.3\n99.2\n17.6\n18.8\n\n\n0.88\n25\n786\n4\n185\n81.1\n11.9\n99.5\n13.8\n19.1\n\n\n0.90\n21\n787\n3\n189\n80.8\n10.0\n99.6\n12.5\n19.4\n\n\n0.92\n18\n787\n3\n192\n80.5\n8.6\n99.6\n14.3\n19.6\n\n\n0.94\n14\n788\n2\n196\n80.2\n6.7\n99.7\n12.5\n19.9\n\n\n0.96\n6\n788\n2\n204\n79.4\n2.9\n99.7\n25.0\n20.6\n\n\n0.98\n2\n790\n0\n208\n79.2\n1.0\n100.0\n0.0\n20.8\n\n\n\n\n\nLe tableau de classification contient des estimations de plusieurs quantités intéressantes, en faisant varier le point de coupure (Niveau de proba dans le tableau SAS). Pour chaque point de coupure, ces estimations ont été obtenues à l’aide d’une approximation de la méthode de validation croisée à \\(n\\) groupes (en anglais, leave-one-out cross-validation, ou LOOCV). Ainsi, ces estimations sont meilleures que les estimés sans ajustement aucun car elles ne sont pas obtenues en utilisant les mêmes observations que celles qui ont servi à estimer le modèle.\nLa colonne correct donne une estimation du taux de bonne classification, \\(\\P{Y = \\widehat{Y}} = 1-\\P{Y \\neq \\widehat{Y}}\\), ou de manière équivalente un moins le taux de mauvaise classification.\nAvec un point de coupure de \\(0\\), on classifie toutes les observations à la classe achat (\\(1\\)), car \\(\\widehat{p}\\) est forcément plus grande que zéro. Le taux de bonne classification dans ce cas de figure sera de \\(21\\)%, puisque 210 individus ont acheté un produit dans le catalogue dans l’échantillon d’apprentissage. L’autre extrême, avec un point de coupure \\(c=1\\), donne un taux de bonne classification de \\(79\\)%.\nOn peut chercher dans le tableau les points de coupure qui donnent le meilleur taux de bonne classification. Ce dernier, à savoir 84.6%, est atteint par trois points de coupure, soit 0.52, soit 0.6, soit 0.62. Une recherche plus fine donne 0.465 comme point de coupure optimal, avec un taux de mauvaise classification de 15.3%.\nLa matrice de confusion, qui compare les vraies valeurs avec les prédictions, peut être construite à partir des colonnes Correct - Événement, Correct - Non-événement, Incorrect - Événement, Incorrect - Non-événement. Il y a deux classifications possibles et le tableau contient, en partant du coin supérieur gauche et dans le sens des aiguilles d’une montre, le nombre de vrai positif (\\(Y=1\\), \\(\\widehat{Y}=1\\)), de faux positif (\\(Y=0\\), \\(\\widehat{Y}=1\\)), de vrai négatif (\\(Y=0\\), \\(\\widehat{Y}=0\\)) et finalement de faux négatif (\\(Y=1\\), \\(\\widehat{Y}=0\\)). Ces nombres proviennent de la validation croisée à \\(n\\) groupes et ne sont pas ceux qu’on obtiendrait si on appliquait directement le modèle ajusté à notre échantillon. Le taux de mauvaise classification est \\((\\mathsf{FP}+\\mathsf{FN})/n\\).\n\n\n\nMatrice de confusion avec point de coupure 0.465\n \n  \n      \n    $Y=1$ \n    $Y=0$ \n  \n \n\n  \n    $\\widehat{Y}=1$ \n    109 \n    52 \n  \n  \n    $\\widehat{Y}=0$ \n    101 \n    738 \n  \n\n\n\n\n\nQuatre autres quantités, dérivées à partir de la matrice de confusion, sont parfois utilisées:\n\nla sensibilité (sensitivity), \\(\\P{\\widehat{Y}=1 \\mid Y=1}\\), ou \\(\\mathsf{VP}/(\\mathsf{VP}+\\mathsf{FN})\\);\nla spécificité (specificity), \\(\\P{\\widehat{Y}=0 \\mid Y=0}\\), ou \\(\\mathsf{VN}/(\\mathsf{VN}+\\mathsf{FP})\\);\nle taux de vrais positifs, \\(\\P{Y=1 \\mid \\widehat{Y}=1}\\), ou \\(\\mathsf{VP}/(\\mathsf{VP}+\\mathsf{FP})\\);\nle taux de vrais négatifs, \\(\\P{Y=0 \\mid \\widehat{Y}=0}\\), ou \\(\\mathsf{VN}/(\\mathsf{VN}+\\mathsf{FN})\\).\n\nLes estimés empiriques sont simplement obtenus en calculant les rapports du nombre d’observations dans chaque classe. SAS rapporte ces quantités, mais notez que les vieilles versions du logiciel retournent le taux de faux positifs et de faux négatifs dans les deux dernières colonnes, tandis que les sorties des nouvelles version du logiciel donnent les taux de vrais positifs et de vrais négatifs.\nLa sensibilité mesure à quel point notre modèle est performant pour détecter un vrai positif (classe 1). La spécificité mesure à quel point notre modèle est performant pour détecter un résultat négatif (classe 0). Plus le point de coupure augmente, plus la sensibilité et le taux de faux positifs diminuent mais plus la spécificité et le taux de faux négatifs augmentent.\nLa fonction d’efficacité du récepteur, parfois appelée courbe ROC (receiver operating characteristic) est parfois utilisée pour représenter globalement la performance du modèle. Elle est obtenue avec l’option plots(only)=(roc) dans SAS. Il s’agit du graphe de la sensibilité en fonction de un moins la spécificité, en faisant varier le point de coupure. Un modèle parfait aurait une sensibilité et une spécificité égales à 1 (correspondant au coin supérieur gauche de la fonction d’efficacité du récepteur). Ainsi, plus le couple (\\(1-\\)spécificité, sensibilité) est près de (\\(0\\), \\(1\\)), meilleur est le modèle. Par conséquent, plus la courbe ROC tend vers (\\(0\\), \\(1\\)) meilleur est le pouvoir prévisionnel des variables.\nL’aire sous la courbe (area under the curve) est souvent utilisée en parallèle est simplement l’aire sous la courbe de la fonction d’efficacité du récepteur. Pour le modèle logistique ajusté, on a une aire sous la courbe de 0.8847. Plus cette valeur est élevée (au plus \\(1\\)), mieux c’est.\n\n\n\n\n\n\n\n\n\nLa courbe ROC et la valeur de l’aire sous la courbe (avec l’option plots(only)=(roc)), sont calculées avec les données d’apprentissage et ne sont pas corrigées. Si on veut les utiliser pour comparer des modèles, il faut plutôt utiliser l’option crossvalidate qui permet d’obtenir des estimations des probabilités par validation-croisée avec \\(n\\) groupes tout comme celle utilisée dans le tableau de classification.\n\n\nCode\nproc logistic data=train;\nclass x3 x4 / ref=glm;\nmodel yachat(ref='0') = x1-x10;\noutput out=pred predprobs=crossvalidate;\nrun;\n\nproc logistic data=pred;\nclass x3 x4 / ref=glm;\nmodel yachat(ref='0') = x1-x10;\nroc pred=xp_1;\nrun;\n\n\nOn sauvegarde d’abord les probabilités estimées par validation-croisée dans le fichier pred avec la commande output out=pred predprobs=crossvalidate La variable xp_1 désigne cette probabilité dans le fichier pred. Ensuite, on exécute de nouveau la procédure logistic avec ce fichier et la commande roc. L’aire sous la courbe pour les prédictions avec la validation-croisée à \\(n\\) groupes est 0.8723: cet estimé est légèrement inférieur à celui obtenu sans la correction (trop optimiste) qui est 0.8847.\n\n\n\n\n\n\n\n\n\nUn autre type de graphe qui est souvent utilisé dans des contextes de gestion est la courbe lift (sic) (en anglais, lift chart). Cette courbe est obtenue en ordonnant les probabilités de succès estimées par le modèle, \\(\\widehat{p}\\), en ordre croissant et en regardant quelle pourcentage de ces derniers seraient bien classifiés (le nombre de vrais positifs sur le nombre de succès).\nSAS ne permet pas de la tracer directement, mais le fichier logit3_lift_chart.sas contient une macro SAS qui permet de le faire.\n\n\nCode\nproc logistic data=train;\nmodel yachat(ref='0') = x1 x2 x31 x32 x41-x44 x5-x10;\noutput out=pred predprobs=crossvalidate;\nrun;\n%liftchart1(pred,yachat,xp_1,10);\n\n\nIci, le tableau présente les 10 déciles. Si on classifiait comme acheteurs les 10% qui ont la plus forte probabilité estimée d’achat, on détecterait 79 des 210 clients (37,6%). En comparaison, on s’attend que 21 clients soient sélectionnés en moyenne si on prend un échantillon aléatoire de 100 personnes. Le ratio 79/21 (dernière colonne) est le lift du modèle: il permet de détecter 3,76 fois plus de succès que le hasard.\n\n\n\n\n\n\n\n\n\nLe graphe Figure 6.2 présente le pourcentage d’observations bien classées parmi les variables (pourcentage des probabilités prédites qui correspondent à un succès parmi les \\(k\\) plus susceptibles selon le modèle). La référence est la ligne diagonale, qui correspond à une détection aléatoire.\n\n\n\n\n\nFigure 6.2: Taux de classement en fonction du lift\n\n\n\n\nIl peut être intéressant de vérifier la calibration de notre modèle, et une statistique simple proposée par Spiegelhalter (1986) peut être utile à cette fin. Pour une variable binaire \\(Y \\in \\{0,1\\}\\), l’erreur moyenne quadratique s’écrit \\[\\begin{align*}\n\\overline{B} &= \\frac{1}{n} \\sum_{i=1}^n (Y_i-p_i)^2\n=\\frac{1}{n} \\sum_{i=1}^n(Y_i-p_i)(1-2p_i) + \\frac{1}{n} \\sum_{i=1}^n p_i(1-p_i).\n\\end{align*}\\] Le premier terme représente le manque de calibration du modèle, tandis que le deuxième correspond à la séparation entre variables. Si notre modèle était parfaitement calibré, alors \\(\\mathsf{E}_0(Y_i)=p_i\\) et \\(\\mathsf{Var}_0(Y_i) = p_i(1-p_i)\\). On peut utiliser ce fait pour construire une statistique de Wald, \\(Z = \\{\\overline{B} - \\mathsf{E}_0(\\overline{B})\\}/\\sqrt{\\mathsf{Var}_0(\\overline{B})}\\), où \\[\\begin{align*}\n\\mathsf{E}_0(\\overline{B})&= \\frac{1}{n} \\sum_{i=1}^n p_i(1-p_i) \\\\\n\\mathsf{Var}_0(\\overline{B})&= \\frac{1}{n^2} \\sum_{i=1}^n p_i(1-p_i)(1-2p_i)^2\n\\end{align*}\\]\n\n\n\nSous l’hypothèse nulle de calibration parfaite, \\(Z \\sim \\mathsf{No}(0,1)\\) en grand échantillon. Pour le modèle simple avec toutes les covariables, la valeur-\\(p\\) approximative calculée avec les probabilités de succès obtenues par validation-croisée et les données de l’échantillon d’apprentissage est 0.22 et il n’y a pas de preuve que le modèle est mal calibré. Cette technique est utile pour vérifier s’il n’y a pas de surajustement (auquel cas le modèle tend à retourner des probabilités très près de 0/1, mais qui ne correspondent pas à la réalité)."
  },
  {
    "objectID": "05-reglogistique.html#classification-avec-une-matrice-de-gain",
    "href": "05-reglogistique.html#classification-avec-une-matrice-de-gain",
    "title": "6  Régression logistique",
    "section": "6.6 Classification avec une matrice de gain",
    "text": "6.6 Classification avec une matrice de gain\nUtiliser le taux de mauvaise classification \\(\\P{Y \\neq \\widehat{Y}}\\), comme critère de performance, revient au même que d’utiliser le taux de bonne classification \\(\\P{Y=\\widehat{Y}}\\), car \\(\\P{Y \\neq \\widehat{Y}} = 1-\\P{Y=\\widehat{Y}}\\). On veut un modèle avec un haut taux de bonne classification (ou un faible taux de mauvaise classification).\nLorsqu’on utilise \\(\\P{Y \\neq \\widehat{Y}}\\) comme critère pour juger de la qualité d’un modèle prévisionnel, on fait l’hypothèse que le gain associé à bien classifier une observation dans la catégorie 0 lorsqu’elle est réellement dans la catégorie 0 est le même que celui associé à classifier une observation dans la catégorie 1 lorsqu’elle est réellement dans la catégorie 1: cela correspond à la matrice de gain.\n\n(#tab:03-gain1) Matrice de gain correspondant au taux de bonne classification\n\n\n\n\nobservation\n\n\n\n\n\n\ngain\n\\(Y=1\\)\n\\(Y=0\\)\n\n\nprédiction\n\\(\\widehat{Y}=1\\)\n1\n0\n\n\n\n\\(\\widehat{Y}=0\\)\n0\n1\n\n\n\nC’est-à-dire, le gain vaut 1 lorsque la prévision est bonne (les deux cas sur la diagonale) et 0 lorsque le modèle se trompe (les deux autres cas). L’unité de mesure du gain n’est pas importante pour l’instant. Le gain total est\n\\[\\begin{align*}\n\\text{gain} &= 1 \\P{\\widehat{Y}=1, Y=1} + 1 \\P{\\widehat{Y}=0, Y=0}\n\\\\ &\\quad + 0 \\P{\\widehat{Y}=1, Y=0}  + 0 \\P{\\widehat{Y}=0, Y=1}\n\\\\& = \\P{Y = \\widehat{Y}}.\n\\end{align*}\\] Maximiser le gain total revient donc à maximiser le taux de bonne classification.\nDans certaines situations, les gains (ou la perte si le gain est négatif) associés aux bonnes décisions et aux erreurs ne sont pas équivalents. Par exemple, un des types d’erreurs peut être plus grave que l’autre. Il peut alors être souhaitable d’en tenir compte dans le choix du modèle de classification.\nSupposons que le gain de classer une observation à \\(i\\) (\\(i \\in \\{0,1\\}\\)) lorsqu’elle vaut \\(j\\) (\\(j \\in \\{0,1\\}\\)) en réalité est de \\(c_{ij}\\). La matrice de gain est alors\n\n(#tab:03-gain2) Matrice de gain pondérée en fonction d’un coût\n\n\n\n\nobservation\n\n\n\n\n\n\ngain\n\\(Y=1\\)\n\\(Y=0\\)\n\n\nprédiction\n\\(\\widehat{Y}=1\\)\n\\(c_{11}\\)\n\\(c_{10}\\)\n\n\n\n\\(\\widehat{Y}=0\\)\n\\(c_{01}\\)\n\\(c_{00}\\)\n\n\n\nEn pratique, l’une de ces quatre quantités peut être fixée à un car seulement les poids relatifs (les ratios) des gains sont importants. Dans ce cas, le gain moyen est \\[\\begin{align*}\n\\text{gain} &= c_{11} \\P{\\widehat{Y}=1, Y=1} + c_{00}\\P{\\widehat{Y}=0, Y=0}\n\\\\ &\\quad + c_{10} \\P{\\widehat{Y}=1, Y=0}  + c_{01} \\P{\\widehat{Y}=0, Y=1}\n\\end{align*}\\]\nLe meilleur modèle est alors celui qui maximise le gain moyen. Le fichier logit4_macro_gain.sas contient des macros SAS qui permettent d’estimer le gain moyen à l’aide de la validation croisée.\nNous allons encore une fois seulement utiliser les 10 variables de base. Mais nous allons intégrer des revenus et coûts afin de trouver le meilleur point de coupure. Rappelez-vous que le coût de l’envoi d’un catalogue est de 10$. Le tableau des variables descriptives qui suit montre que, pour les 210 clients qui ont acheté quelque chose, le revenu moyen est de 67,29$ (moyenne de la variable ymontant).\n\n\n\n\n\n\n\n\n\nNous allons travailler en termes de revenu net. Nous pouvons donc spécifier la matrice de gain du ?tbl-03-gain3 pour notre problème. Si on n’envoit pas de catalogue, notre gain est nul. Si on envoie le catalogue à un client qui n’achète pas, on perd 10$ (le coût de l’envoi). En revanche, notre revenu net est de 57$ (revenu moyen moins coût de l’envoi).\n\n(#tab:03-gain3) Matrice de gain pour l’envoi de catalogue\n\n\n\n\nobservation\n\n\n\n\n\n\ngain\n\\(Y=1\\)\n\\(Y=0\\)\n\n\nprédiction\n\\(\\widehat{Y}=1\\)\n57\n\\(-10\\)\n\n\n\n\\(\\widehat{Y}=0\\)\n0\n0\n\n\n\nL’appel de la macro manycut_cvlogistic, dont les paramètres sont expliqués dans le script, se fait de la manière suivante:\n\n\nCode\n%manycut_cvlogisticclass(\n  yvar=yachat, xvar=x1-x10, xvarclass=x3-x4,\n  n=1000, k=10, ncv=10, dataset=train,\n  c00=0, c01=0, c10=-10, c11=57,\n  manycut=.05 .06 .07 .08 .09 .1 .11 .12 .13 .14 .15 .16 .17 .18 .5);\n\n\nCette macro produit le tableau suivant. Il donne l’estimation du gain moyen (gain) pour différents points de coupures (cutpoint). Cette estimation provient d’une validation-croisée avec 10 groupes (k=10 dans la macro). En fait, on a répété 10 fois (ncv=10 dans la macro) la validation croisée avec 10 groupes et fait la moyenne des 10 répétitions afin d’avoir plus de précisions. Il faut essayer plusieurs points de coupure afin de trouver le meilleur.\nOn voit que le meilleur point de coupure, celui qui maximise le gain est 0.12. Avec ce point de coupure, on estime que le taux de bonne classification est de 0.707 et que la sensitivité est de 0.899. Ainsi, on estime qu’on va détecter 90% des clients qui achètent.\n\n\n\n\n\n\n\n\n\nOn est loin du point de coupure usuel de 0.5 (présenté à la dernière ligne). La raison est simple. Comme il est très coûteux de rater un client qui aurait acheté quelque chose, il est préférable d’envoyer le catalogue à plus de clients, quitte à ce que plusieurs d’entre eux n’achètent rien. En fait, le point de coupure de 0.5 donne un meilleur taux de bonne classification mais un gain moyen plus faible car on rate trop de clients qui achètent (la sensitivité est seulement de 48,8%). Travailler avec la matrice de gain permet de trouver le point de coupure optimal en incorporant des notions de coûts et profits.\nIci, nous avons ajusté un seul modèle, celui contenant uniquement les 10 variables de base et nous nous sommes attardés au choix du point de coupure pour l’assignation aux classes. Il est possible qu’un autre modèle, contenant par exemple des termes d’interactions, des termes quadratiques ou d’autres transformations des variables, soit supérieur à celui-ci. Le choix du modèle de prévision se fait donc souvent en deux étapes\n\ntrouver les bonnes variables\ntrouver le bon point de coupure.\n\nNous avons déjà vu des méthodes de sélections de variables au chapitre précédent. La section suivante reviendra sur ces méthodes dans le contexte de la régression logistique."
  },
  {
    "objectID": "05-reglogistique.html#sélection-de-variables-en-régression-logistique",
    "href": "05-reglogistique.html#sélection-de-variables-en-régression-logistique",
    "title": "6  Régression logistique",
    "section": "6.7 Sélection de variables en régression logistique",
    "text": "6.7 Sélection de variables en régression logistique\nLes principes généraux, concernant la sélection de variables et de modèles, que nous avons vus au chapitre précédent sont toujours valides. Les critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) sont toujours disponibles puisqu’on estime le modèle par maximum de vraisemblance et les techniques générales de division de l’échantillon et de validation-croisée sont toujours valides. Nous allons voir comment appliquer spécifiquement ces techniques au cas de la régression logistique.\n\n6.7.1 Recherche séquentielle\nRappelez-vous qu’avec une variable cible continue, nous avons utilisé avec la procédure reg pour faire une recherche du meilleur sous-ensemble de variables parmi tous les ensembles. Pour ce faire, on sélectionnait le meilleur modèle selon le \\(R^2\\) pour un nombre de variables fixe et il suffisait ensuite de trouver parmi ces variables le meilleur selon le critère d’information choisi.\nParce qu’il n’y a pas de solution explicite pour les estimateurs du maximum de vraisemblance du modèle logistique, ajuster chacun de ces modèles est coûteux. Les options pour la sélection de modèle avec le modèle de régression logistique est très limité dans SAS: toutes les procédures supportent la sélection à des degrés variés (pas de validation externe basée sur la log-vraisemblance, pas de validation croisée). Comble de malheur, le support des options n’est pas cohérent d’une procédure à l’autre. On peut se rabattre sur une recherche séquentielle ou le LASSO: pour cette première, il est possible d’utiliser une stratégie d’élimination rapide avec la statistique du score (ou test des multiplicateurs de Lagrange) pour tester si l’ajout d’une variable est utile ou pas: c’est une approximation de la recherche exhaustive des meilleurs sous-ensembles.\nCe paragraphe est plus technique et peut être omis. La statistique de score, qui est basée sur la vraisemblance, ne nécessite que d’obtenir le maximum de vraisemblance sous l’hypothèse nulle; cela permet d’éviter des ajustements coûteux lors de comparaisons. L’algorithme employé par SAS utilise une méthode de recherche arborescente dite méthode de séparation et d’évaluation, qui ne nécessite pas de tester tous les modèles; à noter que la solution à \\(k\\) variables n’est pas nécessairement imbriquée dans celle à \\(k+1\\) variables. Lorsque la taille d’échantillon tend vers l’infini, la statistique du rapport de vraisemblance et la statistique de score sont équivalentes. Choisir le modèle selon la statistique du score équivaut alors à choisir le modèle qui maximise la vraisemblance (ou qui minimise la quantité \\(-2 \\ell\\)). Ainsi, pour ce nombre fixé de variables, cela va donner le modèle avec le meilleur \\(\\mathsf{AIC}\\) (et \\(\\mathsf{BIC}\\)). Par conséquent, on peut trouver le meilleur modèle, globalement, en minimisant le \\(\\mathsf{AIC}\\) (ou le \\(\\mathsf{BIC}\\)) en faisant varier le nombre de variables. Par contre, cela n’est pas nécessairement vrai pour une taille d’échantillon finie. Le meilleur modèle selon le critère score n’est pas nécessairement celui qui maximise la vraisemblance. Mais en pratique, cette approximation est plus que suffisante et on va procéder comme on a fait avec la procédure reg.\nÀ la section précédente, nous avons inclus les 10 variables de base (14 avec les indicatrices pour les variables catégorielles) dans notre exemple d’envoi ciblé. Nous allons ici faire une recherche de type all-subset parmi ces 14 variables. Le code est dans le fichier logit5_selection_variables.sas.\n\n\nCode\nproc logistic data=train;\nmodel yachat(ref='0') = x1 x2 x3_1 x3_2 x4_1-x4_4 x5-x10 / \n  selection=score best=1;\nrun;\n\n\n\n\n\n\n\n\n\n\n\nLe meilleur modèle avec une seule variable, selon la statistique du score, est celui avec x8, le meilleur avec deux variables est celui avec x2 et x8, et ainsi de suite. Nous voulons ensuite choisir parmi ces 14 modèles, celui qui minimise le \\(\\mathsf{AIC}\\) ou le \\(\\mathsf{BIC}\\). Le problème est que ces critères ne sont pas fournis (contrairement aux sorties de la procédure reg). La solution longue consiste à ajuster chacun de ces modèles, à extraire le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\) et à ainsi trouver le meilleur modèle. Mais le faire manuellement en spécifiant plusieurs modèles est trop long. La macro logistic_aic_BIC_score, qui se trouve dans le fichier logit6_macro_all_subset.sas ajuste tous ces modèles automatiquement.\n\n\nCode\n%logistic_aic_BIC_score(yvariable=yachat,\n                        xvariables=x1 x2 x3_1 x3_2 x4_1-x4_4 x5-x10,\n                        dataset=train, minvar=1, maxvar=14);\n\n\n\n\n\n\n\n\n\n\n\nOn voit que le meilleur modèle selon le \\(\\mathsf{AIC}\\) a neuf variables, contre sept pour le \\(\\mathsf{BIC}\\). Nous verrons plus loin, dans un tableau synthèse, comment auraient performé ces modèles s’ils avaient été utilisés pour cibler les clients restants.\n\n\n6.7.2 Recherche séquentielle\nFaire une recherche de tous les sous-modèles possibles devient impraticable lorsqu’il y a trop de variables en jeu. La procédure logistic permet aussi une recherche de type séquentielle classique. Ceci permet aussi d’utiliser la même approche en deux temps présentée au chapitre précédent. Dans un premier temps, on fait une recherche séquentielle pour sélectionner un nombre de variables qui sera assez petit afin qu’une recherche exhaustive de tous les sous-modèles soit possible. Dans un second temps, on fait cette recherche avec ces variables uniquement. Idéalement, on débute la sélection avec le modèle qui contient toutes les variables, soit start=n où \\(n=104\\) dans notre cas. Si on inclut tous les termes quadratiques et les termes d’interactions d’ordre deux, nous avons 104 variables potentielles: c’est trop pour une recherche exhaustive.\nOn peut faire une recherche descendante avec le test du score pour réduire le nombre de variables à 50, puis passer les variables sélectionnées à la procédure logistic et faire une recherche exhaustive approximative. Le modèle qui a le plus petit \\(\\mathsf{AIC}\\), soit 585.194, est un modèle avec 27 variables. Le \\(\\mathsf{BIC}\\) mène à un modèle beaucoup plus parsimonieux qui inclut sept variables, pour une valeur de critère de 667.704.\n\n\n6.7.3 Algorithme glouton et critères alternatifs avec hpgenselect\nNous avons vu au chapitre précédent que la procédure glmselect permet de faire une recherche de type séquentielle avec un critère autre que la valeur-\\(p\\) du test de Wald pour rajouter ou retirer des variables explicatives du modèle final. Cette procédure est limitée à la régression linéaire, mais la procédure hpgenselect permet de faire une sélection de variables pour d’autres types de modèles, incluant la régression logistique.\nLe code suivant fait une recherche séquentielle en ajoutant ou retranchant les variables selon leur valeur-\\(p\\) (select=sl), la seule méthode disponible pour l’instant. En revanche, le modèle final peut-être choisi selon d’autres critères.\n\n\nCode\nproc hpgenselect data=train;\nclass x3(ref='3' split) x4(ref='5' split);\nmodel yachat(ref='0')=x1|x2|x3|x4|x5|x6|x7|x8|x9|x10 @2 \n x2*x2 x6*x6 x7*x7 x8*x8 x9*x9 x10*x10 /  \nlink=logit distribution=binary;\nselection method=stepwise(select=sl choose=sbc);\nrun;\n\n\nAvec le critère \\(\\mathsf{BIC}\\), on obtient 12 variables tandis que choose=aic donne 13 variables (seule la variable x41 est ajoutée). Il s’agit des mêmes variables que celles sélectionnées par une sélection séquentielle classique en prenant 0.05 comme critère d’entrée et de sortie."
  },
  {
    "objectID": "05-reglogistique.html#performance-des-différents-modèles-pour-lexemple-des-clients-cibles",
    "href": "05-reglogistique.html#performance-des-différents-modèles-pour-lexemple-des-clients-cibles",
    "title": "6  Régression logistique",
    "section": "6.8 Performance des différents modèles pour l’exemple des clients cibles",
    "text": "6.8 Performance des différents modèles pour l’exemple des clients cibles\nNous allons conclure, pour l’instant, notre exemple dans cette section, en évaluant la performance de différentes stratégies. Le critère de performance sera le suivant : revenu net de la stratégie si elle était appliquée aux 100 000 clients restants. Pour chacun des 100 000 clients à catégoriser, nous allons calculer la quantité suivante :\n\nSi le client n’est pas ciblé pour l’envoi d’un catalogue par le modèle, alors le revenu est nul.\nSi le client est ciblé pour l’envoi d’un catalogue par le modèle et qu’il n’achète rien, le revenu est de \\(-10\\)$ (le coût de l’envoi).\nSi le client est ciblé pour l’envoi d’un catalogue par le modèle et qu’il achète quelque chose, le revenu est de (ymontant\\(-10\\))$, c’est-à-dire, le montant qu’il dépense moins le \\(10\\)$ du coût de l’envoi.\n\nPour une stratégie donnée, chaque individu n’appartient qu’à une seule des catégories. Le revenu net de la stratégie est la somme des revenus pour les 100 000 clients. Parmi ces derniers, 23 179 auraient acheté si on leur avait envoyé le catalogue et ces clients auraient généré des revenus de 1 601 212$. Si on enlève le coût des envois (100 000 X 10$ = 1 000 000$), on obtient que la stratégie de référence permet un revenu net de 601 212$.\nNous allons investiguer deux types de stratégies :\n\nune basée sur la régression logistique seulement en utilisant le modèle pour prévoir l’achat et\nune basée sur la combinaison de la régression logistique et la régression linéaire en utilisant un modèle pour prévoir l’achat et un autre pour prévoir le montant.\n\n\n6.8.1 Stratégies en utilisant seulement la régression logistique\nDans ce cas, nous allons estimer la probabilité d’achat avec un modèle de régression logistique. Nous allons ensuite trouver le meilleur point de coupure, avec une matrice de gain adéquatement choisie, afin d’avoir une règle d’assignation optimale. Nous avons déterminé des modèles potentiels à la section précédente. De plus, nous avons déjà vu comment trouver le meilleur point de coupure en spécifiant une matrice de gain, afin de maximiser le gain moyen à partir de la matrice de gain du ?tbl-03-gain3. Nous allons donc trouver le meilleur point de coupure pour quelques-uns des modèles choisis à la section précédente, pour ensuite évaluer le revenu net de ces modèles.\nIl faut encore une fois bien comprendre qu’en pratique, on ne pourrait pas faire cette comparaison, car on ne sait pas d’avance si les clients futurs vont acheter ou non. Mais dans cet exemple, les variables yachat et ymontant sont fournies pour ces 100 000 clients afin qu’on puisse voir ce qui se serait passé avec les différentes stratégies.\nLa stratégie de référence est celle qui consiste à envoyer le catalogue aux 100 000 clients sans les sélectionner. Le tableau qui suit montre des statistiques pour les variables ymontant et yachat pour les 100 000 clients à scorer. Le ?tbl-03-summarylog résume la performance des différentes stratégies basées exclusivement sur le modèle logistique.\nTable:\n\n\n\nRésumé des caractéristiques des modèles logistiques avec (a) référence, soit l'envoi sans sélection à tous les clients; (b) 10 variables de base sans sélection; (c) toutes les variables, incluant les termes quadratiques et les interactions d'ordre 2; (d) sélection séquentielle classique avec critère d'entrée et de sortie à 0.05; (e) idem que (d), mais avec meilleur modèle selon le $\\mathsf{AIC}$; (f) idem que (d), mais avec meilleur modèle selon le $\\mathsf{BIC}$; (g) recherche exhaustive avec 50 variables sélectionnées par recherche séquentielle et modèle final sélectionné selon le $\\mathsf{BIC}$; (h), idem mais sélection avec $\\mathsf{AIC}$. Les points de coupure optimaux ont été déterminés par validation-croisée sur l'échantillon d'apprentissage, tandis que la performance du modèle (sensibilité, taux de faux positifs et taux de bonne classification) ont été calculés à partir de l'échantillon test de 100 000 individus.\n \n  \n    modèle \n    \\# variables \n    point de coupure \n     sensibilité \n    taux de faux positifs \n    taux de bonne classification \n    revenu net \n  \n \n\n  \n    (a) \n    --- \n    --- \n    100 \n    76.8 \n    23.2 \n    601212 \n  \n  \n    (b) \n    14 \n    0.12 \n    89 \n    56.2 \n    70.9 \n    940569 \n  \n  \n    (c) \n    104 \n    0.08 \n    85.8 \n    52.6 \n    74.6 \n    937150 \n  \n  \n    (d), (e) \n    13 \n    0.14 \n    85.7 \n    49.1 \n    77.5 \n    969350 \n  \n  \n    (f) \n    12 \n    0.19 \n    81 \n    44.7 \n    80.4 \n    943935 \n  \n  \n    (g) \n    8 \n    0.16 \n    86 \n    48.1 \n    78.3 \n    985069 \n  \n  \n    (h) \n    28 \n    0.15 \n    83.5 \n    47.4 \n    78.8 \n    952672 \n  \n\n\n\n\n\nNous avons vu plus tôt, qu’avec les 10 variables de base, le meilleur point de coupure est de 0.12. En utilisant cette stratégie sur les 100 000 clients, le revenu net aurait été de 940 569$. C’est une énorme amélioration, de plus de 56%, par rapport à la stratégie de référence qui consiste à envoyer le catalogue à tout le monde (revenu net de 601 212$). Si on inclut tous les termes quadratiques et les termes les interactions d’ordre deux (104 variables en tout), le revenu net est inférieur avec une valeur de 937 150$. Ici, le modèle est trop complexe et surajusté. Si on fait une sélection de variables (quasi méthodes sont présentées), suivie de la détermination du meilleur point de coupure, on fait alors toujours mieux qu’avec le modèle incluant les 10 variables de base seulement. L’approche qui aurait fait le mieux est la recherche séquentielle pour réduire le nombre de variables considérées à 50, suivi d’une recherche exhaustive pour trouver le modèle avec le plus petit \\(\\mathsf{BIC}\\) : cette approche aurait généré 985 069$ de revenus nets. Il s’agit d’un gain de 4.7% par rapport au modèle avec les 10 variables de base.\n\n\n6.8.2 Stratégies alternatives\nNous venons tout juste d’étudier des stratégies qui consistent essentiellement, à estimer \\(\\P{\\texttt{yachat}=1}\\) et un point de coupure afin de décider à qui envoyer le catalogue en partant du postulat que tous les clients dépensent le même montant; le tout est basé uniquement sur la régression logistique. Le revenu moyen peut être estimé à partir de l’équation \\[\\begin{align*}\n\\E{\\texttt{ymontant}} = \\E{\\texttt{ymontant} \\mid \\texttt{yachat}=1} \\P{\\texttt{yachat\n}=1},\n\\end{align*}\\]f c’est-à-dire, la moyenne du montant dépensé est égale à la moyenne du montant dépensé étant donné qu’il y a eu achat, fois la probabilité qu’il ait eu achat. Une autre stratégie possible consiste donc à développer deux modèles : un pour \\(\\E{\\texttt{ymontant} \\mid \\texttt{yachat}=1}\\) et un autre pour \\(\\P{\\texttt{yachat}=1}\\) et à les combiner afin d’obtenir des prévisions du montant dépensé.\nNous avons déjà développé des modèles de régression linéaire pour \\(\\E{\\texttt{ymontant} \\mid \\texttt{yachat}=1}\\) au chapitre précédent et nous venons de développer des modèles de régression logistique pour \\(\\P{\\texttt{yachat}=1}\\) dans ce chapitre. Nous avons donc tous les ingrédients pour implanter cette stratégie.\nNous allons cibler les clients dont la prévision du montant dépensé est plus grande que 10$ (le coût de l’envoi du catalogue).\nLes possibilités de modèles sont nombreuses. Par exemple, si on a cinq modèles potentiels pour \\(\\E{\\texttt{ymontant} \\mid \\texttt{yachat}=1}\\) et cinq pour \\(\\P{\\texttt{yachat}=1}\\), il y a 25 combinaisons possibles. Ici, nous allons seulement présenter les résultats pour deux combinaisons :\n\npour \\(\\texttt{ymontant}\\), nous allons utiliser les variables choisies par glmselect avec les options select=aic, choose=bic, tandis que pour \\(\\texttt{yachat}\\), nous allons utiliser les variables choisies par la procédure séquentielle suivie d’une recherche exhaustive avec le critère BIC\nà la fois pour \\(\\texttt{ymontant}\\) et \\(\\texttt{yachat}\\), nous allons utiliser les variables choisies en faisant une sélection séquentielle classique (tests-\\(t\\)) avec critères d’entrée et de sortie fixés à 0.05.\n\nPour obtenir les prévisions, nous allons estimer conjointement les modèles pour \\(\\E{\\texttt{ymontant} \\mid \\texttt{yachat}=1}\\) et pour \\(\\P{\\texttt{yachat}=1}\\) avec un modèle Tobit de type 2 (aussi appelé modèle Heckit), dont une brève description est donnée dans la section @ref(tobit2). L’avantage de l’estimation simultanée est que l’on a pas à sélectionner le point de coupure, puisque l’on enverra le catalogue uniquement si le montant prédit pour \\(\\E{\\texttt{ymontant}}\\) (non-conditionnel) est supérieur à 10$. Les résultats du modèle Tobit sur l’échantillon de validation sont rapportés dans le ?tbl-03-tobit.\n\n\n\nMatrice de gain pour l'envoi de catalogue avec des modèle Tobit de type II: sensibilité, taux de faux positifs et de bonne classification et gain net de la stratégie.\n \n  \n    modèle \n    sensibilité \n    FP (\\%) \n    bonne classification (\\%) \n    revenu net \n  \n \n\n  \n    (1) \n    88.3 \n    50.9 \n    76.1 \n    997 238 \n  \n  \n    (2) \n    86.3 \n    49.9 \n    76,9 \n    977 422 \n  \n\n\n\n\n\nIl s’avère qu’on aurait eu des performances semblables aux stratégies basées uniquement sur la régression logistique vues à la sous-section précédente. La première combinaison aurait tout de même produit un revenu net de 997 238$, supérieur au revenu net de 985 069$, qui était le meilleur trouvé à la sous-section précédente.\nPour conclure cet exemple, il s’avère donc que la régression logistique permet d’effectuer un bon ciblage des clients potentiels afin de maximiser les revenus. L’approche générale consistant à obtenir des prévisions pour \\(\\P{\\texttt{yachat}=1}\\) et ensuite trouver le meilleur point de coupure est très générale. D’autres types de modèles (arbre de classification, forêt aléatoire, réseau de neurones) pourraient être utilisés à la place de la régression logistique.\nNous reviendrons une dernière fois sur cet exemple dans le chapitre traitant des données manquantes. Nous verrons alors comment procéder si des valeurs manquantes sont présentes dans les variables explicatives.\n\n\n6.8.3 Modèle Tobit de type 2\nCette partie est plus technique et peut être omise.\nIl ne serait pas justifié d’ajuster séparément les deux modèles pour \\(\\E{\\texttt{ymontant} \\mid \\texttt{yachat}=1}\\) et \\(\\P{\\texttt{yachat}=1}\\) et de calculer les prévisions en prenant le produit: \\(\\E{\\texttt{ymontant} \\mid \\texttt{yachat}=1}\\P{\\texttt{yachat}=1}\\). Cela provient du fait que le modèle pour \\(\\E{\\texttt{ymontant} \\mid \\texttt{yachat}=1}\\) aurait été estimé seulement avec les clients qui ont acheté quelque chose et qu’ensuite on l‘appliquerait (au moment de calculer les prévisions) à la fois aux clients qui vont acheter et à ceux qui ne vont pas acheter. Il y a donc un biais de sélection dans l’échantillon qui a servi à ajuster le modèle au départ. Une manière de contourner ce problème est d’ajuster conjointement les deux modèles. Le modèle de Tobit de type 2 permet de faire cela. Ce modèle est basé sur l’hypothèse que les deux variables observées (\\(Y_1\\) et \\(Y_2\\)) proviennent de deux variables latentes non observées (\\(Y_1^{\\star}\\) et \\(Y_2^{\\star}\\)), où \\[\\begin{align*}\nY_1 = \\begin{cases}\n1 & \\text{ si } Y_1^{\\star} \\ge 0, \\\\\n0 & \\text{ si } Y_1^{\\star} < 0,\n\\end{cases}\n\\qquad \\qquad\nY_2 = \\begin{cases}\nY_2^{\\star} & \\text{ si } Y_1^{\\star} \\ge 0, \\\\\n0 & \\text{ si } Y_1^{\\star} < 0.\n\\end{cases}\n\\end{align*}\\] Dans notre exemple, \\(Y_1\\) correspond à \\(\\texttt{yachat}\\) et \\(Y_2\\) à \\(\\texttt{ymontant}\\). Ce qui lie les deux équations est le fait qu’on suppose que les variables sont binormales: les deux termes d’erreur sont de loi normale et sont corrélés, \\(\\boldsymbol{\\varepsilon} \\sim \\mathsf{No}_2(\\boldsymbol{0}_2, \\boldsymbol{\\Sigma})\\). Les variables dépendantes observées sont : \\[\\begin{align*}\nY_{1}^{\\star} &= \\beta_{01} + \\beta_{11} \\mathrm{X}_{11} + \\cdots + \\beta_{1p}\\mathrm{X}_{p1} + \\varepsilon_{1}\\\\\nY_{2}^{\\star} &= \\beta_{02} + \\beta_{12} \\mathrm{X}_{12} + \\cdots + \\beta_{1p}\\mathrm{X}_{q2} + \\varepsilon_{2}\n\\end{align*}\\] Notez que les variables explicatives ne sont pas nécessairement les mêmes dans les deux équations. En estimant conjointement les deux équations, on élimine le biais de sélection mentionné plus haut. La procédure qlim permet d’estimer ce modèle. Cependant, qlim ne fait pas de sélection de variables. Le choix des variables doit être fait avant avec les méthodes qu’on a vues. De plus, pour être précis, le modèle Tobit ajuste un modèle probit et non logistique à la variable binaire."
  },
  {
    "objectID": "05-reglogistique.html#extensions-du-modèle-de-régression-logistique-à-plus-de-deux-catégories",
    "href": "05-reglogistique.html#extensions-du-modèle-de-régression-logistique-à-plus-de-deux-catégories",
    "title": "6  Régression logistique",
    "section": "6.9 Extensions du modèle de régression logistique à plus de deux catégories",
    "text": "6.9 Extensions du modèle de régression logistique à plus de deux catégories\nSupposons que la variable \\(Y\\) que vous cherchez à modéliser est une variable catégorielle pouvant prendre trois valeurs ou plus. Voici quelques exemples :\n\nDestination de vacances l’année dernière (Québec, États-Unis, ailleurs).\nSi les élections avaient lieu aujourd’hui au Québec, pour quel parti voteriez-vous (PLQ, PQ, CAQ, QS).\nCombien de fois êtes-vous allé au cinéma l’année dernière: moins de cinq fois (\\(\\texttt{1}\\)), entre cinq et 10 fois (\\(\\texttt{2}\\)), ou plus de 10 fois (\\(\\texttt{3}\\)).\nQuelle importance accordez-vous au service après-vente? Un parmi « pas important » (\\(\\texttt{1}\\)), « peu important »(\\(\\texttt{2}\\)), « moyennement important » (\\(\\texttt{3}\\)), « assez important » (\\(\\texttt{4}\\)), « très important » (\\(\\texttt{5}\\)).\n\nDans les deux premiers exemples, la variable réponse \\(Y\\) est nominale (elle n’a pas d’ordre) alors qu’elle est ordinale dans les deux derniers. Pour une variable ordinale, le modèle logit multinomial peut être utilisé mais il existe d’autres possibilités comme le modèle logit cumulé. Nous couvrirons ces deux modèles.\n\n6.9.1 Régression logistique multinomiale\nAfin de simplifier la notation, on suppose qu’il y a une seule variable explicative \\(X\\) à disposition et que la variable \\(Y\\) représente trois catégories, une parmi 0, 1 et 2.\nEn régression logistique, \\(Y\\) est une variable binaire qui vaut soit 0, soit 1 et la probabilité de succès est \\[\\begin{align*}\n\\ln\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 \\mathrm{X}_{i}, \\qquad p_i = \\P{Y_i=1 \\mid \\mathrm{X}_i} = \\expit(\\beta_0 + \\beta_1\\mathrm{X}_i).\n\\end{align*}\\] Dans ce modèle logistique, \\(\\ln(p_i)-\\ln(1-p_i) = \\ln\\{\\P{Y_i=1 \\mid \\mathrm{X}_i}\\} - \\ln\\{\\P{Y_i=0 \\mid \\mathrm{X}_i}\\}\\) peut être vu comme étant le logit de la catégorie 1 en utilisant 0 comme catégorie de référence. Le modèle logistique multinomial procède de même en fixant une catégorie de référence et en modélisant le logit de chacune des autres catégories par rapport à la catégorie de référence. Avec \\(K+1\\) catégories et en choisissant la catégorie 0 comme référence, le modèle devient \\[\\begin{align*}\n\\ln\\left(\\frac{p_{i1}}{p_{i0}}\\right) = \\beta_{01} + \\beta_{11} \\mathrm{X}_i, \\, \\ldots, \\, \\ln\\left(\\frac{p_{iK}}{p_{i0}}\\right) = \\beta_{0K} + \\beta_{1K} \\mathrm{X}_i,\n\\end{align*}\\] où \\(p_{ik} = \\P{Y_i=k \\mid \\mathrm{X}_i}\\) \\((k=0, \\ldots, K)\\). Comme en régression logistique, on peut facilement exprimer ce modèle en termes des différentes probabilités, \\[\\begin{align*}\np_{i0} &= \\P{Y_i=0 \\mid \\mathrm{X}_i} = \\frac{1}{1+ \\sum_{j=1}^K\\exp(\\beta_{0j}+\\beta_{1j}\\mathrm{X}_i)}\\\\\np_{ik} &= \\P{Y_i=k \\mid \\mathrm{X}_i} = \\frac{\\exp(\\beta_{0k}+\\beta_{1k}\\mathrm{X}_i)}{1+ \\sum_{j=1}^K\\exp(\\beta_{0k}+\\beta_{1k}\\mathrm{X}_i)}, \\qquad k =1, \\ldots, K.\n\\end{align*}\\] On voit facilement que la somme des probabilités égale 1, c’est-à-dire \\(p_{i0} + \\cdots + p_{iK} = 1\\). En fait, le modèle logit multinomial ne fait que combiner plusieurs logit dans un seul modèle. L’interprétation des paramètres se fait comme en régression logistique sauf qu’il faut y aller équation par équation.\nDestination vacances. Le fichier logit6.sas7bdat contient 100 observations obtenues par voie de sondage auprès d’adultes âgés de 18 à 45 ans. Le fichier contient les réponses aux questions suivantes:\n\ny1: quelle a été votre destination vacances l’année dernière: Québec (\\(\\texttt{0}\\)), États-Unis (\\(\\texttt{1}\\)) ou ailleurs (\\(\\texttt{2}\\))?\ny2: combien de fois êtes-vous allé au cinéma l’année dernière: moins de 5 fois (\\(\\texttt{1}\\)), entre 5 et 10 fois (\\(\\texttt{2}\\)), ou plus de 10 fois (\\(\\texttt{3}\\)).\nx: âge (en année) du répondant.\n\nNous allons modéliser la destination vacance \\(Y_1\\) à l’aide d’une régression logistique multinomiale avec l’âge comme variable explicative.\n\n\n\n\n\n\n\n\n\nOn voit que les gens qui ont passé leurs vacances au Québec (\\(Y_1=0\\)) ont 26.5 ans en moyenne. Ils sont plus jeunes que ceux qui ont passé leurs vacances aux États-Unis (âge moyen de 33 ans). Finalement, ceux dont la destination vacances était ailleurs sont les plus vieux avec une moyenne de 37.3 ans.\nPour le modèle logit multinomial, nous allons prendre \\(Y_1=0\\) comme catégorie de référence. Les commandes sont\n\n\nCode\nproc logistic data=multi.logit6 ;\nmodel y1(ref='0') = x / clparm=pl clodds=pl expb link=glogit;\nrun; \n\n\nL’option link=glogit spécifie le type de fonction de lien, ici celle du modèle logistique multinomial.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComme il y a trois catégories pour la variable dépendante, il y a deux équations pour le modèle ajusté, \\[\\begin{align*}\n\\ln \\left\\{\\frac{\\P{Y_{1i}=1 \\mid \\mathrm{X}_i}}{\\P{Y_{1i}=0 \\mid \\mathrm{X}_i}} \\right\\} = -4.10 + 0.13\\mathrm{X}_i, \\qquad \\qquad \\ln \\left\\{\\frac{\\P{Y_{1i}=2 \\mid \\mathrm{X}_i}}{\\P{Y_{1i}=0 \\mid \\mathrm{X}_i}} \\right\\} = -7.98+0.22\\mathrm{X}_i.\n\\end{align*}\\]\nPlus l’âge du répondant augmente, plus la probabilité qu’il ait passé ses vacances aux États-Unis par rapport au Québec augmente. En fait, pour chaque augmentation de 1 de l’âge, le rapport des cote pour \\(Y_1=1\\) par rapport à \\(Y_1=0\\) est multipliée par \\({1.133}=\\exp({0.1253})\\). Cette valeur est donnée dans la dernière colonne du tableau. De plus, cet effet est significatif car la valeur-\\(p\\) est inférieure à \\(10^{-4}\\).\nDe même, plus l’âge du répondant augmente, plus la probabilité qu’il ait passé ses vacances ailleurs qu’aux États-Unis ou au Québec par rapport au Québec augmente. En fait, pour chaque augmentation de l’âge d’un an, le rapport des cote pour \\(Y_1=1\\) par rapport à \\(Y_1=0\\) est multiplié par \\({1.25}\\). Cet effet est également statistiquement significatif.\nNous venons donc de comparer chacune des catégories \\(Y_1=1\\) et \\(Y_1=2\\) à la catégorie de référence \\(Y_1=0\\). Pour une comparaison directe entre \\(Y_1=1\\) et \\(Y_1=2\\), il suffit de changer la catégorie de référence et de resoumettre le programme SAS.\n\n\n6.9.2 Régression logistique cumulative à cotes proportionnelles\nSi les modalités de la réponse sont ordinales, la régression logistique multinomiale est toujours appropriée. Il peut néanmoins être préférable d’utiliser un modèle qui utilise l’ordre des modalités pour obtenir un modèle plus facile à interpréter et plus parcimonieux. Le modèle de régression logistique cumulative à cotes proportionnelles est un simplification sous l’hypothèse que les cotes sont proportionnelles.\nSupposons que la variable \\(Y\\) est ordinale et peut prendre les \\(K+1\\) valeurs ordonnées de la plus petite à la plus grande selon les catégories de \\(Y\\) (\\(0, 1, 2, \\ldots, K\\)). Supposons que l’on dispose de \\(p\\) variables explicatives \\(\\mathrm{X}_1, \\ldots, \\mathrm{X}_p\\).\nSoit \\(p_{ik}=\\P{Y_i=k \\mid \\mathrm{X}_{i1}, \\ldots, \\mathrm{X}_{ip}}\\) (\\(k=0, 1, \\ldots, K\\)) la probabilité que \\(Y_{ik}\\) prenne la valeur \\(k\\). On dénote \\[\\begin{align*}\nS_{ij}=\\sum_{k=j}^K p_{ik}= \\P{Y_{i} > j - 1 \\mid \\mathrm{X}_{i1}, \\ldots, \\mathrm{X}_{ip}}, \\qquad j=1, \\ldots, K.\n\\end{align*}\\] La quantité \\(S_{ij}\\) est la probabilité que \\(Y_i\\) soit plus grand ou égal à \\(j\\); \\(S_{i0}\\) est égal à 1 et \\(S_{iK} = \\P{Y_i=K \\mid \\mathrm{X}_{i1}, \\ldots, \\mathrm{X}_{ip}}=p_{iK}\\).\nLe modèle logistique cumulé spécifie que \\[\\begin{align*}\n\\ln \\left( \\frac{S_{ij}}{1-S_{ij}}\\right) = \\beta_{0j} + \\beta_1 \\mathrm{X}_{i1} + \\cdots + \\beta_p \\mathrm{X}_{ip}, \\qquad \\qquad  j=1, \\ldots, K.\n\\end{align*}\\]\nIl y a donc \\(K\\) équations logistiques. Les paramètres quantifiant les effets des variables explicatives, \\(\\beta_1, \\ldots, \\beta_p\\) sont les mêmes pour chacune des log-cotes, mais il y a une ordonnée à l’origine différente par log de rapport de cote. Par conséquent, pour modéliser une variable ordinale \\(Y\\) ayant \\(K+1\\) valeurs possibles avec \\(p\\) variables explicatives, le modèle cumulatif logistique utilise \\(p + K\\) paramètres. Le modèle logit multinomial, qui peut également être utilisé pour les données ordinales, utilise \\(K \\cdot(p+1)\\) paramètres. Le modèle multinomial ordonné est donc plus parcimonieux et, pour autant qu’il soit approprié, mènera à des estimations des paramètres plus précises qu’avec le modèle de régression logistique multinomiale. Les deux modèles sont identiques au modèle de régression logistique si la variable ordinale a deux modalités.\nLa cote \\(S_{ij}/(1-S_{ij})\\) mesure à quel point il est plus probable que \\(Y_i\\) prenne une valeur plus grande ou égale à \\(j\\) par rapport à une valeur plus petite que \\(j\\), viz. \\[\\begin{align*}\n\\frac{S_{ij}}{1-S_{ij}} = \\exp( \\beta_{0j} + \\beta_1\\mathrm{X}_{i1} + \\cdots + \\beta_p \\mathrm{X}_{ip}).\n\\end{align*}\\] Dans cet exemple, aucune fonction autre qu’additive en \\(X\\), ni aucune interaction n’est présente. Si le paramètre \\(\\beta_j\\) est positif, cela indique que plus \\(\\mathrm{X}_j\\) prend une valeur élevée, plus la variable \\(Y\\) a tendance à prendre aussi une valeur élevée. Inversement, si le paramètre \\(\\beta_j\\) est négatif, cela indique que plus \\(\\mathrm{X}_j\\) prend une valeur élevée, plus la variable \\(Y\\) a tendance à prendre une valeur basse. Plus précisément, pour chaque augmentation d’une unité de \\(\\mathrm{X}_j\\), la cote \\(S_k/(1-S_k)\\) est multipliée par \\(\\exp(\\beta_j)\\), peu importe la valeur de \\(Y\\). Ainsi, la cote d’être dans une catégorie plus élevée, par rapport à une catégorie moins élevée, est multipliée par \\(\\exp(\\beta_j)\\). En terme de probabilités cumulées d’excéder \\(k\\), \\[\\begin{align*}\nS_{ik} = \\P{Y_i \\geq k \\mid \\mathrm{X}_{i1}, \\ldots, \\mathrm{X}_{ip}} = \\expit(\\beta_{0k} + \\beta_1 \\mathrm{X}_{i1} + \\cdots + \\beta_p \\mathrm{X}_{ip}), \\qquad j =1, \\ldots, K.\n\\end{align*}\\] En utilisant ces expressions, on peut obtenir la probabilité de chaque catégorie, \\[\\begin{align*}\n\\P{Y_i = k \\mid \\mathrm{X}_{i1}, \\ldots, \\mathrm{X}_{ip}} =\\P{Y_i \\geq k \\mid \\mathrm{X}_{i1}, \\ldots, \\mathrm{X}_{ip}} -\\P{Y_i \\geq k+1 \\mid \\mathrm{X}_{i1}, \\ldots, \\mathrm{X}_{ip}} = S_{k} - S_{k+1}.\n\\end{align*}\\]\nNous considérons maintenant la variable \\(Y_2\\) du fichier logit6.sas7bdat, qui donne le nombre de visites au cinéma. Pour cet exemple, nous allons chercher à modéliser \\(Y_2\\) à l’aide de \\(X\\) (âge) en utilisant le modèle multinomial cumulé à cotes proportionnelles.\n\n\n\n \n  \n    y2 \n    n \n    moyenne \n    écart-type \n    minimum \n    maximum \n  \n \n\n  \n    1 \n    44 \n    33.50 \n    7.23 \n    18 \n    44 \n  \n  \n    2 \n    38 \n    30.45 \n    8.16 \n    18 \n    44 \n  \n  \n    3 \n    18 \n    25.11 \n    6.58 \n    18 \n    39 \n  \n\n\n\n\n\nAinsi, les répondants qui sont allés moins de cinq fois au cinéma ont en moyenne 33.5 ans, ceux qui sont allés entre cinq et 10 fois ont 30.4 ans en moyenne, et ceux qui sont allés plus de 10 fois ont 25.1 ans en moyenne. Il y a une progression et on voit que les répondants plus jeunes vont plus souvent au cinéma.\nNous utilisons exactement le même programme que pour une régression logistique habituelle. Si la variable \\(Y\\) prend plus de deux valeurs, SAS utilisera automatiquement le modèle de régression multinomiale cumulé.\n\n\nCode\nproc logistic data=multi.logit6 descending;\nmodel y2 = x / clparm=pl clodds=pl expb;\nrun;\n\n\nL’option descending impose la paramétrisation discutée précédemment. Sans cette option, ce serait plutôt les probabilités de prendre une valeur plus petite, par rapport à une plus grande qui serait modélisée. Le modèle est le même, mais les signes des paramètres des variables explicatives seraient inversés.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvant toute chose, il faut s’assurer que le modèle est approprié. Rappelez-vous que l’une des hypothèses de ce modèle est que les effets des variables explicatives sont les mêmes pour chaque équation. Le tableau « Test de score pour l’hypothèse des cotes proportionnelles » est un test de l’hypothèse nulle\n\n\\(\\Hy_0\\) : l’effet de chaque variable est le même pour les \\(K\\) logit du modèle multinomial logistique, soit \\(\\beta_{11} = \\cdots =\\beta_{1K}\\), \\(\\ldots\\), \\(\\beta_{p1} = \\cdots =\\beta_{pK}\\).\n\nUne très petite valeur-\\(p\\) (rejet de \\(\\Hy_0\\)) pour ce test serait une indication que le modèle de régression multinomiale ordinale n’est pas approprié. Comme la valeur-\\(p\\) est 0.2577 ici, on ne rejette pas \\(\\Hy_0\\) et il n’y a pas lieu de douter de cette hypothèse. On peut donc aller de l’avant et interpréter le modèle.\nIci, l’effet estimé de l’âge (\\(X\\)) est \\(-{0.0916}\\) et ce paramètre est significativement différent de zéro (valeur-\\(p\\) de 0.0004). Rappelez-vous que \\(Y_2\\) représente le nombre d’entrées au cinéma dans la dernière année.\nAinsi, plus l’âge augmente, plus \\(Y_2\\) a tendance à prendre une petite valeur, c’est-à-dire, plus la personne a tendance à aller moins souvent au cinéma. Plus précisément, lorsque l’âge augmente de 1, la cote d’être dans une catégorie plus élevée de \\(Y_2\\), par rapport à une catégorie plus basse, est multipliée par 0.912 (la cote diminue donc et aussi la probabilité d’être dans une catégorie plus élevée)."
  },
  {
    "objectID": "06-survie.html",
    "href": "06-survie.html",
    "title": "7  Analyse de survie",
    "section": "",
    "text": "\\[\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\eps}{\\varepsilon}\n\\newcommand{\\Rlang}{\\textsf{R}}\n\\newcommand{\\SAS}{\\textsf{SAS}}\n\\newcommand{\\Sp}{\\mathscr{S}}\n\\renewcommand{\\P}[1]{{\\mathsf P}\\left(#1\\right)}\n\\newcommand{\\E}[1]{{\\mathsf E}\\left(#1\\right)}\n\\newcommand{\\Va}[1]{{\\mathsf{Var}}\\left(#1\\right)}\n\\newcommand{\\Cor}[1]{{\\mathsf{Cor}}\\left(#1\\right)}\n\\newcommand{\\I}[1]{{\\mathbf 1}_{#1}}\n\\newcommand{\\expit}{\\mathrm{expit}}\n\\newcommand{\\logit}{\\mathrm{logit}}\n\\newcommand{\\code}[1]{\\texttt{#1}}\n\\newcommand{\\Hy}{\\mathcal{H}}\n\\renewcommand{\\d}{\\mathrm{d}}\n\\]"
  },
  {
    "objectID": "06-survie.html#introduction",
    "href": "06-survie.html#introduction",
    "title": "7  Analyse de survie",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nLe but de cette section est d’étudier l’effet de variables explicatives sur le temps de survie. Plusieurs méchanismes de survie peuvent impacter la survie, de façon aléatoire ou pas. Considérons l’exemple d’une étude sur le chômage dû à la crise du coronavirus. On s’intéresse à tous ceux qui étaient en recherche active d’emploi entre mars et juin; seuls ceux qui étaient au chômage durant cette période seront considérés. Certaines personnes seront déjà au chômage en avril et donc leur durée de chômage sera plus longue (troncature à gauche). Lors de notre suivi, d’autre mentionneront avoir trouvé un emploi lors de notre appel, mais ne pourront nous renseigner sur la date exacte de leur embauche (censure à gauche); cette dernière précèdera notre prise de contact, mais nous est inconnue. D’autres personnes seront toujours au chômage en juin à la fin de l’étude et on ignorera le nombre réel de mois passés au chômage (censure à droite). Enfin, certaines personnes cesseront de chercher activement un emploi et donc quitteront l’étude. Tous ces méchanismes (complexes) peuvent être dictés par certaines covariables (employabilité, découragement) et être aléatoires ou pas. Pour estimer le taux de chômage, il faudra prendre en compte les méchanismes de survie dans notre modèle. On se concentrera sur le cas simple des données censurées à droite de façon aléatoire.\n\n7.1.1 Exemple du temps d’abonnement\nUne entreprise oeuvrant dans le secteur des télécommunications s’intéresse aux facteurs influençant le temps qu’un client reste abonné à son service de téléphone cellulaire. Des données provenant d’un échantillon de clients se trouvent dans le fichier survival1.sas7bdat, qui contient les variables suivantes:\n\n\\(\\texttt{t}\\): temps (en semaines) que le client est resté abonné au service de téléphone cellulaire. Il s’agit du vrai temps si le client n’est plus abonné et d’un temps censuré à droite si le client est toujours abonné.\n\\(\\texttt{censure}\\): variable binaire qui indique si la variable \\(\\texttt{t}\\) est censurée (\\(1\\) si le client est toujours abonné) ou non (\\(0\\), la variable \\(\\texttt{t}\\) est la durée finale de l’abonnement).\n\\(\\texttt{age}\\): âge du client au début de l’abonnement.\n\\(\\texttt{sexe}\\): sexe du client, soit femme (\\(1\\)), soit homme (\\(0\\)).\n\\(\\texttt{service}\\): nombre de services en plus du cellulaire auquel le client est abonné parmi internet, téléphone fixe, télévision (câble ou antenne parabolique).\n\\(\\texttt{region}\\): région où habite le client en ce moment (valeurs entre 1 et 5).\n\n\n\n7.1.2 Contexte\nOn s’intéresse au temps avant qu’un événement survienne. On observe chaque sujet jusqu’à ce que l’une des deux choses suivantes se produise : l’événement survient avant la fin de la période d’observation ou bien l’étude se termine et l’événement n’est toujours pas survenu. Dans l’exemple, l’événement correspond au fait d’interrompre son abonnement. On dispose donc d’une variable « temps », que l’on nomme \\(T\\), pour chaque individu qui est soit censurée soit non censurée. Si l’individu a expérimenté l’événement avant la fin de la période d’observation, la valeur de \\(T\\) n’est pas censurée. Si l’événement n’est toujours pas survenu à la fin de la période d’observation, la valeur de \\(T\\) est censurée. Pour chaque individu, on dispose également d’un ensemble de variables explicatives \\(X_1, \\ldots, X_p\\). Pour l’instant, supposons que les valeurs de ces variables sont fixes dans le temps mais on reviendra plus loin au cas où leurs valeurs peuvent varier dans le temps. Bien que le terme analyse de survie semble implicitement référer à la santé, de nombreux autres exemples sont envisageables\n\ntemps qu’un client demeure abonné à un service offert par notre compagnie.\ntemps de survie d’un individu après avoir été diagnostiqué avec un certain type de cancer.\nancienneté d’un travailleur au service d’une compagnie.\ndurée de vie d’une franchise.\ntemps avant la faillite d’une entreprise (ou d’un particulier).\ntemps avant le prochain achat d’un client.\ntemps durant lequel un(e) employé(e) est au chômage.\n\nSi aucune observation n’est censurée, c’est-à-dire, si on a observé le « vrai » temps pour chaque sujet, on pourrait alors simplement modéliser \\(T\\) en incluant des covariables dans les paramètres de vraisemblance d’une loi positive (par exemple, une régression log-linéaire). En revanche, si des observations sont censurées dans l’échantillon, leur omission biaiserait l’analyse.\nCe chapitre se veut une introduction à l’analyse de données de survie. Comme le développement de la théorie de l’analyse de survie est assez complexe (plus encore que celle de la régression linéaire ou logistique), on s’intéressera ici uniquement aux principes de base afin d’être en mesure d’appliquer les méthodes et de bien interpréter les résultats. Plusieurs extensions sont également possibles. Un survol de ces dernières sera effectué dans des sections plus loin.\nIl existe deux grandes approches pour analyser des données de survies :\n\nnonparamétrique ou semi-paramétrique: estimateur de Kaplan-Meier, modèle de Cox (à risques proportionnels).\nparamétrique : modèle paramétrique avec loi continue (Weibull, log-normal, log-logistique, gamma).\n\nNous discuterons seulement de la première approche dans ce chapitre. Le tableau suivant fait une analogie entre ce que nous ferons dans ce chapitre et des méthodes que vous connaissez.\n\n\n\n\n\n\n\n\n\nréponse \\(Y\\)\nrésumé descriptif\ncomparaison de deux groupes\nmodèle général\n\n\n\n\ncontinue\nmoyenne\ntest-\\(t\\) pour deux échantillons\nrégression linéaire\n\n\nbinaire\nproportion\ntest d’indépendance du khi-deux\nrégression logistique\n\n\ntemps de survie (censure à droite)\nfonction de survie temps de survie médian\ntest log-rang test de Wilcoxon généralisé (Gehan)\nmodèle de Cox\n\n\n\nLa structure de données de base que l’on doit avoir pour travailler en SAS (et avec la plupart des autres logiciels) est la suivante:\n\nune variable temps, \\(T\\).\nune variable \\(C\\) (censure), qui vaut un si l’observation est censurée et zéro sinon.\nd’autres variables explicatives \\(X_1, \\ldots, X_p\\), une par colonne."
  },
  {
    "objectID": "06-survie.html#fonctions-de-survie-et-de-risque",
    "href": "06-survie.html#fonctions-de-survie-et-de-risque",
    "title": "7  Analyse de survie",
    "section": "7.2 Fonctions de survie et de risque",
    "text": "7.2 Fonctions de survie et de risque\nUn des éléments de base d’une analyse de survie (survival analysis) est la fonction (ou courbe) de survie. Soit \\(F(t)=\\P{T \\leq t}\\) la fonction de répartition du temps de survie \\(t\\) et \\(f(t) = \\d/ \\d t F(t)\\). La fonction de survie est \\[\\begin{align*}\nS(t)= \\P{T > t} = 1-F(t)\n\\end{align*}\\] et donne la probabilité que le temps de survie soit supérieur à \\(t\\). On verra plus loin comment estimer cette fonction avec un échantillon et comment tester l’égalité de deux (ou plusieurs) fonctions de survie.\nLa fonction de risque (en anglais, hazard) est \\[\\begin{align*}\nh(t) =  \\frac{f(t)}{S(t)}\n\\end{align*}\\] où \\(f(t)\\) est la fonction de densité (pour \\(T\\) continu) ou de masse pour \\(T\\) discret. Dans le cas discret où le temps peut seulement prendre les valeurs \\(0, 1, 2, \\ldots\\), la fonction de risque est donc simplement la probabilité que l’événement survienne au temps \\(t\\), étant donné qu’il n’était pas survenu avant, \\(\\P{T=t \\mid T > t} = \\P{T=t} / \\P{T >t} = f(t)/S(t)\\); c’est une probabilité conditionnelle. Dans le cas général, la fonction de risque est nécessairement positive mais peut prendre des valeurs supérieures à un. On ne peut donc pas, à strictement parler, la voir comme une probabilité et c’est pourquoi on parle plutôt de risque. En fait, cette fonction mesure le risque instantané que l’événement survienne au temps \\(t\\), étant donné qu’il n’était pas survenu avant.\nCette fonction est importante car il s’agit de celle que nous allons modéliser avec le modèle de régression de Cox. Si, en régression logistique, on modélise le logarithme des cotes, on modélise plutôt la fonction de risque en analyse de survie. Les fonctions de survie et de risque sont intimement reliées et \\[\\begin{align*}\nh(t) = - \\frac{\\d \\ln\\{S(t)\\}}{\\d t}, \\qquad \\qquad S(t) = \\exp \\left\\{ -\\int_0^t h(u) \\d u\\right\\}.\n\\end{align*}\\] Ainsi, si on connaît la fonction de survie, on peut retrouver la fonction de risque et vice-versa. Par conséquent, un modèle pour la fonction de survie spécifie une fonction de risque (et vice-versa)."
  },
  {
    "objectID": "06-survie.html#estimation-dune-courbe-de-survie-et-de-risque",
    "href": "06-survie.html#estimation-dune-courbe-de-survie-et-de-risque",
    "title": "7  Analyse de survie",
    "section": "7.3 Estimation d’une courbe de survie et de risque",
    "text": "7.3 Estimation d’une courbe de survie et de risque\nL’estimateur nonparamétrique le plus couramment utilisé pour l’estimation de la fonction de survie en présense de censure à droite est l’estimateur de Kaplan–Meier. De plus, cette méthode est nonparamétrique en ce sens qu’on ne suppose aucun modèle et qu’on suppose uniquement que la censure est non-informative. Dans SAS, la procédure lifetest fournit l’estimation de Kaplan–Meier de la courbe de survie et permet aussi d’estimer la fonction de risque correspondante.\nSi l’échantillon ne contient aucune observation censurée (on a des temps exacts pour tous les sujets), l’estimateur de Kaplan–Meier de la fonction de survie à un temps \\(t\\) donné est alors simplement la proportion des observations dans l’échantillon qui possède un temps de survie supérieur à \\(t\\). Par convention, on considère qu’une observation censurée à droite faisait partie de l’ensemble d’observations à risque au temps de censure observé.\nOn considère l’exemple des temps d’abonnement pour illustrer le concept; les commandes SAS sont dans survival1_fonction_survie.sas. L’estimation de la fonction de survie selon la méthode Kaplan–Meier est obtenue grâce aux commandes suivantes:\n\n\nCode\nproc lifetest data=multi.survival1 method=km\n       plots=survival(cl nocensor);\ntime t*censure(1);\nrun;\n\n\nLe premier tableau renvoyé par SAS contient une ligne par observation et permet de lire l’estimé de la fonction de survie. Par exemple, on voit que l’estimation de la probabilité que le temps d’abonnement soit supérieur à 30 semaines est \\(\\widehat{S}(30)=0,986\\).\n\n\n\n\n\nFigure 7.1: ?(caption)\n\n\n\n\n\\[\\begin{align*} \\vdots \\end{align*}\\]\n\n\n\n\n\nFigure 7.2: ?(caption)\n\n\n\n\nLa sortie contient également un tableau contenant les quartiles. On utilise généralement le temps de survie médian (au lieu de la moyenne) dans ce type d’étude. Ici, l’estimé du temps de survie médian est de 114 semaines: on estime que la moitié des clients vont avoir une durée d’abonnement supérieure à 114 semaines. De même, la moitié des clients vont avoir une durée d’abonnement inférieure à 114 semaines. Un intervalle de confiance de niveau 95% pour ce temps médian est [\\(110; 119\\)].\nUn estimé de la moyenne et de l’écart-type est donné, mais ce dernier est biaisé (trop bas) puisque les données censurées ne donnent qu’une borne inférieure pour la vraie valeur. Avec un modèle paramétrique pour la survie (par ex., une loi exponentielle), les paramètres estimés du modèle dicteraient ces deux valeurs. Le modèle de Kaplan–Meier estime la survie, mais si la plus grande observation est censurée, la courbe n’atteindra pas zéro.\nLe graphique de la fonction de survie permet de lire le temps de survie pour une probabilité donnée. Les bandes donnent un intervalle de confiance ponctuel de niveau 95% pour chaque temps donné.\n\n\n\n\n\nFigure 7.3: ?(caption)\n\n\n\n\nLe tableau Figure 7.4 donne le nombre de données censurées: parmi les 500 observations, il y a 334 clients qui ont terminé leur abonnement et 166 qui sont censurées (le client est toujours abonné et le temps est donc une borne inférieure de la durée d’abonnement).\n\n\n\n\n\nFigure 7.4: Fraction de valeurs censurées\n\n\n\n\n\n7.3.1 Calcul de l’estimateur de Kaplan–Meier\nCette partie peut être omise; elle est incluse seulement par souci de complétude. Soit \\(T_1, \\ldots, T_n\\) les \\(n\\) réalisations aléatoires de la variable temps (certaines censurées, d’autres pas). Supposons qu’il y a \\(m\\) temps distincts où au moins un individu expérimente l’événement. Soient \\(t_{(1)} < \\cdots < t_{(m)}\\), ces temps ordonnés en ordre croissant et \\(r_i\\) le nombre d’individus à risque au temps \\(t_{(i)}\\) (les individus qui n’ont pas encore expérimenté l‘événement, et qui ne sont pas encore censurés avant \\(t_{(i)}\\)). On note \\(d_i\\) le nombre d’individus qui expérimentent l’événement au temps \\(t_{(i)}\\). L’estimateur de Kaplan–Meier de la fonction de survie à un temps \\(t\\) est [ (t) = ( 1- ) ( 1- ), t_{(1)} t t_{(m)}, ] où \\(i(t) =\\max(j \\in \\{1, \\ldots, m\\}: t \\geq t_{j})\\), soit le plus grand indice parmi \\(1, \\ldots, m\\) tel que \\(t \\geq t_{i(t)}\\). Par convention, si \\(t < t_{(1)}\\), on fixe \\(\\widehat{S}(t)=1\\)."
  },
  {
    "objectID": "06-survie.html#comparaison-de-deux-courbes-de-survie",
    "href": "06-survie.html#comparaison-de-deux-courbes-de-survie",
    "title": "7  Analyse de survie",
    "section": "7.4 Comparaison de deux courbes de survie",
    "text": "7.4 Comparaison de deux courbes de survie\nSupposons que les individus ont été divisés en deux groupes et que \\(S_1(t)\\) et \\(S_2(t)\\) dénotent respectivement la fonction de survie du premier groupe et du deuxième groupe. On est souvent intéressé à tester l’égalité des fonctions de survie, c’est-à-dire, les hypothèses \\(\\Hy_0: S_1(t) = S_2(t)\\) pour tout \\(t\\) et \\(\\Hy_1: S_1(t) \\neq S_2(t)\\) pour au moins une valeur de \\(t\\).\nPar exemple, dans une étude sur le temps de survie après avoir été diagnostiqué avec un certain type de cancer, on pourrait vouloir comparer le temps de survie des individus ayant reçu le traitement standard (groupe 1) au temps de survie des individus ayant reçu un nouveau traitement (groupe 2).\nLes deux tests utilisés habituellement sont le test du log-rang (log-rank test) et le test de Wilcoxon généralisé (ou test de Gehan).\nTestons l’hypothèse que la courbe de survie des clients masculins est la même que celle des clients féminins dans l’exemple des données d’abonnement. Ce test est effectué grâce à l’option strata de la procédure lifetest à l’aide des commandes suivantes (voir le fichier survival1_fonction_survie.sas):\n\n\nCode\nproc lifetest data=multi.survival1 method=km plots=(s) censoredsymbol=none;\ntime t*censure(1);\nstrata sexe;\nrun;\n\n\nOn retrouve dans la sortie les estimés de la fonction de survie, de même que les quartiles par strate: la première strate correspond aux hommes (sexe=0) et la deuxième aux femmes (sexe=1).\n\n\n\n\n\nFigure 7.5: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.6: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.7: ?(caption)\n\n\n\n\nOn voit qu’il y a 309 hommes et 191 femmes. L’estimation du temps de survie médian est de 110 semaines pour les hommes et de 123 semaines pour les femmes.\n\n\n\n\n\nFigure 7.8: ?(caption)\n\n\n\n\nUn des tableaux contient les statistiques et valeurs-\\(p\\) pour trois tests de l’hypothèse d’égalité des fonctions de survie, le test du log-rang et le test de Gehan. Le troisième test dans le tableau (-2Log(LR)) est un test du rapport de vraisemblance sous l’hypothèse que les temps de survie des deux groupes suivent une loi exponentielle. Il est préférable d’utiliser les deux premiers qui ne font pas d’hypothèses quant à la distribution du temps de survie. Les valeurs-\\(p\\) des tests log-rang et de Wilcoxon généralisé sont toutes les deux inférieures à \\(10^{-4}\\). On rejette donc \\(\\Hy_0\\) pour conclure qu’il y a donc une différence significative entre les deux courbes de survie.\nLe test de Wilcoxon généralisé accorde plus de poids au temps près du début de la distribution qu’au temps plus loin. Il est donc, en général, plus puissant lorsque la différence entre les deux fonctions de survie survient tôt dans la distribution. Le test log-rang quant à lui suppose que le ratio des fonctions de risques des deux groupes est constant pour toute la période d’intérêt.\nCes courbes sont représentées dans la Figure Figure 7.9. On voit que la courbe des femmes est systématiquement au-dessus de celle des hommes. Les femmes ont donc tendance à rester abonnées plus longtemps que les hommes, et cette différence est significative.\n\n\n\n\n\nFigure 7.9: Courbes de survie estimées par sexe (Kaplan–Meier)\n\n\n\n\nIl est également possible de tester l’égalité des courbes de survie avec plus de deux groupes. Par exemple, s’il y a trois groupes, l’hypothèse nulle est alors \\(\\Hy_0: S_1(t)=S_2(t)=S_3(t)\\) pour tout \\(t\\), versus l’alternative qu’au moins deux des fonctions ont une valeur différente pour au moins une valeur de \\(t\\). Les commandes SAS pour exécuter le test sont les mêmes: il suffit de mettre la variable identifiant les groupes à la ligne strata.\nL’estimateur de Kaplan–Meier ne permet pas l’inclusion de variables explicatives à proprement parler: si on peut veut les différences au niveau de la survie selon les modalités d’une variable explicative catégorielle, on divise pour ce faire l’échantillon en sous-groupes et on utilise l’estimateur de Kaplan–Meier pour chacune des modalités en gardant en tête que cela réduit la taille de l’échantillon disponible et que l’estimation résultante est possiblement trop variable pour être utile."
  },
  {
    "objectID": "06-survie.html#modèle-à-risques-proportionnels-de-cox",
    "href": "06-survie.html#modèle-à-risques-proportionnels-de-cox",
    "title": "7  Analyse de survie",
    "section": "7.5 Modèle à risques proportionnels de Cox",
    "text": "7.5 Modèle à risques proportionnels de Cox\nLe modèle à risques proportionnels de Cox (proportional hazard model) est l’un des modèles les plus utilisés pour analyser des données de survie.\n\n7.5.1 Description du modèle de Cox\nSoit \\(h(t; \\boldsymbol{x})\\) la valeur de la fonction de risque au temps pour un individu dont les valeurs des variables explicatives sont \\(X_1=x_1, \\ldots, X_p=x_p\\). Le modèle à risques proportionnels est \\[\\begin{align*}\nh(t; \\boldsymbol{x}) = h_0(t)\\exp(\\beta_1x_1 + \\cdots + \\beta_p x_p)\n\\end{align*}\\] où \\(h_0(t)\\) est la fonction de risque de base; il n’est pas nécessaire de spécifier cette dernière, d’où la nature semiparamétrique du modèle de Cox. Le postulat de risques proportionnels implique que le terme de droite \\(\\exp(\\mathbf{X}\\boldsymbol{\\beta})\\) ne dépend pas du temps, et plus particulièrement \\(\\beta_1, \\ldots, \\beta_p\\) ne dépend pas du temps. Nous verrons subséquemment une extension qui permet de prendre en compte les variables explicatives dont la valeur change dans le temps en scindant ces observations.\nLorsque toutes les variables explicatives prennent la valeur zéro, \\(\\boldsymbol{X}=\\boldsymbol{0}\\), on recouvre \\(h(t; \\boldsymbol{0})= h_0(t)\\). Par conséquent, la fonction \\(h_0(t)\\) peut être interprétée comme la fonction de risque lorsque toutes les variables explicatives valent zéro. Toutefois, tout comme la valeur de l’ordonnée à l’origine dans un modèle de régression linéaire, cette interprétation n’est pas nécessairement valide si la situation où toutes les variables explicatives valent zéro n’est pas possible ou si elle ne survient pas dans notre échantillon.\nLa deuxième partie du modèle, \\(\\exp(\\beta_1x_1 + \\cdots + \\beta_p x_p)\\), vient modéliser l’effet d’un changement des valeurs des variables explicatives sur la fonction de risque de base. Tout comme dans le cas de la régression logistique (l’effet des variables sur la cote), c’est un effet multiplicatif, d’où le terme risques proportionnels.\nPour l’interprétation des paramètres, il sera plus simple de penser en termes de rapport de risque (hazard ratio), qui est défini comme étant le rapport des fonctions de risque pour deux ensembles de valeurs des variables explicatives. Pour simplifier l’illustration, supposons que nous avons seulement une variable explicative \\(X\\) et que \\(h(t; x) = h_0(t)\\exp(\\beta x)\\). Le rapport de risque lorsque \\(X=x_1\\) par rapport à \\(X=x_0\\) est \\[\\begin{align*}\n\\frac{h(t; x_1)}{h(t; x_0)} = \\exp\\{\\beta(x_1-x_0)\\}.\n\\end{align*}\\] Par conséquent, l’impact d’une augmentation de \\(X\\) d’une unité (quand \\(x_1-x_0=1\\)) est \\(\\exp(\\beta)\\). Ainsi, pour chaque augmentation d’une unité pour \\(X\\), le risque que l’événement survienne est multiplié par \\(\\exp(\\beta)\\).\nLe terme risques proportionnels fait référence à la situation où le rapport de risque dépend seulement de la différence \\(x_1-x_0\\) et non pas du temps lui-même. Le rapport de risque est constant par rapport au temps \\(t\\). Cela implique que l’effet d’une variable est stable dans le temps. Nous verrons plus loin comment faire en sorte que l’effet d’une variable puisse varier dans le temps.\nDébutons avec un exemple simple en utilisant les données d’abonnement: on ajuste un modèle de Cox en utilisant seulement la variable binaire sexe (survival2_cox.sas). Ceci peut être fait avec la procédure phreg, comme suit:\n\n\nCode\nproc phreg data=multi.survival1;\nmodel t*censure(1)=sexe / ties=exact; \nrun;\n\n\nLa sortie inclut notamment des tests de significativité globale basés sur la vraisemblance pour les variables explicatives (rapport de vraisemblance, score et Wald) ainsi qu’un tableau des coefficients et des statistiques sur la qualité de l’ajustement pour le modèle sans variable explicative et celui qui inclut sexe.\n\n\n\n\n\nFigure 7.10: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.11: ?(caption)\n\n\n\n\nIl y a une seule variable explicative, le sexe de l’individu. L’estimation du paramètre de l’effet de sexe est \\(\\hat{\\beta}=-0,466\\). Ce paramètre est significativement différent de \\(0\\) (valeur-\\(p\\) inférieure à \\(10^{-4}\\)). Pour l’interprétation, on utilise la colonne « Rapport de risque » qui contient la valeur \\(\\exp(\\hat{\\beta}_{\\texttt{sexe}}) = \\exp(-0,466) = 0,628\\). Ainsi, le rapport du risque d’une femme par rapport à un homme est \\[\\begin{align*}\n\\frac{\\hat{h}(t; \\texttt{sexe}=1)}{\\hat{h}(t; \\texttt{sexe}=0)}= 0,628.\n\\end{align*}\\] Par conséquent, le risque qu’une femme interrompe son abonnement est 0,628 fois celui d’un homme. Une femme est donc moins à risque de quitter qu’un homme. Nous avions déjà vu cela à la section précédente lorsque nous avions comparé les courbes de survie des hommes et des femmes. Il est important de se rappeler qu’avec ce modèle, l’effet d’une variable est le même dans le temps (peu importe la valeur de \\(t\\)). Donc, une femme est moins à risque de quitter qu’un homme à tout moment, d’après ce modèle. Inversement, le ratio du risque d’un homme par rapport à une femme est \\(1/0,628=1,59\\). Ainsi, à tout moment, un homme a un risque d’interrompre son abonnement qui est 59% plus élevé que celui d’une femme.\nComme il y a un seul paramètre ici, les tests basés sur la vraisemblance pour \\(\\Hy_0: \\boldsymbol{\\beta}=\\boldsymbol{0}\\) reviennent à tester l’effet de la variable sexe. Le test de Wald est le même que celui du tableau des coefficients. Dans le cas particulier où il y a une seule variable explicative binaire (comme ici), le test du score est équivalent au test du log-rang que nous avons vu à la section précédente (à une petite différence près lorsqu’il y a des ex aequo dans les temps de survie).\nOn pourrait également utiliser une variable explicative continue plutôt qu’une variable binaire; le principe est le même.\n\n\n\n\n\nFigure 7.12: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.13: ?(caption)\n\n\n\n\nLe rapport de risque pour âge est 0,959 et donc le risque diminue de 4,1% chaque fois que l’âge augmente d’un an — le risque d’interrompre l’abonnement diminue lorsque l’âge augmente et cet effet est significatif (valeur-\\(p\\) des tests inférieures à \\(10^{-4}\\)).\nGénéralement, on considérera le modèle de Cox avec toutes les variables explicatives simultanément. La variable \\(\\texttt{region}\\) est nominale tandis que la variable \\(\\texttt{service}\\) est ordinale (avec quatre modalités). Nous allons les incorporer, comme d’habitude, en utilisant des variables indicatrices avec \\(\\texttt{region=5}\\) et \\(\\texttt{service=0}\\) (le client n’est abonné à aucun autre service) comme catégories de référence.\n\n\nCode\nproc phreg data=multi.survival1;\nclass region(ref='5') service(ref='0') / param=ref;\nmodel t*censure(1)=age sexe region service / ties=exact; \nrun;\n\n\n\n\n\n\n\nFigure 7.14: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.15: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.16: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.17: ?(caption)\n\n\n\n\nLes effets des variables sont maintenant des effets marginaux. Ainsi, lorsque les autres variables demeurent fixes, le risque de quitter d’une femme est \\(0,514\\) fois plus petit que celui d’un homme. L’effet marginal (une fois que les autres variables sont incluses) de la variable sexe est significatif (valeur-\\(p\\) inférieure à \\(10^{-4}\\)).\nToutes autres choses étant égales, chaque augmentation de l’âge d’un an fait diminuer le risque d’interrompre l’abonnement. Plus précisément, le risque est multiplié par 0,95 lorsque l’âge augmente d’un an et cet effet est significatif.\nPour la variable service, l’interprétation se fait par rapport à la catégorie de référence, qui est la catégorie \\(\\texttt{0}\\) (abonné à aucun autre service). Ainsi, si le client est abonné à un autre service, son risque de quitter est \\(0,358\\) fois celui d’un client qui n’est pas abonné à un autre service (toutes autres choses étant égales). Si le client est abonné à deux autres services, son risque de quitter est encore plus petit comparativement à un client qui n’est pas abonné à un autre service (rapport de risque de \\(0,177\\)) Finalement, si le client est abonné à trois autres services, son risque de quiter est encore plus petit (rapport de risque de \\(0,118\\)). Les paramètres de ces trois variables sont tous significatifs. Ainsi, les clients qui sont abonnés à un, deux ou trois services ont un risque de quitter qui est significativement plus faible que celui d’un client qui n’est pas abonné à un autre service. Le tableau Tests Type 3 permet de tester globalement la significativité d’une variable explicative modélisée avec plusieurs indicatrices. Pour la variable \\(\\texttt{service}\\), le test présenté teste l’hypothèse nulle \\(\\Hy_0: \\beta_{\\texttt{service}_1}=\\beta_{\\texttt{service}_2}=\\beta_{\\texttt{service}_3}=0\\) contre l’alternative qu’un moins un de ces trois paramètres est non-nul. Le test est largement significatif (statistique de Wald du \\(\\chi^2_4\\) valant 149,52 avec une valeur-\\(p\\) inférieure à \\(10^{-4}\\)). L’effet de la variable \\(\\texttt{service}\\) est donc globalement significatif. Afin de comparer les autres modalités entre elles, par exemple afin de voir si le risque de quitter est différent entre un client qui a deux services et un autre qui a trois services, il suffit de changer la catégorie de référence à la commande class et de réajuster le modèle.\nFinalement, la variable \\(\\texttt{region}\\) n’est pas globalement significative (statistique de Wald de 7,56 avec une valeur-\\(p\\) de 0,11).\n\n\n7.5.2 Estimation de la fonction de survie pour des valeurs particulières des variables explicatives\nIl est possible d’obtenir l’estimation de la fonction de survie pour des valeurs particulières des variables explicatives avec la commande baseline (voir le script survival2_cox.sas pour plus de détails). Pour ce faire, il faut avoir un autre fichier de données qui contient les valeurs des variables explicatives pour lesquelles on veut une estimation de la fonction de survie.\nSi on ajuste le modèle avec aucune variable explicative, on retrouvera alors l’estimation de Kaplan–Meier de la fonction de survie comme avec la procédure lifetest.\nSupposons qu’on ajuste le modèle avec les variables sexe et âge seulement dans l’exemple du temps d’abonnement, et que l’on désire la fonction de survie pour les hommes de 25 et 60 ans et pour les femmes de 25 et 60 ans. Le fichier survival2.sas7bdat contient les données qui seront utilisées à cette fin. Il contient seulement les quatre lignes suivantes.\n\n\n\n\n\nFigure 7.18: ?(caption)\n\n\n\n\nLes quatre fonctions de la Figure Figure 7.19 correspondent aux profils pour lesquels nous désirons une estimation de la courbe de survie. La courbe 1 est pour les hommes de 25 ans, la courbe 2 pour les femmes de 25 ans, la courbe 3 pour les hommes de 60 ans et la courbe 4 pour les femmes de 60 ans. On voit donc que, parmi ces quatre profils, les hommes de 25 ans sont le plus à risque de quitter tandis que les femmes de 60 ans sont le moins à risque de quitter.\n\n\n\n\n\nFigure 7.19: Courbes de survie pour hommes et femmes de 25 et 60 ans."
  },
  {
    "objectID": "06-survie.html#extensions-du-modèle-de-cox",
    "href": "06-survie.html#extensions-du-modèle-de-cox",
    "title": "7  Analyse de survie",
    "section": "7.6 Extensions du modèle de Cox",
    "text": "7.6 Extensions du modèle de Cox\nDans cette section, nous allons voir deux extensions du modèle à risques proportionnels de base.\n\nInclusion de variables explicatives dont la valeur change dans le temps.\nModèle à risques compétitifs (competing risks) pour étudier la situation où il y a plusieurs manières de quitter l’état.\n\n\n7.6.1 Variables explicatives dont la valeur change dans le temps\nIl est clair que certaines caractéristiques d’un individu évoluent dans le temps (time-varying covariates). Si le sexe d’un individu est stable dans le temps, son revenu, son statut matrimonial, l’endroit où il habite, sont par contre des caractéristiques qui peuvent changer dans le temps. Il peut alors être intéressant d’en tenir compte dans l’analyse. Rappelez-vous que le modèle à risques proportionnels est \\[\\begin{align*}\nh(t; \\boldsymbol{x}) = h_0(t) \\exp(\\beta_1x_1 + \\cdots + \\beta_px_p).\n\\end{align*}\\]\nSupposons que la variable \\(X_1\\) change au fil du temps et que les autres demeurent fixes. On peut alors réécrire le modèle \\[\\begin{align*}\nh(t; \\boldsymbol{x}) = h_0(t) \\exp\\{\\beta_1x_1(t) + \\cdots + \\beta_px_p\\},\n\\end{align*}\\] où \\(x_1(t)\\) indique que la valeur de \\(X_1\\) dépend du temps \\(t\\).\nSupposons que la variable \\(\\texttt{service}\\), qui représente le nombre d’autres services souscrits, est la seule que nous voulons modéliser comme une variable qui varie dans le temps. Pour l’âge, nous prenons simplement l’âge au début de l’abonnement, idem pour la région.\nLe plus difficile est de créer correctement le fichier de données pour effectuer ce genre d’analyse. Supposons pour cet exemple qu’il y a eu au plus un changement dans la variable \\(\\texttt{service}\\), comme présenté dans le fichier survival3.sas7bdat. Les variables \\(\\texttt{t}\\), \\(\\texttt{censure}\\), \\(\\texttt{age}\\), \\(\\texttt{sexe}\\) et \\(\\texttt{region}\\) sont comme précédemment. Trois nouvelles variables remplacent l’ancienne variable service.\n\n\\(\\verb+service_avant+\\): nombre d’autres services auxquels le client est abonné au début de son abonnement.\n\\(\\verb+temps_ch+\\): temps au moment où un changement est survenu quant au nombre d’autres services. En l’absence de changement, l’observation est remplacée par une valeur manquante (.).\n\\(\\verb+service_apres+\\): nombre d’autres services auxquels le client est abonné à partir du temps \\(\\verb+temps_ch+\\).\n\n\n\n\n\n\nFigure 7.20: ?(caption)\n\n\n\n\nOn regarde plus en détail le profil des cinq premiers clients, dont seuls deux ont changé le nombre d’abonnements; les individus 3–5 se sont désabonnés du service cellulaire à un moment donné. Le premier client était abonné à deux autres services au début de son abonnement au téléphone cellulaire mais, après 130 semaines d’abonnement, a effectué un changement à son forfait pour ne conserver qu’un autre service en plus du cellulaire. Pour le deuxième client, comme \\(\\verb+temps_ch+\\) est manquante, il est toujours abonné à deux autres services et ce, jusqu’à la fin de l’étude.\nLes commandes SAS permettant d’ajuster le modèle avec point de rupture se trouvent dans le fichier survival3_varie_temps.sas. Notez que lorsqu’on a une variable catégorielle qui varie dans le temps (comme ici avec la variable service), on ne peut pas utiliser class pour la déclarer catégorielle; il faut plutôt créer nous-mêmes les variables indicatrices nécessaires à l’intérieur même de l’appel à la fonction phreg (voir le script). Ici, nous utiliserons la catégorie \\(\\texttt{0}\\) (aucun autre service) comme catégorie de référence.\n\n\n\n\n\nFigure 7.21: ?(caption)\n\n\n\n\nL’interprétation se fait comme précédemment. Ici, c’est la valeur d’une variable qui varie dans le temps et non pas son effet. Ainsi, le ratio de risque de quitter pour un client qui a un autre service est 0,604 fois celui d’un client qui n’a aucun autre service (référence). Le fait d’avoir deux ou trois services diminue encore plus le risque de quitter (ratios de risque de 0,267 et 0,21, respectivement).\n\n\n7.6.2 Modèle à risques compétitifs\nParfois, la raison pour laquelle un individu quitte l’état étudié peut avoir un intérêt en soi. Par exemple si on s’intéresse au temps qu’un employé demeure au service de la compagnie, la distinction entre le fait qu’il ait démissionné ou bien qu’il ait été renvoyé peut avoir un impact sur l’effet des variables explicatives. Comme autre exemple, si on s’intéresse au temps de survie d’un individu après qu’il ait été diagnostiqué avec un certain type de cancer, il pourrait être important de distinguer selon la cause exacte de la mort.\nDe manière générale, supposons qu’il y a \\(K\\) manières possibles que l’événement survienne. On peut alors spécifier \\(K\\) fonctions de risques (une pour chaque manière) et obtenir le modèle de Cox à risques compétitifs (competing risks), \\[\\begin{align*}\nh_1(t; \\boldsymbol{x})&= h_{01}(t) \\exp(\\beta_{11}x_1 + \\cdots + \\beta_{p1} x_p)\\\\\n&\\vdots\\\\\nh_K(t; \\boldsymbol{x})&= h_{0K}(t) \\exp(\\beta_{1K}x_1 + \\cdots + \\beta_{pK} x_p)\\\\\n\\end{align*}\\] Notez que les coefficients \\(K\\) sont différents d’une équation à l’autre. En estimant ce modèle, on obtient donc une estimation de l’effet des variables selon la raison du départ de l’état. De plus, on peut aussi inclure des variables dont la valeur change dans le temps, comme vu précédemment. Ce qui simplifie énormément la situation est qu’il est prouvé qu’on peut estimer les paramètres de chaque équation séparément sans perte de précision. Par conséquent, en pratique, il suffira d’ajuster \\(K\\) modèles séparément.\nDans notre exemple d’abonnement cellulaire, supposons que nous avons trois causes possibles pour la perte d’un client: soit il a interrompu son abonnement pour aller chez le compétiteur A, soit pour aller chez le compétiteur B, soit il n’a plus de cellulaire du tout.\nLes données pour cet exemple se trouvent dans le fichier survival4.sas7bdat et le programme dans le fichier survival4_risques_competitifs.sas. La seule nouveauté par rapport au fichier original est la variable \\(\\texttt{censure}\\) qui est maintenant codée ainsi\n\n1, si le temps est censuré (l’individu est toujours abonné à notre service)\n2, si l’individu a quitté pour aller chez le compétiteur A\n3, si l’individu a quitté pour aller chez le compétiteur B\n4, si l’individu a quitté parce qu’il n’a plus besoin de cellulaire.\n\nOn peut calculer la fréquence de chaque modalité.\n\n\n\n\n\nFigure 7.22: ?(caption)\n\n\n\n\nAinsi, il y a donc 166 clients toujours abonnés, 170 qui nous ont quitté pour aller chez A, 121 pour aller chez B, et 43 qui n’ont plus de cellulaires.\nPour ajuster le modèle lorsque la cause du départ est le compétiteur A, le code est ::: {.cell}\n\nCode\nproc phreg data=multi.survival4;\nclass region(ref='5') service(ref='0') / param=ref;\nmodel t*censure(1,3,4)=age sexe region service / ties=exact; \nrun;\n\n:::\nNotez qu’on précise que les valeurs 1, 3 et 4 sont des observations censurées. Ici, l’événement d’intérêt est que le client est parti chez le compétiteur A. S’il est toujours abonné (\\(\\texttt{censure=1}\\)), s’il est parti chez le compétiteur B (\\(\\texttt{censure=3}\\)) ou s’il nous a quitté car il n’a plus de cellulaire (\\(\\texttt{censure=4}\\)), alors l’événement « quitter pour aller chez A » n’est pas survenu. C’est pourquoi on doit traiter ces situations comme des censures.\n\n\n\n\n\nFigure 7.23: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.24: ?(caption)\n\n\n\n\nAinsi, on voit que l’événement est survenu 170 fois et qu’il y a 330 censures. L’interprétation des paramètres se fait comme précédemment. Sauf qu’il faut préciser qu’il s’agit du risque de quitter pour aller chez le compétiteur A. Par exemple, le risque de quitter pour aller chez le compétiteur A d’une femme est 0,444 fois le risque de quitter pour aller chez le compétiteur A d’un homme. Ainsi, les femmes sont moins à risque de quitter pour aller chez le compétiteur A que les hommes.\nPour ajuster le modèle lorsque la cause du départ est le compétiteur B, procède de la même manière. Notez que cette fois-ci, ce sont les valeurs 1, 2 et 4 de la variable censure qui correspondent au fait que l’événement n’est pas survenu; on spécifie donc censure(1,2,4) dans l’appel à la fonction phreg. Il y a 121 clients qui ont quitté pour aller chez B et 379 cas autre (censure). L’interprétation des paramètres se fait en termes de risque de quitter pour aller chez le compétiteur B.\n\n\n\n\n\nFigure 7.25: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.26: ?(caption)\n\n\n\n\nSi on ajuste le modèle pour le cas de figure où la cause de départ est que le client n’a plus besoin de cellulaire, on obtient une sortie similaire. On voit, que contrairement aux deux premiers modèles, l’effet de la variable sexe n’est pas significatif ici.\n\n\n\n\n\nFigure 7.27: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.28: ?(caption)\n\n\n\n\nEn fait, comme il y a seulement 43 événements (quitter car on n’a plus besoin de cellulaire), les estimations des paramètres sont moins précises, ce qu’on peut voir avec les erreurs-types qui sont plus élevés. Les observations censurées contiennent moins d’information que les événements, d’où cette perte de précision."
  },
  {
    "objectID": "06-survie.html#risques-non-proportionnels",
    "href": "06-survie.html#risques-non-proportionnels",
    "title": "7  Analyse de survie",
    "section": "7.7 Risques non proportionnels",
    "text": "7.7 Risques non proportionnels\nComme son nom l’indique, le modèle à risques proportionnels suppose que les risques sont proportionnels. Cela implique que l’effet d’une variable est stable dans le temps. Nous verrons dans cette section deux façons de modéliser le cas de risques non proportionnels.\n\n7.7.1 Non-proportionnalité avec un terme d’interaction avec le temps\nPour simplifier l’exposition, supposons que nous avons une seule variable explicative \\(X\\). L’équation du modèle à risques proportionnels est \\(h(t; x) = h_0(t)\\exp(\\beta x)\\) et suppose que la fonction de risque de base \\(h_0(t)\\) est indépendante de la variable explicative \\(X\\). Une manière de modéliser la non-proportionnalité est d’inclure un terme d’interaction entre la variable et le temps. Il existe plusieurs façons de le faire, l’une d’entre elles consiste à inclure une nouvelle variable qui est le produit entre le temps et la variable \\(X\\). Le modèle est alors \\[\\begin{align*}\nh(t; x) = h_0(t) \\exp(\\beta_1x + \\beta_2xt).\n\\end{align*}\\] Pour ce modèle, le rapport de risque, pour une augmentation d’une unité de \\(X\\) est \\(\\exp(\\beta_1+ \\beta_2t)\\) et dépend du temps \\(t\\): c’est un modèle avec risques non proportionnels. On retombe sur le modèle à risques proportionnels lorsque \\(\\beta_2=0\\).\nAjustons le modèle pour l’abonnement cellulaire en ajoutant une interaction entre l’âge et le temps. Les commandes se trouvent dans survival5_non_proportionnel.sas, dont l’extrait montre comment créer la variable produit.\n\n\nCode\nproc phreg data=temp;\nclass region(ref='5') service(ref='0') / param=ref;\nmodel t*censure(1)=age iaget sexe region service / ties=exact;\niaget=age*t; \nrun;\n\n\nL’interaction entre l’âge et le temps est spécifiée en incluant une nouvelle variable, \\(\\texttt{iaget}\\), créée à l’intérieur de l’appel à la procédure phreg, et qui est égale au produit entre l’\\(\\texttt{age}\\) et \\(\\texttt{t}\\). On remarque que le terme d’interaction est tout juste non significatif (valeur-\\(p\\) de 0,061).\n\n\n\n\n\nFigure 7.29: ?(caption)\n\n\n\n\n\n\n7.7.2 Stratification\nUne autre manière de modéliser la non-proportionnalité est par la stratification. Il est important de comprendre qu’on ne pourra pas estimer l’effet de la variable de stratification. On devrait donc seulement utiliser une variable qui ne nous intéresse pas en soi, mais qui peut avoir un effet sur le temps de survie.\nEncore une fois, pour simplifier, supposons que nous avons deux variables \\(X\\) et \\(Z\\); cette dernière est binaire et prend les valeurs \\(0\\) et \\(1\\). On s’intéresse à l’effet de la variable \\(X\\) mais pas à celui de la variable \\(Z\\); néanmoins, on croit que \\(Z\\) impacte le temps de survie et que, de ce fait, le postulat de proportionnalité de la fonction de risque n’est pas validé. Le modèle de Cox avec stratification (pour la variable \\(Z\\)) est \\[\\begin{align*}\nh(t; x, z) = h_z(t) \\exp(\\beta x),\n\\end{align*}\\] où \\(h_0(t)\\) est la fonction de risque de base quand \\(Z=0\\) et \\(h_1(t)\\) est la fonction de risque de base lorsque \\(Z=1\\). L’effet de la variable \\(X\\) est supposé être le même peut importe la valeur de \\(Z\\), mais la fonction de risque de base peut différer. Le rapport de risque pour \\(Z=1\\) versus \\(Z=0\\) est \\(h_1(t)/h_0(t)\\); cette quantité dépend du temps \\(t\\). L’effet de la variable est donc variable dans le temps (et non pas constant). Ce modèle permet donc de modéliser la non-proportionnalité pour la variable \\(Z\\). Si on stratifie par rapport à une variable, il ne faut pas l’inclure dans le modèle en plus car elle est déjà modélisée via la stratification. Notez que les paramètres \\(\\boldsymbol{\\beta}\\) seront estimés à l’aide des données de toutes les strates.\nL’avantage de la stratification est que cette méthode permet de modéliser n’importe quel changement dans l’effet d’une variable dans le temps sans devoir spécifier un type de changement particulier, comme lorsqu’on doit choisir la forme de l’interaction. Par contre, on perd la possibilité de tester l’effet de la variable de stratification on réduit la taille de l’échantillon pour l’estimation de la fonction de risque de base. On devrait donc utiliser la stratification seulement avec des variables pour lesquelles nous n’avons pas besoin d’estimer l’effet (variables secondaires ou de contrôles).\nOn considère la stratification par rapport à la région pour notre modèle pour le temps d’abonnement à un forfait cellulaire; un test permet de voir que l’hypothèse de non-proportionnalité des fonctions de risque n’est pas valide pour région. Le modèle contient toutes les variables explicative, hormis région qui est utilisée pour la stratification.\nLes commandes pour ajuster le modèle sont ::: {.cell}\n\nCode\nproc phreg data=multi.survival1;\nclass service(ref='0') / param=ref;\nmodel t*censure(1)=age sexe service / ties=exact;\nstrata region;\nrun;\n\n:::\n\n\n\n\n\nFigure 7.30: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 7.31: ?(caption)\n\n\n\n\nOn voit à la lecture de la sortie SAS qu’il n’y a pas de paramètres pour la variable région. Les paramètres des autres variables s’interprètent comme d’habitude.\nLe commentaire suivant est technique et peut être omis. Le postulat de risques proportionnels est rarement validé en pratique (et difficile à tester). Cela aura pour conséquences que les erreurs-types des variables explicatives autres que binaires sont faussées, problème qu’on peut régler en utilisant des procédures d’autoamorçage. La valeur du rapport de risque dépend de la distribution des pertes de suivi, même quand ces dernières surviennent de manière aléatoire; une recommendation récente est d’utiliser des rapports de risques pondérées par la probabilité inverse d’appartenance, le tout complémenté par des mesures d’effets comme la différence de survie, la moyenne de survie (estimation restreinte) à des temps préspécifiés."
  },
  {
    "objectID": "07-donneesmanquantes.html",
    "href": "07-donneesmanquantes.html",
    "title": "8  Données manquantes",
    "section": "",
    "text": "Il arrive fréquemment d’avoir des valeurs manquantes dans notre échantillon. Ces valeurs peuvent être manquantes pour diverses raisons. Si on prélève nous-mêmes nos données, un répondant peut refuser de répondre à certaines questions. Si on acquiert nos données d’une source externe, les valeurs de certaines variables peuvent être manquantes directement dans le fichier obtenu. Si on ne prend pas en compte le méchanisme générant les valeurs manquantes, ces dernières peuvent également biaiser nos analyses. Le but de ce chapitre est de faire un bref survol de ce sujet."
  },
  {
    "objectID": "07-donneesmanquantes.html#principes-de-base",
    "href": "07-donneesmanquantes.html#principes-de-base",
    "title": "8  Données manquantes",
    "section": "8.1 Principes de base",
    "text": "8.1 Principes de base\nSoit \\(X\\) une variable pour laquelle des données sont manquantes. Voici la définition de trois processus de génération de données manquantes.\n\nLes données manquantes de \\(X\\) sont dites manquantes de façon complètement aléatoire (MCAR, de l’anglais missing completely at random) si la probabilité que la valeur de \\(X\\) soit manquante ne dépend ni de la valeur de \\(X\\) (qui n’est pas observée), ni des valeurs des autres variables.\n\nLe fait qu’une variable est manquante peut être relié au fait qu’une autre soit manquante. Des gens peuvent refuser systématiquement de répondre à deux questions dans un sondage. Dans ce cas, si la probabilité qu’une personne ne réponde pas ne dépend pas des valeurs de ces variables (et de toutes les autres), nous sommes encore dans le cas MCAR. Si par contre, la probabilité que les gens ne répondent pas à une question sur leur revenu augmente avec la valeur de ce revenu, alors nous ne sommes plus dans le cas MCAR.\nLe cas MCAR peut se présenter par exemple si des questionnaires, ou des pages ont été égarés ou détruits par inadvertance (effacées du disque rigide, etc.) Si les questionnaires manquants constituent un sous-ensemble choisi au hasard de tous les questionnaires, alors le processus est MCAR. L’hypothèse que les données manquantes sont complètement aléatoires est en général considérée comme trop restrictive.\n\nLes données manquantes de \\(X\\) sont dites données manquantes de façon aléatoire (MAR, de l’anglais missing at random) si la probabilité que la valeur de \\(X\\) soit manquante ne dépend pas de la valeur de \\(X\\) (qui n’est pas observée) une fois qu’on a contrôlé pour les autres variables.\n\nIl est possible par exemple que les femmes refusent plus souvent que les hommes de répondre à une question (et donc, le processus n’est pas MCAR). Si pour les femmes, la probabilité que \\(X\\) est manquante ne dépend pas de la valeur de \\(X\\) et que pour les hommes, la probabilité que \\(X\\) est manquante ne dépend pas de la valeur de \\(X\\), alors le processus est MAR. Les probabilités d’avoir une valeur manquante sont différentes pour les hommes et les femmes mais cette probabilité ne dépend pas de la valeur de \\(X\\) elle-même. L’hypothèse MAR est donc plus faible que l’hypothèse MCAR.\n\nLes données manquantes de \\(X\\) sont dites manquantes de façon non-aléatoire (MNAR, de l’anglais missing not at random) si la probabilité que la valeur de \\(X\\) soit manquante dépend de la valeur de \\(X\\) elle-même.\n\nPar exemple, les gens qui ont un revenu élevé pourraient avoir plus de réticences à répondre à une question sur leur revenu. La méthode de traitement que nous allons voir dans ce chapitre, l’imputation multiple, est très générale et est valide dans le cas MAR (et donc aussi dans le cas MCAR). Le cas MNAR est beaucoup plus difficile à traiter et ne sera pas considéré ici."
  },
  {
    "objectID": "07-donneesmanquantes.html#méthodes-dimputation",
    "href": "07-donneesmanquantes.html#méthodes-dimputation",
    "title": "8  Données manquantes",
    "section": "8.2 Méthodes d’imputation",
    "text": "8.2 Méthodes d’imputation\n\n8.2.1 Cas complets\nLa première idée naïve pour une analyse est de retirer les observations avec données manquantes pour conserver les cas complets (listwise deletion, ou complete case analysis).\nCette méthode consiste à garder seulement les observations qui n’ont aucune valeur manquante pour les variables utilisées dans l’analyse demandée. Dès qu’une variable est manquante, on enlève le sujet au complet. C’est la méthode utilisée par défaut dans la plupart des logiciels, dont SAS.\n\nSi le processus est MCAR, cette méthode est valide car l’échantillon utilisé est vraiment un sous-échantillon aléatoire de l’échantillon original. Par contre, ce n’est pas nécessairement la meilleure solution car on perd de la précision en utilisant moins d’observations.\nSi le processus est seulement MAR ou MNAR, cette méthode produit généralement des estimations biaisées des paramètres.\n\nEn général, l’approche des cas complet est la première étape d’une analyse afin d’obtenir des estimés initiaux que nous corrigerons pas d’autre méthode. Elle n’est vraiment utile que si la proportion d’observations manquantes est très faible et le processus est MCAR. Évidemment, la présence de valeurs manquantes mène à une diminution de la précision des estimateurs (caractérisée par une augmentation des erreurs-types) et à une plus faible puissance pour les tests d’hypothèse et donc ignorer l’information partielle (si seulement certaines valeurs des variables explicatives sont manquantes) est sous-optimal.\n\n\n8.2.2 Imputation simple\nLa deuxième approche est l’imputation simple. L’idée ici est de ne pas enlever les observations avec des valeurs manquantes mais de remplacer ces valeurs par des valeurs raisonnables. Par exemple, on peut remplacer les valeurs manquantes d’une variable par la moyenne de cette variable dans notre échantillon. On peut aussi ajuster un modèle de régression avec cette variable comme variable dépendante et d’autres variables explicatives comme variables indépendantes et utiliser les valeurs prédites comme remplacement. Une fois que les valeurs manquantes ont été remplacées, on fait l’analyse avec toutes les observations.\nIl existe d’autres façons d’imputer les valeurs manquantes mais le problème de toutes ces approches est que l’on ne tient pas compte du fait que des valeurs ont été remplacées et on fait comme si c’était de vraies observations. Cela va en général sous-évaluer la variabilité dans les données. Par conséquent, les écarts-type des paramètres estimés seront en général sous-estimés et l’inférence (tests et intervalles de confiance) ne sera pas valide. Cette approche n’est donc pas recommandée.\nUne manière de tenter de reproduire correctement la variabilité dans les données consiste à ajouter un terme aléatoire dans l’imputation. C’est ce que fait la méthode suivante, qui possédera l’avantage de corriger automatiquement les écarts-type des paramètres estimés.\n\n\n8.2.3 Imputation multiple\nCette méthode peut être appliquée dans à peu près n’importe quelle situation et permet d’ajuster les écarts-type des paramètres estimés. Elle peut être appliquée lorsque le processus est MAR (et donc aussi MCAR).\nL’idée consiste à procéder à une imputation aléatoire, selon une certaine technique, pour obtenir un échantillon complet et à ajuster le modèle d’intérêt avec cet échantillon. On répète ce processus plusieurs fois et on combine les résultats obtenus.\nL’estimation finale des paramètres du modèle est alors simplement la moyenne des estimations pour les différentes répétitions et on peut également obtenir une estimation des écarts-type des paramètres qui tient compte du processus d’imputation.\nPlus précisément, supposons qu’on s’intéresse à un seul paramètre \\(\\theta\\) dans un modèle donné. Ce modèle pourrait être un modèle de régression linéaire, de régression logistique, etc. Le paramètre \\(\\theta\\) serait alors un des \\(\\boldsymbol{\\beta}\\) du modèle.\nSupposons qu’on procède à \\(K\\) imputations, c’est-à-dire, qu’on construit \\(K\\) ensemble de données complets à partir de l’ensemble de données initial contenant des valeurs manquantes. On estime alors les paramètres du modèle séparément pour chacun des ensembles de données imputés. Soit \\(\\widehat{\\theta}_k\\), l’estimé du paramètre \\(\\theta\\) pour l’échantillon \\(k \\in \\{1, \\ldots, K\\}\\) et \\(\\widehat{\\sigma}_k^2=\\mathsf{Va}(\\widehat{\\theta}_k)\\) l’estimé de la variance de \\(\\widehat{\\theta}_k\\) produite par le modèle estimé.\nL’estimation finale de \\(\\theta\\), dénotée \\(\\widehat{\\theta}\\), est obtenue tout simplement en faisant la moyenne des estimations de tous les modèles, c’est-à-dire, \\[\\begin{align*}\n\\widehat{\\theta} = \\frac{\\widehat{\\theta}_1 + \\cdots + \\widehat{\\theta}_K}{K}.\n\\end{align*}\\] Une estimation ajustée de la variance de \\(\\widehat{\\theta}\\) est \\[\\begin{align*}\n\\mathsf{Va}(\\hat{\\theta}) &= W+ \\frac{K+1}{K}B,\n\\\\ W &= \\frac{1}{K} \\sum_{k=1}^K \\widehat{\\sigma}^2_k = \\frac{\\widehat{\\sigma}_1^2 + \\cdots + \\widehat{\\sigma}_K^2}{K},\\\\\nB &= \\frac{1}{K-1} \\sum_{k=1}^K (\\widehat{\\theta}_k - \\widehat{\\theta})^2.\n\\end{align*}\\] Ainsi, le terme \\(W\\) est la moyenne des variances et \\(B\\) est la variance entre les imputations. Le terme \\((1+1/K)B\\) est celui qui vient corriger le fait qu’on travaille avec des données imputées et non pas des vraies données en augmentant la variance estimée du paramètre.\nC’est ici qu’on voit l’intérêt à procéder à de l’imputation multiple. Si on procédait à une seule imputation (même en ajoutant une part d’aléatoire pour essayer de reproduire la variabilité des données), on ne serait pas en mesure d’estimer la variance inter-groupe de l’estimateur. Notez que la formule présentée n’est valide que pour le cas unidimensionnel; l’estimation de la variance dans le cas multidimensionnel est différente (voir Little et Rubin, 2002).\nLa méthode d’imputation multiple possède l’avantage d’être applicable avec n’importe quel modèle sous-jacent. Une fois qu’on a des échantillons complets (imputés), on ajuste le modèle comme d’habitude. Mais une observation imputée ne remplacera jamais une vraie observation. Il faut donc faire tout ce qu’on peut pour limiter le plus possible les données manquantes.\nIl faut utiliser son jugement. Par exemple, si la proportion d’observations perdues est petite (moins de 5%), ça ne vaut peut-être pas la peine de prendre des mesures particulières et on peut faire une analyse avec les données complètes seulement. S’il y a un doute, on peut faire une analyse avec les données complètes seulement et une autre avec imputations multiples afin de valider la première.\nSi, à l’inverse, une variable secondaire cause à elle seule une grande proportion de valeurs manquantes, on peut alors considérer l’éliminer afin de récupérer des observations. Par exemple, si vous avez une proportion de 30% de valeurs manquantes en utilisant toutes vos variables et que cette proportion baisse à 3% lorsque vous éliminez quelques variables peu importantes pour votre étude (ou qui peuvent être remplacées par d’autres jouant à peu près le même rôle qui elles sont disponibles), alors vous pourriez considérer la possibilité de les éliminer. Il est donc nécessaire d’examiner la configuration des valeurs manquantes avant de faire quoi que ce soit."
  },
  {
    "objectID": "07-donneesmanquantes.html#example-dapplication-de-limputation",
    "href": "07-donneesmanquantes.html#example-dapplication-de-limputation",
    "title": "8  Données manquantes",
    "section": "8.3 Example d’application de l’imputation",
    "text": "8.3 Example d’application de l’imputation\nOn examine l’exemple de recommandations de l’association professionnelle des vachers de la section @cowboy.\nLe but est d’examiner les effets des variables \\(X_1\\) à \\(X_6\\) sur les intentions d’achat; la base de données manquantes contient les observations. Il s’agit des mêmes données que celles du fichier logit1 mais avec des valeurs manquantes.\n\n\n\nTableau 8.1:  Tableau des configurations des données manquantes. \n \n  \n    \\(X_1\\) \n    \\(X_2\\) \n    \\(X_3\\) \n    \\(X_4\\) \n    \\(X_5\\) \n    \\(X_6\\) \n    \\(y\\) \n  \n \n\n  \n    1 \n    4 \n    0 \n    1 \n    35 \n    2 \n    0 \n  \n  \n    . \n    1 \n    0 \n    . \n    33 \n    3 \n    0 \n  \n  \n    2 \n    3 \n    1 \n    . \n    46 \n    3 \n    0 \n  \n  \n    5 \n    2 \n    1 \n    . \n    32 \n    1 \n    1 \n  \n  \n    3 \n    2 \n    1 \n    . \n    38 \n    3 \n    1 \n  \n  \n    . \n    4 \n    0 \n    0 \n    36 \n    3 \n    0 \n  \n  \n    . \n    3 \n    0 \n    . \n    35 \n    3 \n    0 \n  \n  \n    . \n    5 \n    1 \n    0 \n    26 \n    2 \n    0 \n  \n  \n    . \n    3 \n    1 \n    1 \n    39 \n    2 \n    1 \n  \n  \n    5 \n    2 \n    1 \n    . \n    38 \n    3 \n    0 \n  \n\n\n\n\n\n\nLes points (.) indiquent des valeurs manquantes. Le premier sujet n’a pas de valeur manquante. Le deuxième a une valeur manquante pour \\(X_1\\) (emploi) et \\(X_4\\) (éducation), etc.\nUne première façon de voir combien il y a de valeurs manquantes consiste à faire sortir les statistiques descriptives avec summary. Ainsi, il y 192 valeurs manquantes pour \\(X_1\\), 48 pour \\(X_2\\) et 184 pour \\(X_4\\). Les autres variables n’ont pas de valeurs manquantes, incluant la variable dépendante \\(Y\\). La procédure unidimensionnelle nous permet seulement de voir combien il y a de valeurs manquantes variable par variable.\n\n\n\nTableau 8.2:  Pourcentage de valeurs manquantes par variable. \n \n  \n      \n    x1 \n    x2 \n    x3 \n    x4 \n    x5 \n    x6 \n    y \n  \n \n\n  \n    nombre \n    192 \n    49 \n    0 \n    184 \n    0 \n    0 \n    0 \n  \n  \n    pourcentage \n    0.384 \n    0.098 \n    0 \n    0.368 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\n\n\nCode\ndata(manquantes, package = 'hecmulti')\nsummary(manquantes)\n# Pourcentage de valeurs manquantes\napply(manquantes, 2, function(x){mean(is.na(x))})\n# Voir les configurations de valeurs manquantes\nmd.pattern(missing1)\n\n\n\n\n\n\n\nFigure 8.1: Configurations des valeurs manquantes pour la base de données manquantes.\n\n\n\n\nLa Figure 8.1 donne une indication sur les différentes combinaisons de données complètes (cases bleues) et les observations manquantes (cases roses) avec leur fréquence. Les variables sont indiquées au dessus, les effectifs manquants en dessous, le nombre de cas de chaque combinaisons à gauche et le nombre de variables avec des valeurs manquantes à droite. Ainsi, il y a 180 sujets (36% de l’échantillon) avec aucune observation manquante. Il y en a 99 avec seulement \\(X_4\\) manquante et ainsi de suite. On voit donc, par exemple, que pour 14 sujets, à la fois \\(X_1\\) et \\(X_2\\) sont manquantes.\nLa recommandation d’usage est d’imputer au moins le pourcentage de cas incomplet, ici 64% donc 64 imputations. Si la procédure est trop coûteuse en calcul, on peut diminuer le nombre d’imputations, mais il faut au minimum 10 réplications pour avoir une bonne idée de la variabilité.\nOn peut comparer l’inférence avec toutes les variables explicatives pour les données sans valeurs manquantes (\\(n=500\\) observations), avec les cas complets uniquement (\\(n=180\\) observations). Le Tableau 8.3 présente les estimations des paramètres du modèle de régression logistique s’il n’y avait pas eu de valeurs manquantes, avec les cas complets et les résultats de l’imputation multiple.\nSi on ajuste un modèle à une base de données qui contient des valeurs manquantes, le comportement par défaut est de retirer les observations qui ont au moins une valeur manquante pour une des variables nécessaires à l’analyse (voir la sortie de glm(y ~ ., data = manquantes)). Il ne serait pas raisonnable de faire l’analyse avec seulement 180 observations et de laisser tomber les 320 autres. De plus, comme nous l’avons vu plus haut, ce n’est pas valide à moins que le processus ne soit MCAR. La partie du milieu du Tableau 8.3 présente les estimations obtenues. Plusieurs variables significatives à niveau \\(\\alpha=0.05\\) ne le sont plus (puisqu’il y a moins d’information quand on réduit le nombre d’observations). Il y a même pire: non seulement la variable \\(\\mathsf{I}(X_2=1)\\) est passée de significative à non significative, mais en plus l’estimé de son paramètre a changé de signe.\nNous allons donc faire l’analyse avec l’imputation multiple, en prenant la méthode d’imputation par défaut\n\n\nCode\nlibrary(mice)\n# Imputation multiple avec équations enchaînées\n# Intensif en calcul, réduire `m` si nécessaire\nimpdata <- mice(data = manquantes,\n                m = 50,\n                seed = 2021,\n                method = \"pmm\",\n                printFlag = FALSE)\n# Chaque copie est disponible (1, ..., 50)\ncomplete(impdata, action = 1)\n# ajuste les modèles avec les données imputées\n\nadj_im <- with(data = impdata,\n               expr = glm(y ~ x1 + x2 + x3 + x4 + x5 + x6,\n                          family = binomial(link = 'logit')))\n\n# combinaison des résultats \nfit <- pool(adj_im)\nsummary(fit)\n\n\nLa procédure mice du paquet éponyme crée les copies complètes du jeu de données. On peut ensuite appliquer une procédure quelconque et combiner les estimations avec pool.\n\n\n\nTableau 8.3:  Estimés, erreurs-type et valeurs-p des paramètres avec les 500 données complètes (gauche), avec les 180 cas complets (milieu) et avec l’imputation multiple (droite). \n \n\n\nDonnées complètes\nCas complets\nImputation multiple\n\n  \n      \n    \\(\\widehat{\\boldsymbol{\\beta}}\\) \n    \\(\\mathrm{se}(\\widehat{\\boldsymbol{\\beta}})\\) \n    valeur-\\(p\\) \n    \\(\\widehat{\\boldsymbol{\\beta}}\\) \n    \\(\\mathrm{se}(\\widehat{\\boldsymbol{\\beta}})\\) \n    valeur-\\(p\\) \n    \\(\\widehat{\\boldsymbol{\\beta}}\\) \n    \\(\\mathrm{se}(\\widehat{\\boldsymbol{\\beta}})\\) \n    valeur-\\(p\\) \n  \n \n\n  \n    cste \n    -6.89 \n    1.02 \n    0.00 \n    -5.25 \n    1.70 \n    0.00 \n    -6.57 \n    1.04 \n    0.00 \n  \n  \n    \\(x_1=1\\) \n    0.36 \n    0.48 \n    0.46 \n    -0.09 \n    0.85 \n    0.92 \n    0.55 \n    0.54 \n    0.31 \n  \n  \n    \\(x_1=2\\) \n    -0.47 \n    0.37 \n    0.21 \n    -0.57 \n    0.66 \n    0.39 \n    -0.13 \n    0.45 \n    0.78 \n  \n  \n    \\(x_1=3\\) \n    -0.31 \n    0.35 \n    0.37 \n    -0.47 \n    0.66 \n    0.47 \n    0.07 \n    0.44 \n    0.87 \n  \n  \n    \\(x_1=4\\) \n    -0.32 \n    0.40 \n    0.43 \n    -0.93 \n    0.74 \n    0.21 \n    -0.04 \n    0.48 \n    0.93 \n  \n  \n    \\(x_2=1\\) \n    1.33 \n    0.60 \n    0.03 \n    -0.74 \n    1.14 \n    0.52 \n    1.10 \n    0.65 \n    0.09 \n  \n  \n    \\(x_2=2\\) \n    1.15 \n    0.50 \n    0.02 \n    0.46 \n    0.91 \n    0.61 \n    1.03 \n    0.55 \n    0.06 \n  \n  \n    \\(x_2=3\\) \n    0.77 \n    0.48 \n    0.11 \n    -0.41 \n    0.89 \n    0.64 \n    0.52 \n    0.52 \n    0.31 \n  \n  \n    \\(x_2=4\\) \n    -1.11 \n    0.54 \n    0.04 \n    -2.74 \n    1.02 \n    0.01 \n    -1.04 \n    0.57 \n    0.07 \n  \n  \n    \\(x_3\\) \n    1.35 \n    0.26 \n    0.00 \n    0.80 \n    0.44 \n    0.07 \n    1.19 \n    0.27 \n    0.00 \n  \n  \n    \\(x_4\\) \n    1.83 \n    0.30 \n    0.00 \n    2.25 \n    0.58 \n    0.00 \n    1.52 \n    0.37 \n    0.00 \n  \n  \n    \\(x_5\\) \n    0.11 \n    0.02 \n    0.00 \n    0.11 \n    0.03 \n    0.00 \n    0.10 \n    0.02 \n    0.00 \n  \n  \n    \\(x_6=1\\) \n    2.41 \n    0.38 \n    0.00 \n    2.23 \n    0.66 \n    0.00 \n    2.26 \n    0.38 \n    0.00 \n  \n  \n    \\(x_6=2\\) \n    1.04 \n    0.25 \n    0.00 \n    0.83 \n    0.44 \n    0.06 \n    1.00 \n    0.25 \n    0.00 \n  \n\n\n\n\n\n\nOn peut remarquer que la précision est systématiquement meilleure avec l’imputation multiple; les erreurs-type pour l’imputation multiple sont plus petits que celle du modèle qui retire les données incomplètes.\nOn voit que la variable \\(X_3\\) (sexe) est significative avec l’imputation multiple. Son paramètre estimé est 1.19, comparativement à 1.349 s’il n’y avait pas eu de valeurs manquantes. La précision dans l’estimation avec l’imputation multiple est seulement un peu moins bonne (erreur-type de 0.27) que celle s’il n’y avait pas eu de manquantes (erreur type de 0.26). Le paramètre de \\(\\mathsf{I}(X_6=2)\\) redevient aussi significatif, alors qu’il ne l’était plus si on retirait les manquantes. Il est peu probable que les données soit \\(\\mathsf{MCAR}\\) et donc les résultats de l’analyse des cas complets seraient biaisés."
  },
  {
    "objectID": "07-donneesmanquantes.html#valeurs-manquantes-dans-un-contexte-de-prédiction",
    "href": "07-donneesmanquantes.html#valeurs-manquantes-dans-un-contexte-de-prédiction",
    "title": "8  Données manquantes",
    "section": "8.4 Valeurs manquantes dans un contexte de prédiction",
    "text": "8.4 Valeurs manquantes dans un contexte de prédiction\nNous avons vu que l’imputation multiple permet de corriger les écarts-type des paramètres estimés afin d’obtenir une inférence valide. Mais cette fois-ci, le but n’est pas d’estimer un modèle afin de tester formellement certaines hypothèses, mais plutôt de développer un modèle pour obtenir des prédictions. Dans ce cas, l’imputation multiple peut aussi être utile.\nNous allons revenir une dernière fois sur l’exemple de ciblage de clients pour l’envoi d’un catalogue. Rappelez-vous qu’on a un échantillon d’apprentissage de 1000 clients pour lesquels la variable \\(\\texttt{yachat}\\) est disponible (est-ce que le client a acheté quelque chose lorsqu’on lui a envoyé un catalogue). Nous avons développé des modèles avec ces 1000 clients afin de décider à qui, parmi les 100 000 clients restants, envoyer le catalogue. Nous avions alors utilisé des données sans valeurs manquantes. Cette fois-ci, nous allons faire comme s’il y avait des valeurs manquantes pour les variables explicatives à la fois dans l’échantillon d’apprentissage mais aussi dans l’échantillon à prédire. Nous allons chercher à développer un modèle de régression logistique pour \\(\\mathsf{P}(\\texttt{yachat}=1)\\). Plusieurs approches sont possibles et il n’est pas clair à priori laquelle est la meilleure. Voici la description de deux approches.\nApproche 1: \n\nObtenir \\(K\\) ensembles de données complets par imputations multiples (simultanément pour les échantillons tests et d’apprentissage).\nPour chaque ensemble de données complet,\n\nFaire une sélection de variables\nObtenir les estimations de \\(\\mathsf{P}(\\texttt{yachat}=1)\\)\n\nPour chaque observation dans les deux échantillons faire la moyenne des \\(K\\) estimations de \\(\\mathsf{P}(\\texttt{yachat}=1)\\) de manière à avoir une seule prédiction par observation de la probabilité d’achat.\nTrouver le meilleur point de coupure avec les probabilités calculées en 3) pour l’échantillon d’apprentissage.\nAssigner les observations de l’échantillon à prédire avec ce point de coupure en utilisant les probabilités calculées en 3) pour les données de l’échantillon test.\n\nApproche 2:\n\nObtenir \\(K\\) ensembles de données complets par imputations multiples (simultanément pour les échantillons tests et d’apprentissage).\nPour chaque ensemble de données complet,\n\nFaire une sélection de variables\nTrouver le meilleur point de coupure par validation-croisée\nObtenir les prédictions (\\(0\\) ou \\(1\\)) pour l’échantillon à scorer, avec ce point de coupure.\n\nAssigner l’observation à la classe majoritaire (celle qui a le plus de votes parmi zéro ou un pour les \\(K\\) prédictions) pour chaque observation à prédire.\n\nNous allons seulement utiliser une approximation de la première approche ici et ignorer les valeurs de yachat et ymontant lors de l’imputation car ces dernières sont manquantes dans l’échantillon test et on se trouverait imputer à partir de modèles différentes dans les deux échantillons (test et apprentissage). Il y a des valeurs manquantes dans chaque variable: par exemple, 99 valeurs manquantes pour \\(X_1\\). Globalement, seulement 201 des 1000 clients n’ont aucune valeur manquante sur les 10 variables. Il y a 164 configurations différentes de valeurs manquantes.\nEn utilisant l’approche 1 présentée plus haut, nous allons imputer simultanément les valeurs manquantes pour l’échantillon d’apprentissage et l’échantillon test à prédire avec \\(K=5\\) échantillons imputés. La méthode de sélection de variables utilisée est la procédure séquentielle classique avec 0.05 comme critère d’entrée et de sortie. De plus, afin de simplifier le tout, le point de coupure a été fixé à 0.14 (celui que l’on avait obtenu dans le cas où il n’y a pas de valeurs manquantes) et non pas estimé par validation-croisée.\nIl s’avère que le revenu net, sur les 100 000 clients restants, aurait été de 926 917$. S’il n’y avait pas eu de valeurs manquantes, la sélection basée sur une procédure séquentielle classique aurait généré un revenu net de 969 350$. Les données manquantes rendent plus difficile le développement du modèle. Mais on fait quand même encore beaucoup mieux que la stratégie de référence qui consiste à envoyer le catalogue aux 100 000 clients, qui aurait généré un revenu net de 601 112$."
  }
]