[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 60602 - Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "d’Astous, A. (2000). Le projet de recherche en marketing, 2e édition. Chenelière/McGraw-Hill. ↩︎"
  },
  {
    "objectID": "01-analyseexploratoire.html",
    "href": "01-analyseexploratoire.html",
    "title": "2  Analyse exploratoire",
    "section": "",
    "text": "L’analyse exploratoire, comme son nom l’indique, est une étape préliminaire à la modélisation servant à l’acquisition d’une meilleure compréhension des données. L’analyse exploratoire sert à nous assurer que notre analyse ou notre traitement de ces dernières est cohérent. Le but de l’analyse exploratoire graphique est d’extraire des informations utiles, le plus souvent par le biais d’une série de questions qui sont raffinées au fur et à mesure que progresse l’analyse. On s’intéresse particulièrement aux relations et interactions entre différentes variables et la distribution empirique de chaque variable. Les étapes majeures sont:\nDans un rapport, un résumé des caractéristiques les plus importantes devrait être inclut pour que le lecteur ou la lectrice puisse valider son interprétation des données."
  },
  {
    "objectID": "01-analyseexploratoire.html#types-de-variables",
    "href": "01-analyseexploratoire.html#types-de-variables",
    "title": "2  Analyse exploratoire",
    "section": "2.1 Types de variables",
    "text": "2.1 Types de variables\n\nUne variable représente une caractéristique de la population d’intérêt, par exemple le sexe d’un individu, le prix d’un article, etc.\nune observation, parfois appelée donnée, est un ensemble de mesures collectées sous des conditions identiques, par exemple pour un individu ou à un instant donné.\n\nLe choix de modèle statistique ou de test dépend souvent du type de variables collectées. Les variables peuvent être de plusieurs types: quantitatives (discrètes ou continues) si elles prennent des valeurs numériques, qualitatives (binaires, nominales ou ordinales) si elles sont décrites par un adjectif; je préfère le terme catégorielle, plus évocateur.\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Illustrations par Allison Horst de variables numériques (gauche) et catégorielles (droite).\n\n\nOn distingue deux types de variables quantitatives:\n\nune variable discrète prend un nombre dénombrable de valeurs; ce sont souvent des variables de dénombrement ou des variables dichotomiques.\nune variable continue peut prendre (en théorie) une infinité de valeurs, même si les valeurs mesurées sont arrondies ou mesurées avec une précision limitée (temps, taille, masse, vitesse, salaire). Dans bien des cas, nous pouvons considérer comme continues des variables discrètes si elles prennent un assez grand nombre de valeurs.\n\nLes variables catégorielles représentent un ensemble fini de possibilités. On les regroupe en deux types, pour lesquels on ne fera pas de distinction:\n\nnominales s’il n’y a pas d’ordre entre les modalités (sexe, couleur, pays d’origine) ou\nordinale (échelle de Likert, tranche salariale).\n\nLa codification des modalités des variables catégorielle est arbitraire; en revanche, on préservera l’ordre lorsqu’on représentera graphiquement les variables ordinales. Lors de l’estimation, chaque variable catégorielle doit est transformée en un ensemble d’indicateurs binaires: il est donc essentiel de déclarer ces dernières dans votre logiciel statistique, surtout si elles sont encodées dans la base de données à l’aide de valeurs entières."
  },
  {
    "objectID": "01-analyseexploratoire.html#validation-des-données.",
    "href": "01-analyseexploratoire.html#validation-des-données.",
    "title": "2  Analyse exploratoire",
    "section": "2.2 Validation des données.",
    "text": "2.2 Validation des données.\nAvant de regarder les données, il est souvent utile de se plonger dans la description de la base de données. Il n’est pas rare que cette dernière contienne des informations pertinentes sur la codification des données, par exemple\n\ntelle variable catégorielle est stockée avec des valeurs entières et les étiquettes ne sont disponibles que dans la description.\ndes valeurs manquantes sont encodées avec \\(-1\\) (pour les variables positives) ou \\(999\\).\nune variable est une fonction, transformation ou combinaison d’autres variables."
  },
  {
    "objectID": "01-analyseexploratoire.html#graphiques",
    "href": "01-analyseexploratoire.html#graphiques",
    "title": "2  Analyse exploratoire",
    "section": "2.3 Graphiques",
    "text": "2.3 Graphiques\nLe principal type de graphique pour représenter la distribution d’une variable catégorielle est le diagramme en bâtons, dans lequel la fréquence de chaque catégorie est présentée sur l’axe des ordonnées (\\(y\\)) en fonction de la modalité, sur l’axe des abscisses (\\(x\\)), et ordonnées pour des variables ordinales. Cette représentation est en tout point supérieur au diagramme en camembert, une engeance répandue qui devrait être honnie (notamment parce que l’humain juge mal les différences d’aires, qu’une simple rotation change la perception du graphique et qu’il est difficile de mesurer les proportions) — ce n’est pas de la tarte!\n\n\nCode\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\ndata(renfe, package = \"hecmodstat\")\ng1 <- renfe |>\n    count(classe) |>\n    mutate(classe = forcats::fct_reorder(classe, n))  |>\nggplot(mapping = aes(y = classe, x = n)) + \n    geom_col() + \n    labs(subtitle = \"classe\", \n    x = \"dénombrement\", \n    y = \"\")\ng2 <- renfe |>\n    count(type) |>\n    mutate(type = forcats::fct_reorder(type, n))  |>\nggplot(mapping = aes(y = type, x = n)) + \n    geom_col() + \n    labs(subtitle = \"type de train\", \n         x = \"dénombrement\", \n         y = \"\")\ng1 + g2\n\n\n\n\n\n\n\nFigure 2.2: Diagramme en bâtons pour la classe des billets de trains du jeu de données Renfe.\n\n\n\n\nPuisque les variables continues peuvent prendre autant de valeurs distinctes qu’il y a d’observations, on ne peut simplement compter le nombre d’occurrence par valeur unique. On regroupera plutôt dans un certain nombre d’intervalle, en discrétisant l’ensemble des valeurs en classes pour obtenir un histogramme. Le nombre de classes dépendra du nombre d’observations si on veut que l’estimation ne soit pas impactée par le faible nombre d’observations par classe: règle générale, le nombre de classes ne devrait pas dépasser \\(\\sqrt{n}\\), où \\(n\\) est le nombre d’observations de l’échantillon. On obtiendra la fréquence de chaque classe, mais si on normalise l’histogramme (de façon à ce que l’aire sous les bandes verticales égale un), on obtient une approximation discrète de la fonction de densité. Faire varier le nombre de classes permet parfois de faire apparaître des caractéristiques de la variable (notamment la multimodalité, l’asymmétrie et les arrondis).\nPuisque qu’on groupe les observations en classe pour tracer l’histogramme, il est difficile de voir l’étendue des valeurs que prenne la variable: on peut rajouter des traits sous l’histogramme pour représenter les valeurs uniques prises par la variable, tandis que la hauteur de l’histogramme nous renseigne sur leur fréquence relative.\n\n\nCode\nrenfe |>\n  subset(tarif == \"Promo\") |>\n  ggplot(aes(x = prix)) + \n    geom_histogram(aes(y = after_stat(density)), \n                   bins = 30) +\n    geom_rug(sides = \"b\") + \n    labs(x = \"prix de billets au tarif Promo (en euros)\", \n         y = \"densité\") \n\n\n\n\n\nFigure 2.3: Histogramme du prix des billets au tarif Promo de trains du jeu de données Renfe\n\n\n\n\nUne boîte à moustaches représente graphiquement cinq statistiques descriptives.\n\nLa boîte donne les 1e, 2e et 3e quartiles \\(q_1, q_2, q_3\\). Il y a donc 50% des observations sont au-dessus/en-dessous de la médiane \\(q_2\\) qui sépare en deux la boîte.\nLa longueur des moustaches est moins de \\(1.5\\) fois l’écart interquartile \\(q_3-q_1\\) (tracée entre 3e quartile et le dernier point plus petit que \\(q_3+1.5(q_3-q_1)\\), etc.)\nLes observations au-delà des moustaches sont encerclées. Notez que plus le nombre d’observations est élevé, plus le nombres de valeurs extrême augmente. C’est un défaut de la boîte à moustache, qui a été conçue pour des jeux de données qui passeraient pour petits selon les standards actuels.\n\n\n\n\n\n\nFigure 2.4: Boîte à moustache.\n\n\n\n\nOn peut représenter la distribution d’une variable réponse continue en fonction d’une variable catégorielle en traçant une boîte à moustaches pour chaque catégorie et en les disposant côte-à-côte. Une troisième variable catégorielle peut être ajoutée par le biais de couleurs, comme dans la Figure 2.5.\n\n\nCode\nrenfe |> \n   subset(tarif == \"Promo\") |>\n   ggplot(aes(y = prix, x = classe, col = type)) + \n    geom_boxplot() + \n    labs(y = \"prix (en euros)\", \n         col = \"type de train\") + \n    theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2.5: Boîte à moustaches du prix des billets au tarif Promo en fonction de la classe pour le jeu de données Renfe.\n\n\n\n\nSi on veut représenter la covariabilité de deux variables continues, on utilise un nuage de points où chaque variable est représentée sur un axe et chaque observation donne la coordonnée des points. Si la représentation graphique est dominée par quelques valeurs très grandes, une transformation des données peut être utile: vous verrez souvent des données positives à l’échelle logarithmique.\nPlutôt que de décrire plus en détail le processus de l’analyse exploratoire, on présente un exemple qui illustre le cheminement habitue sur les données de trains de la Renfe introduites précédemment."
  },
  {
    "objectID": "01-analyseexploratoire.html#exemple",
    "href": "01-analyseexploratoire.html#exemple",
    "title": "2  Analyse exploratoire",
    "section": "2.4 Exemple",
    "text": "2.4 Exemple\nLa première étape consisterait à lire la description de la base de données. Le jeu de données renfe contient les variables suivantes:\n\nprix: prix du billet (en euros);\ndest: indicateur binaire du trajet, soit de Barcelone vers Madrid (0) ou de Madrid vers Barcelone (1);\ntarif: variable catégorielle indiquant le tarif du billet, un parmi AdultoIda, Promo et Flexible;\nclasse: classe du billet, soit Preferente, Turista, TuristaPlus ou TuristaSolo;\ntype: variable catégorielle indiquant le type de train, soit Alta Velocidad Española (AVE), soit Alta Velocidad Española conjointement avec TGV (un partenariat entre la SNCF et Renfe pour les trains à destination ou en provenance de Toulouse) AVE-TGV, soit les trains régionaux REXPRESS; seuls les trains étiquetés AVE ou AVE-TGV sont des trains à grande vitesse.\nduree: longueur annoncée du trajet (en minutes);\njour entier indiquant le jour de la semaine du départ allant de dimanche (1) à samedi (7).\n\nIl n’y a pas de valeurs manquantes et un aperçu des données (head(renfe)) montre qu’elles sont en format long, ce qui veut dire que chaque ligne contient une seule valeur pour la variable réponse, ici le prix d’un billet de train. On entame l’analyse exploratoire avec des questions plutôt vagues, par exemple\n\nQuels sont les facteurs déterminant le prix et le temps de parcours?\nEst-ce que le temps de parcours est le même peut importe le type de train?\nQuelles sont les caractéristiques distinctives des types de train?\nQuelles sont les principales différences entre les tarifs?\n\nÀ l’exception de prix et de duree, toutes les variables explicatives sont catégorielles. La variable jour prends des valeurs entre 1 et 7; s’en souvenir pour éviter les mauvaises surprises ultérieures. En analysant le nombre de trains dans les catégories, on remarque qu’il y a autant de billets de type REXPRESS que le nombre de billets au tarif AdultoIda. On peut faire le décompte par catégorie avec un tableau de contingence, qui compte le nombre respectif dans chaque sous-catégorie. Dans la base de données Renfe, tous les billets pour les RegioExpress sont vendus au tarif AdultoIda en classe Turista. Le nombre de billets est minime, à peine 397 sur 10000. Cela suggère une nouvelle question: pourquoi ces trains sont-ils si peu populaires?\nOn remarque également que seulement 17 temps de parcours sont affichés sur les billets. On peut donc penser que la durée affichée sur le billet (en minutes) est le temps de trajet annoncé. La majeure partie (15 sur 17) des temps de parcours sont sous la barre des 3h15, hormis deux qui dépassent les 9h! Selon Google Maps, les deux villes sont distantes de 615km par la route, 500km à vol d’oiseau. Cela implique que, vraisemblablement, certains trains dépassent les 200km/h, tandis que d’autres vont plutôt à 70km/h. Quels sont ces trains plus lent? La variable type codifie probablement ce fait, et permet de voir que ce sont les trains RegioExpress qui sont dans cette catégorie.\nAller de Madrid à Barcelone à l’aide d’un train régulier prend 18 minutes de plus. Avec plus de 9h de trajet, pas étonnant donc que ces billets soient peu courus. Encore plus frappant, on note que le prix des billets est fixe: 43.25 euros peu importe que le trajet soit aller ou retour. C’est probablement la trouvaille la plus importante jusqu’à maintenant, car les billets de train de type RegioExpress ne forment pas un échantillon: il n’y a aucune variabilité! On aurait également pu découvrir cette anomalie en traçant une boîte à moustaches du prix en fonction du type de train.\n\n\nCode\nggplot(data = renfe, \n       mapping = aes(x = type, y = prix, col = dest)) + \n  geom_boxplot() + \n  labs(y = \"prix (en euros)\",\n       x = \"type de train\",\n       color = \"destination\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2.6: Boîte à moustaches du prix de billets de train de Renfe en fonction de la destination et du type de train.\n\n\n\n\nOn pourrait soupçonner que les trains étiquetés AVE sont plus rapides, sachant que c’est l’acronyme de Alta Velocidad Española, littéralement haute vitesse espagnole. Qu’en est-il des distinctions entre les deux types de trains étiquetés AVE? Selon le site de la SNCF, les trains AVE-TGV sont des partenariats entre la Renfe et la SNCF et effectuent des liaisons entre la France et l’Espagne.\nLes prix sont beaucoup plus élevés, en moyenne plus de deux fois plus que les trains régionaux. Les écarts de prix importants (l’écart type est de 20 euros) indique qu’il y a peut-être d’autres sources d’hétérogénéité, mais on pourrait soupçonner que la Renfe pratique la tarification dynamique. Il y un seul temps de parcours prévu pour les trains AVE-TGV. On ne note pas de différence de prix notable selon la direction ou le type de train grande vitesse, mais peut-être que les tarifs ou la classe disponibles diffèrent selon que le train ou non est en partenariat avec la compagnie française.\nOn n’a pas encore considéré le tarif et la classe des billets, hormis pour les trains RegioExpress. On voit dans la Figure 2.8 une forte différente dans l’hétérogénéité des prix selon le tarif; le tarif Promo prend plusieurs valeurs distinctes, tandis que les tarifs AdultoIda et Flexible semblent ne prendre que quelques valeurs. La première classe (Preferente) est plus chère et il y a moins d’observations dans ce groupe. La classe Turista est la classe la moins dispendieuse et la plus populaire. TuristaPlus offre plus de confort, tandis que TuristaSolo permet d’obtenir un siège individuel.\nCôté tarif, Promo et PromoPlus permette d’obtenir des rabais pouvant aller jusqu’à respectivement 70% et 65%. Les annulations et changements ne sont pas possibles avec Promo, mais disponibles avec PromoPlus moyennant une pénalité équivalent à 30-20% du prix du billet. Le tarif Flexible est disponible au même prix que les billets réguliers, avec des bénéfices additionnels.\n\n\nCode\nrenfe |> \n  subset(tarif  != \"AdultoIda\") |>\n  ggplot(aes(y = prix, \n             x = classe, \n             col = tarif)) + \n    geom_boxplot() + \n    labs(y = \"prix (en euros)\",\n         x = \"classe\",\n         color = \"tarif\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2.7: Boîte à moustaches du prix en fonction du tarif et de la classe de billets de trains à haute vitesse de la Renfe.\n\n\n\n\n\n\nCode\nggplot(data = renfe, \n       mapping = aes(x = prix, \n                     y = after_stat(density), \n                     fill = tarif)) +\n    geom_histogram(binwidth = 5) +\n    labs(x = \"prix (en euros)\", \n         y = \"densité\") + \n    theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2.8: Histogrammes du prix en fonction du tarif de billets de trains de la Renfe.\n\n\n\n\n\n\nCode\nrenfe |>\n  dplyr::subset(tarif  == \"Flexible\") |>\n  dplyr::count(prix, classe)\n\n\n\n\n\n\nTableau 2.1:  Nombre de billets au tarif Flexible selon le prix de vente. \n \n  \n    prix \n    classe \n    n \n  \n \n\n  \n    107.7 \n    Turista \n    1050 \n  \n  \n    107.7 \n    TuristaSolo \n    67 \n  \n  \n    127.1 \n    Turista \n    285 \n  \n  \n    127.1 \n    TuristaSolo \n    9 \n  \n  \n    129.3 \n    TuristaPlus \n    31 \n  \n  \n    140.0 \n    Preferente \n    2 \n  \n  \n    152.5 \n    TuristaPlus \n    10 \n  \n  \n    181.5 \n    Preferente \n    78 \n  \n  \n    214.2 \n    Preferente \n    12 \n  \n\n\n\n\n\n\nOn note que la répartition des prix pour les billets de classe Flexible est inhabituelle. Notre boîte à moustaches est écrasée et l’écart interquartile semble nul, même si quelques valeurs inexpliquées sont aussi présentes. L’écrasante majorité des billets Flexibles sont en classe Turista, donc ça pourrait être dû à un (trop) faible nombre de billets dans chaque catégorie. On peut rejeter cette hypothèse en calculant le nombre de trains au tarif Flexible pour les différents types de billets, comme dans le Tableau 2.1. Ni la durée, ni le type de train, ni la destination n’expliquent pas pourquoi le prix de certains billets Flexibles est plus faible ou élevés. Le prix des billets Promo est plus faible, et les billets au tarif Preferente (la première classe) sont plus élevés.\nOn peut résumer notre brève analyse exploratoire:\n\nplus de 91% des trains sont des trains à grande vitesse AVE.\nle temps de trajet dépend du type de train: les trains à grande vitesse mettent 3h20 au maximum pour relier Madrid et Barcelone.\nles temps de trajets sont ceux annoncés (variable discrète avec 17 valeurs uniques, dont 13 pour les trains AVE)\nle prix de trains RegioExpress est fixe (43.25€); tous ces billets sont dans la classe Turista et au tarif Adulto Ida. 57% de ces trains vont de Barcelone à Madrid. La durée du trajet pour les RegioExpress est de 9h22 de Barcelona à Madrid, 18 minutes de plus que dans l’autre direction.\nles billets en classe Preferente sont plus chers et moins fréquents. La classe Turista est la classe la moins dispendieuse et la plus populaire. TuristaPlus offre plus de confort, tandis que TuristaSolo permet d’obtenir un siège individuel.\nselon le site web de la Renfe, les billets au tarif Flexible « viennent avec des offres additionnelles qui permettent au passagers d’échanger leurs billets ou annuler s’ils manquent leurs trains. »; en contrepartie, ces billets sont plus chers et leur tarif est fixe sauf une poignée de billets dont le prix reste inexpliqué.\nla distribution des prix des billets de TGV au tarif Promo est plus ou moins symmétrique, tandis que les billets au tarif Flexible apparaissent tronqués à gauche (le prix minimum pour ces billets est 107.7€ dans l’échantillon).\nla Renfe pratique la tarification dynamique pour les billets au tarif promotionnel Promo: ces derniers peuvent être jusqu’à 70% moins chers que les billets à prix régulier lorsqu’achetés via l’agence officielle ou le site de Renfe. Ces billets ne peuvent être ni remboursés, ni échangés.\nil n’y a pas d’indication à effet de quoi les prix varient selon la direction du trajet.\n\n\n2.4.1 Commentaire sur les graphiques\nSi vous incluez un graphique (ou un tableau), il est important d’ajouter une légende qui décrit le graphique et le résume, les noms de variables (avec les unités) sur les axes, mais aussi de soigner le rendu et le formatage pour obtenir un produit fini propre, lisible et cohérent: en particulier, votre description devrait coïncider avec le rendu. Votre graphique raconte une histoire, aussi prenez-soin que cette dernière soit nécessaire et attrayante.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nUne base de données est normalement constituée de plusieurs variables (colonnes); les lignes représentent les différentes observations.\nOn classifie grossièrement les variables en variables catégorielles (nominales, ordinales ou binaires) et numériques (continues, entières).\nL’analyse exploratoire est une procédure dynamique qui sert à mieux comprendre les données pour proposer des modèles adéquats. Elle consiste à poser des questions et à raffiner les conclusions à l’aide de tableaux résumés et de graphique.\nLe nettoyage et la validation des données est la première étape de toute analyse. On peut formuler nos attentes et l’utiliser pour vérifier de la conformité de notre base de données (en utilisant des outils comme le paquet pointblank).\nUne bonne compréhension des données est nécessaire avant d’envisager la modélisation.\nIl faut déclarer correctement les variables explicatives catégorielles (souvent encodées avec des entiers).\nL’utilisation de graphiques est privilégiée par rapport aux tableaux: une image vaut mille mots.\nLes graphiques usuels employés sont l’histogramme et la boîte à moustaches (données numériques) et le diagramme à bande (données catégorielles). L’utilisation de diagramme circulaires est à proscrire.\nOn peut utiliser la couleur, la forme ou la taille comme dimensions additionnelles.\nToujours utiliser une palette de couleurs pour daltoniens.\nUn graphique devrait toujours inclure une légende globale décrivant la représentation (en plus d’être discuté dans le texte), une légende pour les axes et des unités. Les caractères doivent être lisibles (suffisamment grands).\nUne analyse exploratoire inclut un résumé des principales trouvailles."
  },
  {
    "objectID": "02-analysefactorielle.html#introduction",
    "href": "02-analysefactorielle.html#introduction",
    "title": "3  Réduction de la dimension",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nCe chapitre traite de réduction de la dimensionalité d’un problème d’analyse multidimensionnelle. On dispose de \\(p\\) variables \\(X_1, \\ldots, X_p\\): comment résumer cet ensemble avec moins de variables (disons \\(k\\)) tout en conservant le plus de variabilité possible? Nous couvrons deux méthodes dans ce chapitre: la première, intitulée analyse en composantes principales, cherche à réduire le nombre de variables explicatives tout en préservant le plus possible de variabilité exprimée et en créant de nouvelles variables explicatives qui ne sont pas corrélées les unes avec les autres.\nLa deuxième, appelée analyse factorielle exploratoire, cherche à expliquer la structure de corrélation entre les \\(p\\) variables à l’aide d’un nombre restreint de facteurs. Elle répond aux questions suivantes:\n\nY a-t-il des groupements de variables?\nEst-ce que les variables faisant partie d’un groupement semblent mesurer certains aspects d’un facteur commun (non observé)?\n\nDe tels groupements peuvent être détecté si plusieurs variables sont très corrélées entre elles. Une analyse factorielle cherchera à identifier automatiquement ces groupes de variables.\nLes facteurs sont des variables latentes qui mesurent des constructions. Par exemple, l’habileté quantitative, habileté sociale, importance accordée à la qualité du service, importance accordée à la loyauté, habileté de leader, etc.\nL’analyse factorielle est aussi une méthode de réduction du nombre de variables. En effet, une fois qu’on a identifié les facteurs, on peut remplacer les variables individuelles par un résumé pour chaque facteur (qui est souvent la moyenne des variables qui font partie du facteur)."
  },
  {
    "objectID": "02-analysefactorielle.html#coefficient-de-corrélation-linéaire",
    "href": "02-analysefactorielle.html#coefficient-de-corrélation-linéaire",
    "title": "3  Réduction de la dimension",
    "section": "3.2 Coefficient de corrélation linéaire",
    "text": "3.2 Coefficient de corrélation linéaire\nOn veut examiner la relation entre deux variables \\(X_j\\) et \\(X_k\\) et on dispose de \\(n\\) couples d’observations, où \\(x_{i, j}\\) (respectivement \\(x_{i, k}\\)) est la valeur de la variable \\(X_j\\) (\\(X_k\\)) pour la \\(i\\)e observation.\nLe coefficient de corrélation linéaire entre \\(X_j\\) et \\(X_k\\), que l’on note \\(r_{j, k}\\), cherche à mesurer la force de la relation linéaire entre deux variables, c’est-à-dire à quantifier à quel point les observations sont alignées autour d’une droite. Le coefficient de corrélation est \\[\\begin{align*}\nr_{j, k} &= \\frac{\\widehat{\\mathsf{Co}}(X_j, X_k)}{\\{\\widehat{\\mathsf{Va}}(X_j) \\widehat{\\mathsf{Va}}(X_k)\\}^{1/2}}\n%\\\\&=\\frac{\\sum_{i=1}^n (x_{i, j}-\\overline{x}_j)(x_{i, k} -\\overline{x}_{k})}{\\left\\{\\sum_{i=1}^n (x_{i, j}-\\overline{x}_j)^2 \\sum_{i=1}^n(x_{i, k} -\\overline{x}_{k})^2\\right\\}^{1/2}}\n\\end{align*}\\]\nLes propriétés les plus importantes du coefficient de corrélation linéaire \\(r\\) sont les suivantes:\n\n\\(-1 \\leq r \\leq 1\\);\n\\(r=1\\) (respectivement \\(r=-1\\)) si et seulement si les \\(n\\) observations sont exactement alignées sur une droite de pente positive (négative). C’est-à-dire, s’il existe deux constantes \\(a\\) et \\(b>0\\) (\\(b<0\\)) telles que \\(y_i=a+b x_i\\) pour tout \\(i=1, \\ldots, n\\).\n\nRègle générale,\n\nLe signe de la corrélation détermine l’orientation de la pente (négative ou positive)\nPlus la corrélation est près de 1 en valeur absolue, plus les points auront tendance à être alignés autour d’une droite.\nLorsque la corrélation est presque nulle, les points n’auront pas tendance à être alignés autour d’une droite. Il est très important de noter que cela n’implique pas qu’il n’y a pas de relation entre les deux variables. Cela implique seulement qu’il n’y a pas de relation linéaire entre les deux variables. La Figure 3.1 montre bien ce point: ces jeux de données ont la même corrélation linéaire (quasi-nulle), mais ne sont pas clairement pas indépendantes puisqu’elles permettent de dessiner un dinosaure ou une étoile.\n\n\n\n\n\n\n\nFigure 3.1: Datasaurus (Alberto Cairo): une douzaine de jeux de données qui ont les mêmes statistiques descriptives (à deux décimales près) et dont les deux variables sont très faiblement corrélées.\n\n\n\n\n\nLa matrice de corrélation entre \\(X_1, \\ldots, X_p\\), dont l’entrée \\((i, j)\\) contient la corrélation entre \\(X_i\\) et \\(X_j\\), est une matrice symmétrique dont les éléments de la diagonale sont égaux à 1. À mesure que le nombre de variables augmente, le nombre de corrélations à estimer augmente: puisque la matrice est \\(p \\times p\\), ce nombre augmente comme le carré du nombre de variables explicatives. L’estimation ne sera pas fiable à moins que \\(n \\gg p\\)."
  },
  {
    "objectID": "02-analysefactorielle.html#présentation-des-données",
    "href": "02-analysefactorielle.html#présentation-des-données",
    "title": "3  Réduction de la dimension",
    "section": "3.3 Présentation des données",
    "text": "3.3 Présentation des données\nLe questionnaire suivant porte sur une étude dans un magasin. Pour les besoins d’une enquête, on a demandé à 200 consommateurs adultes de répondre aux questions suivantes par rapport à un certain type de magasin sur une échelle de 1 à 5, où\n\npas important\npeu important\nmoyennement important\nassez important\ntrès important\n\nPour vous, à quel point est-ce important\n\nque le magasin offre de bons prix tous les jours?\nque le magasin accepte les cartes de crédit majeures (Visa, Mastercard)?\nque le magasin offre des produits de qualité?\nque les vendeurs connaissent bien les produits?\nqu’il y ait des ventes spéciales régulièrement?\nque les marques connues soient disponibles?\nque le magasin ait sa propre carte de crédit?\nque le service soit rapide?\nqu’il y ait une vaste sélection de produits?\nque le magasin accepte le paiement par carte de débit?\nque le personnel soit courtois?\nque le magasin ait en stock les produits annoncés?\n\nLes statistiques descriptives ainsi que la matrice des corrélations sont obtenues en exécutant les lignes suivantes:\n\ndata(factor, package = \"hecmulti\")\n# Matrice de corrélation\ncor(factor)\n# Statistiques descriptives\nsummary(factor)\n\n\n\n\n\nTableau 3.1:  Matrice de corrélation de factor. \n \n  \n      \n    x1 \n    x2 \n    x3 \n    x4 \n    x5 \n    x6 \n    x7 \n    x8 \n    x9 \n    x10 \n    x11 \n    x12 \n  \n \n\n  \n    x1 \n     \n    -0.08 \n    -0.14 \n    -0.07 \n    0.38 \n    -0.01 \n    -0.10 \n    -0.13 \n    -0.03 \n    -0.11 \n    -0.12 \n    -0.01 \n  \n  \n    x2 \n     \n     \n    0.04 \n    -0.02 \n    -0.08 \n    0.06 \n    0.50 \n    0.01 \n    -0.01 \n    0.43 \n    -0.12 \n    0.07 \n  \n  \n    x3 \n     \n     \n     \n    0.10 \n    -0.06 \n    0.39 \n    0.00 \n    0.05 \n    0.47 \n    0.08 \n    0.13 \n    0.46 \n  \n  \n    x4 \n     \n     \n     \n     \n    -0.05 \n    0.06 \n    0.08 \n    0.57 \n    0.01 \n    0.09 \n    0.50 \n    0.09 \n  \n  \n    x5 \n     \n     \n     \n     \n     \n    -0.04 \n    -0.04 \n    -0.02 \n    0.03 \n    -0.07 \n    -0.06 \n    -0.07 \n  \n  \n    x6 \n     \n     \n     \n     \n     \n     \n    0.07 \n    0.04 \n    0.32 \n    0.07 \n    -0.04 \n    0.32 \n  \n  \n    x7 \n     \n     \n     \n     \n     \n     \n     \n    0.09 \n    -0.02 \n    0.51 \n    -0.03 \n    0.02 \n  \n  \n    x8 \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.03 \n    0.16 \n    0.55 \n    0.04 \n  \n  \n    x9 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n    0.01 \n    0.02 \n    0.39 \n  \n  \n    x10 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n    0.01 \n    0.02 \n  \n  \n    x11 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n    0.05 \n  \n  \n    x12 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Corrélogramme de la base de données factor.\n\n\n\n\n\n\n\n\n\nTableau 3.2:  Statistiques descriptives des 12 variables du jeu de données factor. \n \n  \n    moyenne \n    écart-type \n    min \n    max \n    histogramme \n  \n \n\n  \n    2.26 \n    1.13 \n    1 \n    5 \n    ▇▇▆▃▁ \n  \n  \n    2.51 \n    1.24 \n    1 \n    5 \n    ▇▇▇▅▂ \n  \n  \n    3.00 \n    1.19 \n    1 \n    5 \n    ▃▆▇▇▃ \n  \n  \n    2.91 \n    1.33 \n    1 \n    5 \n    ▆▇▇▆▅ \n  \n  \n    3.55 \n    1.17 \n    1 \n    5 \n    ▂▃▇▇▇ \n  \n  \n    2.14 \n    1.14 \n    1 \n    5 \n    ▇▅▅▂▁ \n  \n  \n    1.82 \n    1.06 \n    1 \n    5 \n    ▇▃▂▁▁ \n  \n  \n    2.92 \n    1.32 \n    1 \n    5 \n    ▆▇▇▇▅ \n  \n  \n    3.04 \n    1.12 \n    1 \n    5 \n    ▃▃▇▇▂ \n  \n  \n    2.59 \n    1.32 \n    1 \n    5 \n    ▇▆▆▅▂ \n  \n  \n    2.98 \n    1.33 \n    1 \n    5 \n    ▆▅▇▆▅ \n  \n  \n    3.45 \n    1.16 \n    1 \n    5 \n    ▂▃▇▇▆ \n  \n\n\n\n\n\n\n\nOn voit dans la Figure 3.2 que quelques groupes de variables sont corrélés entre eux. On peut également regrouper certaines questions sous des thèmes manuellement: le but de l’analyse factorielle sera d’automatiser ce regroupement."
  },
  {
    "objectID": "02-analysefactorielle.html#analyse-en-composantes-principales",
    "href": "02-analysefactorielle.html#analyse-en-composantes-principales",
    "title": "3  Réduction de la dimension",
    "section": "3.4 Analyse en composantes principales",
    "text": "3.4 Analyse en composantes principales\nLe but de l’analyse en composantes principales est de réduire le nombre de variables explicatives. En partant de \\(p\\) variables \\(X_1, \\ldots, X_p\\), on forme de nouvelles variables qui sont des combinaisons linéaires des variables originales, \\[\\begin{align*}\nC_j &= \\underset{\\text{somme de poids fois variables explicatives}}{w_{j1} X_1 + w_{j2} X_2 + \\cdots + w_{jp} X_p}, \\qquad (j=1, \\ldots, p),\n\\\\\n1 &= \\underset{\\text{poids standardisés}}{w_{j1}^2 + \\cdots + w_{jp}^2}\n\\end{align*}\\] de telle sorte que\n\nLa première variable formée, \\(C_1\\), appelée première composante principale, possède la variance maximale parmi toutes les combinaisons linéaires sous la contrainte \\(w_{1i}^2 + \\cdots + w_{1p}^2=1\\).1\nPour \\(j=2, \\ldots, p\\), la \\(j\\)e composante principale \\(C_j\\) possède la variance maximale parmi toutes les combinaisons linéaires qui sont non corrélées avec \\(C_1, \\ldots, C_{j-1}\\) sous la contrainte \\(w_{j1}^2 + \\cdots + w_{jp}^2=1\\).\n\nAinsi, les composantes principales forment un ensemble de variables non corrélées entre elles, qui récupèrent en ordre décroissant le plus possible de la variance des variables originales. La somme des variances des \\(p\\) composantes principales est égale à la somme des variances des \\(p\\) variables originales.\nMathématiquement, les composantes principales correspondent aux vecteurs propres de la matrice de covariance, mais on peut également utiliser la matrice de corrélation2. L’avantage de la matrice de corrélation (ou de la standardisation des variables) est que l’unité de mesure n’impacte pas le résultat; autrement, un poids plus important est attribué aux variables qui ont la plus forte hétérogénéité.\nSi on conserve toutes les composantes principales, cela revient à changer le système de coordonnées dans lequel sont exprimées nos observations en effectuant une rotation: avec deux variables, on on trouve la direction dans le système 2D dans lequel l’étendue est la plus grande. Si une simple rotation peut sembler inutile, la méthode est fort utile en haute dimension. On espère en général qu’un petit nombre de composantes principales réussira à expliquer la plus grande partie de la variance totale.\n\n\n\n\n\nFigure 3.3: Nuage de points avant (gauche) et après (droite) analyse en composantes principales. Les directions des composantes principales (lignes pleines et traitillés), qui forment un angle droit, sont ajoutées au nuage de points à gauche. On peut constater que la corrélation entre les deux composantes principales est nulle.\n\n\n\n\nLa Figure 3.3 démontre cette décomposition sur des données bidimensionnelles simulées. La variance des données dans le premier panneau est 13.51 pour l’axe des abscisse et 6.43 pour l’axe des ordonnées avec une corrélation de 0.86, à comparer avec des variances de 18.65 et 1.21 et une corrélation nulle entre les deux composantes principales.\nDans une analyse en composantes principales, on conservera un nombre \\(k<p\\) de variables explicatives pour résumer les données. Ce outil est utilisé à des fins exploratoires, puisqu’on n’implique pas de variable réponse dans le modèle. L’analyse en composantes principales est utilisé pour réduire la dimension afin de faire de la classification, de l’analyse de regroupements et aussi réduire les coûts associés à ces méthodes en projetant les données dans un sous-espace de dimension plus faible.\nEn R, on effectue l’analyse en composantes principales avec la fonction princomp ou prcomp3.\nLa sortie contient\n\nles coordonnées des composantes principales, acp$scores; la première est celle qui a la plus grande variabilité.\nl’écart-type de chaque composante, acp$sdev. Chaque écart-type est la racine carrée d’une des valeurs propres.\nles poids \\(w_{ij}\\), appelés chargements (loadings), qui donnent la correspondance entre le système de coordonnées des composantes principales et celui des variables \\(\\boldsymbol{X}\\) originales.\n\nOn peut représenter les données à l’aide d’un bigramme: c’est une nuage de points de chaque observations dans l’espace des deux premières composantes principales. Si on couple cela avec les directions offertes par les chargements pour chacune des variables explicatives \\(X_1, \\ldots, X_p\\), il en ressort que certaines variables augmentent/diminuent de pair. Ainsi, on voit dans la Figure 3.4 que les variables x3, x6, x9 et x12 tendent dans la même direction, comme x4, x8 et x11. On reviendra sur ce point dans une section subséquente.\nUne fois qu’on a choisit le nombre de composantes, on pourrait ne conserver que les \\(k\\) premières colonnes de la matrice des composantes principales acp$scores pour faire les graphiques ou pour approximer la matrice de covariance. Il faut garder en tête qu’il faudra néanmoins collecter les mêmes questions pour recréer les composantes principales avec de nouvelles observations, ce qui est peu commode si on veut réduire le coût de la collecte.\n\n# Analyse en composantes principales\n# de la matrice de corrélation\nacp <- princomp(factor, cor = TRUE)\nloadings(acp) # chargements\nbiplot(acp) # bigramme\n\n\n\n\n\n\nFigure 3.4: Bigramme: nuage de point des coordonnées des deux premières composantes principales et direction selon chargements des variables explicatives originales.\n\n\n\n\nOn peut étudier la sortie pour vérifier les propriétés de notre décomposition. Le Tableau 3.3 montre la variance de chaque composante principale. Si on additionne l’ensemble des variances (sans arrondir), on obtient une variance cumulative des 12 composantes principales, 12, soit le même que le nombre de variables explicatives puisque les variables standardisées ont variance unitaire. Si on calcule la matrice de corrélation, cor(acp$scores), on remarquera que la corrélation est nulle entre les variables.\n\n\n\n\nTableau 3.3:  Variance des composantes principales \n \n  \n    C1 \n    C2 \n    C3 \n    C4 \n    C5 \n    C6 \n    C7 \n    C8 \n    C9 \n    C10 \n    C11 \n    C12 \n  \n \n\n  \n    2.43 \n    2.00 \n    1.94 \n    1.30 \n    0.74 \n    0.69 \n    0.57 \n    0.54 \n    0.51 \n    0.47 \n    0.46 \n    0.36 \n  \n\n\n\n\n\n\n\n3.4.1 Choix du nombre de composantes principales\nSi on désire réduire la dimension, il nous faudra choisir \\(k \\leq p\\) variables. Cette section traite du choix du nombre de variables explicatives à retenir. Idéalement, ce nombre devrait être beaucoup plus petit que le nombre original de variables.\nUne première approche est de regarder le pourcentage de la variance totale expliquée. Puisque les composantes principales sont ordonnées en ordre décroissant de variance, on peut étudier la variance cumulative des \\(k\\) premières composantes et choisir un nombre qui explique le plus possible. Si l’ajout d’une variable augmente peu la variabilité totale expliquée par l’ensemble, alors cette variable est probablement superflue. On pourrait choisir un nombre de composantes pour expliquer un pourcentage prédéfini de la vairance totale, disons 70%. Deux autres critères couramment employés sont:\n\ncritère du coude de Cattell: ce critère consiste à sélectionner un nombre de composantes dans le diagramme d’éboulis (screeplot), un graphique des variances des composantes principales4. Habituellement, il y a une décroissance rapide de la variance suivie d’un plateau: on prendra le nombre de composantes qui correspond au \\(k\\) juste avant l’apparition du plateau (le début du coude, où il a stabilisation apparente). C’est un critère très subjectif, puisqu’il y a souvent plusieurs plateaux et que la variance peut décroître très lentement. On peut utiliser la fonction screeplot pour obtenir le diagramme d’éboulis mais il est facile de le créer manuellement et le résultat est esthétiquement plus réussi.\ncritère des valeurs propres de Kaiser: un critère basé sur les valeurs propres de la matrice de corrélation. Le nombre de facteurs choisis est le nombre de composantes principales dont la variance est supérieures à 1. L’idée est de garder seulement les facteurs qui expliquent plus de variance qu’une variable individuelle.\n\nSi on utilise le critère de Kaiser avec les données factor, on conservera 4 composantes principales qui expliqueront 63.9 pourcent de la variance totale des variables originales - voir le Tableau 3.3. Le diagramme d’éboulis de la Figure 3.5, qui peut être produit avec la fonction hecmulti::eboulis(eigen(cor(factor)) suggère quant à lui cinq composantes.\n\n\nCode\nhecmulti::eboulis(eigen(cor(factor)))\n\n\n\n\n\nFigure 3.5: Diagramme d’éboulis (gauche) représentant la variance des composantes principales (en ordonnée) en fonction du nombre composantes principales (en abscisse). Variance cumulative en fonction du nombre de composantes principales (droite).\n\n\n\n\nUne fois qu’on a déterminé le nombre de facteurs, on peut extraire les nouvelles variables explicatives à partir de l’analyse en composantes principales. Les colonnes sont stockées dans acp$score et il suffit de conserver les premières colonnes.\n\n\n3.4.2 Formulation mathématique\nCe complément d’information est optionnel.\nMathématiquement, le problème de l’analyse en composantes principales revient à calculer la décomposition en valeurs propres et vecteurs propres de la matrice de covariance \\(\\mathsf{Co}(\\boldsymbol{X})=\\boldsymbol{\\Sigma}\\). On peut écrire \\[\\begin{align*}\n\\boldsymbol{\\Sigma} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^\\top\n\\end{align*}\\] où \\(\\boldsymbol{\\Lambda} = \\mathrm{diag}\\{\\lambda_1, \\ldots, \\lambda_p\\}\\) est une matrice diagonale contenant les valeurs propres en ordre décroissant (\\(\\lambda_1 \\geq \\cdots \\geq \\lambda_p > 0\\)) et \\(\\boldsymbol{Q}\\) est une matrice carrée \\(p \\times p\\) orthogonale contenant les vecteurs propres. La meilleure approximation de rang \\(k \\leq p\\) de \\(\\boldsymbol{\\Sigma}\\) est obtenue en spécifiant \\[\\begin{align*}\n\\widetilde{\\boldsymbol{\\Sigma}}_k = \\sum_{j=1}^k \\lambda_j \\boldsymbol{q}_j\\boldsymbol{q}_j^\\top,\n\\end{align*}\\] une combinaison des vecteurs propres \\(\\boldsymbol{q}_1, \\ldots, \\boldsymbol{q}_k \\in \\mathbb{R}^p\\) non corrélés.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nLa corrélation mesure la force de la dépendance linéaire entre deux variables: plus elle est élevée, plus les points s’alignent.\nSi le nombre de variables explicatives \\(p\\) est conséquent par rapport au nombre d’observations \\(n\\), on a peu d’information disponible pour estimer de manière fiable les corrélations.\nUne analyse en composante principales fait une décomposition en valeurs propres/vecteurs propres de la matrice de covariance ou de corrélation.\n\nCes nouvelles variables sont orthogonales (corrélation nulle) entre elles.\nLes composantes principales sont ordonnées en ordre décroissant de variance: si on ne conserve que \\(k<p\\) de variables, on maximise la variance expliquée.\nLe choix du nombre de variables est basé sur des règles du pouce: le critère des valeurs propres de Kaiser suggère de prendre autant de composantes principales que de variances supérieures à 1.\nUn bigramme permet de représenter graphiquement les directions des variables en fonction des deux premières composantes principales."
  },
  {
    "objectID": "02-analysefactorielle.html#analyse-factorielle-exploratoire",
    "href": "02-analysefactorielle.html#analyse-factorielle-exploratoire",
    "title": "3  Réduction de la dimension",
    "section": "3.5 Analyse factorielle exploratoire",
    "text": "3.5 Analyse factorielle exploratoire\nSi le bigramme a permis de faire ressortir quelques orientations communes, on aimerait aller plus loin dans notre exploration. On considère encore une fois la matrice de covariance associée avec \\(p\\) variables explicatives \\(X_1, \\ldots, X_p\\): le modèle d’analyse factorielle cherche à décrire cette dernière en fonction d’un plus petit nombre de paramètres.\nConceptuellement, le modèle d’analyse factorielle suppose qu’on peut regrouper les variables explicatives numériques (parfois avec quelques variables binaires) à l’aide de concepts communs appelés facteurs. Certaines variables explicatives devraient donc idéalement être fortements corrélées entre elles. Le choix des variables est dicté par le bon sens: on inclut dans le modèle des variables qui peuvent logiquement être associée, par exemple des items de questionnaires excluant les données sociodémographiques.\nLe modèle d’analyse factorielle fait l’hypothèse que les variables dépendent linéairement d’un plus petit nombre de variables aléatoires, \\(F_1, \\ldots, F_m\\), appelées facteurs communs. Cette relation n’est pas parfaite, aussi on inclut \\(p\\) termes d’aléas \\(\\varepsilon_1, \\ldots, \\varepsilon_p\\), de moyenne zéro et de variance \\(\\mathsf{Va}(\\varepsilon_i)=\\psi_i\\) (\\(i=1, \\ldots, p\\)). À des fins d’identifiabilité, on suppose que les aléas ne sont pas corrélées aux facteurs \\(F\\) et entre elles et que les facteurs \\(F_1, \\ldots, F_m\\) ont une moyenne nulle et une variance unitaire, donc \\(\\mathsf{E}(F_i)=0\\) et \\(\\mathsf{Va}(F_i)=1\\) (\\(i=1, \\ldots, p\\)).\nLe modèle d’analyse factorielle s’écrit \\[\\begin{align*}\n\\boldsymbol{X} &= \\underset{\\text{moyenne}}{\\boldsymbol{\\mu}} + \\underset{\\text{combinaison linéaire de facteurs latents}}{\\boldsymbol{\\Gamma}\\boldsymbol{F}} + \\underset{\\text{aléa}}{\\boldsymbol{\\varepsilon}},\n\\end{align*}\\] ou si on écrit le système ligne par ligne, \\[\\begin{align*}\nX_1 &= \\mu_1 + \\gamma_{11}F_1 + \\gamma_{12} F_2 + \\cdots + \\gamma_{1m}F_m + \\varepsilon_1\\\\\nX_2 &= \\mu_2 + \\gamma_{21}F_1 + \\gamma_{22} F_2 + \\cdots + \\gamma_{2m}F_m + \\varepsilon_2\\\\\n&\\vdots \\\\\nX_p &= \\mu_p + \\gamma_{p1}F_1 + \\gamma_{p2} F_2 + \\cdots + \\gamma_{pm}F_m + \\varepsilon_p,\n\\end{align*}\\] où \\(\\mu_i\\) est l’espérance (moyenne théorique) de la variable aléatoire \\(X_i\\), \\(\\boldsymbol{\\Gamma}\\) est une matrice \\(p \\times m\\) avec éléments \\(\\gamma_{ij}\\), qui représentent le chargement (poids) de la variable \\(X_i\\) sur le facteur \\(F_j\\) (\\(i=1, \\ldots, p\\); \\(j=1, \\ldots, m\\)).\nLes espérances (\\(\\mu_i\\)), les chargements (\\(\\gamma_{ij}\\)) et les variances (\\(\\psi_i\\)) sont des quantités fixes, mais inconnues, tandis que les facteurs communs (\\(F_i\\)) et les aléas (\\(\\varepsilon_i\\)) sont des variables aléatoires non observables.\nSelon ce modèle, on obtient \\[\\begin{align*}\n\\mathsf{Va}(\\boldsymbol{X}) &= \\boldsymbol{\\Gamma}\\mathsf{Va}(\\boldsymbol{F})\\boldsymbol{\\Gamma}^\\top + \\mathsf{Va}(\\boldsymbol{\\varepsilon})\\\\\n& = \\boldsymbol{\\Gamma}\\boldsymbol{\\Gamma}^\\top + \\mathrm{diag}(\\boldsymbol{\\psi}).\n\\end{align*}\\] Les éléments diagonaux de cette matrice sont \\(\\mathsf{Va}(X_j) = \\sum_{l=1}^k \\gamma_{jl}^2 + \\psi_j\\): on appelle communalité le terme \\(h_j = \\sum_{l=1}^k \\gamma_{jl}^2\\), qui représente la proportion de variance totale de \\(X_j\\) due à la corrélation entre les facteurs. Le terme \\(\\psi_j\\) est dénommé unicité.\nSi les variables ont été préalablement standardisées de telle sorte que \\(\\mathsf{E}(X_i)=0\\) et \\(\\mathsf{Va}(X_i)=1\\) (ce qui revient à utiliser la matrice de corrélation des observations dans l’analyse), alors \\(\\mathsf{Cor}(X_i, F_j)=\\gamma_{ij}\\), c’est-à-dire, le chargement de la variable \\(X_i\\) sur le facteur \\(F_j\\) est le coefficient de corrélation entre les deux.\nSans aucune contrainte sur le modèle, la matrice de covariance de \\(X_1, \\ldots, X_p\\) possède \\(p(p+1)/2\\) paramètres, soit \\(p\\) variances et \\(p(p-1)/2\\) termes de corrélation. Avec le modèle d’analyse factorielle, on suppose que l’on peut décrire cette structure en utilisant seulement \\(p(m+1) - m(m-1)/2\\) paramètres5. Par exemple, avec \\(p=50\\) variables explicatives et \\(m=6\\) facteurs, on essaie de décrire la structure de covariance à l’aide de 350 paramètres au lieu de 1275.\nPour faire une analyse factorielle, la taille d’échantillon devrait être quand même conséquente: le nombre d’entrées dans la base de données est \\(np\\) et ce nombre représente la quantité d’unités (information) disponible pour estimer les covariances. Plusieurs références suggèrent d’avoir une taille d’échantillon entre cinq et 20 fois le fois le nombre de variables, ou bien un nombre minimal de 100 à 1000 observations. Des études de simulations suggèrent que la taille critique dépend des paramètres, communalités, distribution des données, etc. Ces règles du pouce sont donc essentiellement arbitraires.\nIl existe plusieurs méthodes pour extraire les facteurs, c’est-à-dire pour estimer les paramètres du modèle (les \\(\\psi_i\\) et les \\(\\gamma_{ij}\\)). Nous allons discuter de deux d’entre elles: la méthode du maximum de vraisemblance et la méthode des composantes principales. L’avantage de l’estimation par maximum de vraisemblance est qu’elle permet l’utilisation de critères d’information et de statistiques de tests pour guider le choix du nombre de facteurs, en supposant toutefois la normalité des facteurs et des aléas.\n\n3.5.1 Rotation des facteurs\nDans le modèle d’analyse factorielle, on peut montrer que, lorsqu’il y a deux facteurs ou plus, il existe plusieurs configurations de facteurs qui donnent la même structure de covariance. En fait, les chargements peuvent seulement être déterminés à une transformation orthogonale près6. Si les chargements provenant d’une méthode d’extraction des facteurs ne sont pas uniques, la matrice de corrélation estimée par le modèle est par contre unique.\nIl existe plusieurs techniques de rotation de facteurs. Le but de ces techniques est d’essayer de trouver une solution qui fera en sorte que les facteurs seront facilement interprétables. La méthode la plus utilisée est la méthode varimax: elle produit une configuration de chargement en maximisant la variance de la somme des carrés des chargements pour les \\(m\\) facteurs.\nLa méthode varimax tend à produire une configuration de facteurs tel que les chargements de chaque variable sont dispersés (des chargements élevés positifs ou négatifs et d’autres presque nuls). Il est conseillé de toujours tenter d’interpréter la solution avec une rotation varimax. Si ce n’est pas suffisamment clair, il existe d’autres méthodes de rotation dont certaines (les rotations de type oblique) permettent la présence de corrélation entre les facteurs.\n\n3.5.1.1 Estimation par la méthode des composantes principales\nLa façon la plus simple d’estimer les chargements est d’utiliser la méthode des composantes principales en prenant comme estimation \\[\\begin{align*}\n\\widehat{\\boldsymbol{\\Gamma}} = \\boldsymbol{Q}_{1:m} \\mathrm{diag}(\\lambda_1^{1/2}, \\ldots, \\lambda_m^{1/2}),\n\\end{align*}\\] où \\(\\lambda_j\\) est la \\(j\\)e plus grande valeur propre de la matrice de covariance empirique \\(\\mathbf{S}\\) et \\(\\boldsymbol{Q}_{1:m}\\) est la sous-matrice formée par les \\(m\\) premières colonnes de vecteurs propres de \\(\\boldsymbol{Q}\\). On peut estimer les variances des aléas à travers \\[\\begin{align*}\n\\widehat{\\boldsymbol{\\psi}} = \\mathrm{diag}\\left(\\mathbf{S} - \\widehat{\\boldsymbol{\\Gamma}}\\widehat{\\boldsymbol{\\Gamma}}^\\top\\right).\n\\end{align*}\\] L’avantage de cette approche est que l’on peut utiliser la même décomposition en valeurs propres et vecteurs propres pour chaque valeur de \\(m\\): seule la rotation dépend de la dimension choisie. La solution est également toujours valide avec la garantie que \\(\\widehat{\\psi}_j>0\\). On peut utiliser la discussion de la Section 3.4.1 pour choisir le nombre de variables.\n\n# Solution (chargements) avec rotation varimax\nfacto_cp <- hecmulti::factocp(factor, nfact = \"kaiser\", cor = TRUE)\n\n\n\n3.5.1.2 Estimation par maximum de vraisemblance\nSi on suppose que les aléas et les facteurs suivent des lois Gaussiennes, alors on peut obtenir une forme explicite pour la fonction de vraisemblance de la matrice de covariance. L’estimation des paramètres requiert une optimisation numérique qui est souvent difficile et qui mène parfois à des solutions paradoxales. On obtient un cas de quasi-Heywood quand \\(h_j=1\\) pour une variable \\(j\\), (on parle de cas de Heywood si \\(h_j > 1\\)). Si on modélise des variables explicatives centrées réduites, \\(\\mathsf{Va}(X_j)=1\\), d’où un problème d’interprétation car le terme \\(\\psi_j\\) serait nul (cas de quasi-Heywood) ou négatif (cas de Heywood) alors même que ce terme représente la variance du \\(j\\)e aléa. Les cas de quasi-Heywood ont plusieurs causes, lesquelles sont listées dans la documentation SAS. Souvent, c’est dû à l’utilisation d’un trop petit ou trop grand nombre de facteurs ou une taille d’échantillon trop petite, etc. Cela complique l’interprétation et nous amène à questionner la validité du modèle d’analyse factorielle comme simplification de la structure de covariance.\nLes chargements estimés pour la solution à quatre facteurs, suite à la rotation varimax, sont obtenus avec le code suivant:\n\n# Ajuster le modèle factoriel par maximum de vraisemblance\nfa4 <- factanal(x = factor, \n                factors = 4L)\n# Imprimer les chargements en omettant les valeurs inférieures à 0.3\nprint(fa4$loadings, \n      cutoff = 0.3)\n\n\n\n\n\nTableau 3.4:  Estimés des chargements (multipliés par 100) pour le modèle à quatres facteurs avec rotation varimax estimé à l’aide de la méthode du maximum de vraisemblance. Les chargements inférieurs à 0.3 sont omis. \n \n  \n      \n    F1 \n    F2 \n    F3 \n    F4 \n  \n \n\n  \n    x1 \n     \n     \n     \n    99 \n  \n  \n    x2 \n     \n     \n    67 \n     \n  \n  \n    x3 \n     \n    75 \n     \n     \n  \n  \n    x4 \n    71 \n     \n     \n     \n  \n  \n    x5 \n     \n     \n     \n    37 \n  \n  \n    x6 \n     \n    51 \n     \n     \n  \n  \n    x7 \n     \n     \n    75 \n     \n  \n  \n    x8 \n    79 \n     \n     \n     \n  \n  \n    x9 \n     \n    63 \n     \n     \n  \n  \n    x10 \n     \n     \n    66 \n     \n  \n  \n    x11 \n    71 \n     \n     \n     \n  \n  \n    x12 \n     \n    61 \n     \n     \n  \n\n\n\n\n\n\nOn constate à la lecture du Tableau 3.4 des chargements que le chargement associé à la première variable est de 0.992 pour le facteur 4: cela correspondrait à un facteur avec une corrélation de presque un, donc \\(F_4 \\approx X_1\\). Le modèle obtenu avec la méthode du maximum de vraisemblance n’est donc pas adéquat puisque l’optimisation a convergée vers un cas de quasi-Heywood et que le facteur n’est pas une variable latente, mais une des variables de la base de données de départ. Pour diagnostiquer le tout, on peut aussi analyser les valeurs d’unicité: la minimum de min(fa4$uniqueness) est 0.005, ce qui correspond à la tolérance de l’algorithme (valeur minimale permise dans l’optimisation), voir ?factanal. On retourne à la planche à dessin en réduisant le nombre de variables.\nEn général, on associe une variable à un groupe (facteur) si son chargement est supérieur à 0.3 (en valeur absolue), ce qui donne\n\nFacteur 1: \\(X_4\\), \\(X_8\\) et \\(X_{11}\\)\nFacteur 2: \\(X_3\\), \\(X_6\\), \\(X_9\\) et \\(X_{12}\\)\nFacteur 3: \\(X_2\\), \\(X_7\\) et \\(X_{10}\\)\nFacteur 4: \\(X_1\\) et \\(X_5\\).\n\nCe point de coupure est arbitraire et peut être augmenté si on note qu’il y a trop de variables disparates. Le signe des chargements est arbitraires.\nCes facteurs sont interprétables:\n\nLe facteur 1 représente l’importance accordée au service.\nLe facteur 2 représente l’importance accordée aux produits.\nLe facteur 3 représente l’importance accordée à la facilité de paiement.\nLe facteur 4 représente l’importance accordée aux prix.\n\nDans cet exemple, les choses se sont bien passées et le nombre de facteurs que nous avons spécifié semble être adéquat (hormis le cas de quasi-Heywood), mais ce n’est pas toujours aussi évident. Il est utile d’avoir des outils pour guider le choix du nombre de facteurs.\n\n\n\n3.5.2 Choix du nombre de facteurs\nIl existe différentes méthodes pour se guider dans le nombre de facteurs, \\(m\\), à utiliser. Cependant, le point important à retenir est que, peu importe le nombre choisi, il faut que les facteurs soient interprétables. Par conséquent, les méthodes qui suivent ne devraient servir que de guide et non pas être suivies aveuglément. La méthode du maximum de vraisemblance que nous avons utilisée dans l’exemple possède l’avantage de fournir trois critères pour choisir le nombre de facteurs appropriés. Ces critères sont:\n\nle critère d’information d’Akaike (AIC)\nle critère d’information bayésien de Schwarz (BIC)\nle test du rapport de vraisemblance pour l’hypothèse nulle que le modèle de corrélation décrit le modèle factoriel avec \\(m\\) facteurs est adéquat, contre l’alternative qu’il n’est pas adéquat.\n\nLes critères d’information servent à la sélection de modèles; ils seront traités plus en détail dans les chapitres qui suivent. Pour l’instant, il est suffisant de savoir que le modèle avec la valeur du critère AIC (ou BIC) la plus petite est considéré le « meilleur » (selon ce critère).\nLe paquet hecmulti contient des méthodes pour extraire la log-vraisemblance, les critères d’information pour un modèle d’analyse factorielle (objet de classe factanal). On peut extraire la valeur-\\(p\\) pour le test du rapport de vraisemblance comparant le modèle à 12 variables (corrélation empirique) avec le modèle simplifié obtenu en utilisant quatre facteurs: une valeur-\\(p\\) supérieur à un seuil prédéfini (typiquement \\(\\alpha=0.05\\)) indique que la simplification est adéquate puisqu’on ne rejette pas l’hypothèse nulle. La sortie suivante dans le Tableau 3.5 présente les diagnostics du modèle en fonction du nombre de facteurs pour les modèles ajustés selon la méthode du maximum de vraisemblance.\n\nlibrary(hecmulti)\najustement_factanal(\n    covmat = cov(factor),\n    factors = 1:5,\n    n.obs = nrow(factor))\n\n\n\n\n\nTableau 3.5:  Ajustement de modèles d’analyse factorielle par la méthode du maximum de vraisemblance pour différent nombres de facteurs: critères d’informations AIC et BIC, valeur-p du test de rapport de vraisemblance, nombre de paramètres estimés et indicateur pour les cas de (quasi)-Heywood \n \n  \n      \n    k \n    AIC \n    BIC \n    valeur-p \n    npar \n    heywood \n  \n \n\n  \n    1 \n    1 \n    2267.14 \n    2346.30 \n    < 2e-16 \n    24 \n    0 \n  \n  \n    2 \n    2 \n    2137.87 \n    2253.31 \n    < 2e-16 \n    35 \n    0 \n  \n  \n    3 \n    3 \n    2017.19 \n    2165.61 \n    0.09604 \n    45 \n    0 \n  \n  \n    4 \n    4 \n    2002.56 \n    2180.67 \n    0.97262 \n    54 \n    1 \n  \n  \n    5 \n    5 \n    2012.70 \n    2217.19 \n    0.97445 \n    62 \n    1 \n  \n\n\n\n\n\n\nLe nombre de facteurs à utiliser selon le AIC est 4, versus 3 selon le BIC. Le nombre mimimal de critères selon le test du rapport de vraisemblance est NA. Ainsi, on retient la solution à trois facteurs dans tous les cas: cette adéquation entre les critères est l’exception plutôt que la règle.\nIl faut garder en tête que l’estimation par maximum de vraisemblance du modèle d’analyse factorielle est très sensible à l’initialisation: on peut aussi parfois obtenir des valeurs différentes selon les logiciels. Cette fragilité, couplée à la haute fréquence de cas de Heywood, fait en sorte que je préfère utiliser la méthode des composantes principales pour l’estimation.\nOn peut considérer le modèle avec trois facteurs: les chargements (après rotation varimax) sont données dans le Tableau 3.6\n\n\n\n\nTableau 3.6:  Estimés des chargements (multipliés par 100) pour le modèle à trois facteurs avec rotation varimax estimé à l’aide de la méthode du maximum de vraisemblance. Les chargements inférieurs à 0.3 sont omis. \n \n  \n      \n    F1 \n    F2 \n    F3 \n  \n \n\n  \n    x1 \n     \n     \n     \n  \n  \n    x2 \n     \n     \n    67 \n  \n  \n    x3 \n     \n    76 \n     \n  \n  \n    x4 \n    71 \n     \n     \n  \n  \n    x5 \n     \n     \n     \n  \n  \n    x6 \n     \n    50 \n     \n  \n  \n    x7 \n     \n     \n    75 \n  \n  \n    x8 \n    79 \n     \n     \n  \n  \n    x9 \n     \n    63 \n     \n  \n  \n    x10 \n     \n     \n    67 \n  \n  \n    x11 \n    72 \n     \n     \n  \n  \n    x12 \n     \n    60 \n     \n  \n\n\n\n\n\n\nCette solution récupère les trois facteurs service, produits et paiement de la solution précédente à quatre facteurs. Le facteur prix (qui était formé de \\(X_1\\) et \\(X_5\\)) n’est plus présent.\nOn suggère d’utiliser les trois critères découlant de l’utilisation de la vraisemblance et de déterminer le nombre de facteurs à extraire selon différents critères avant d’examiner les modèles avec ce nombre de facteurs et ceux avec un facteur de moins ou de plus. Au final, le plus important est de pouvoir interpréter raisonnablement les facteurs: la configuration de facteurs choisie est logique et compréhensible.\n\n\n3.5.3 Construction d’échelles à partir des facteurs\nSi le seul but de l’analyse factorielle est de comprendre la structure de corrélation entre les variables, alors se limiter à l’interprétation des facteurs est suffisant.\nSi par contre, le but est de réduire le nombre de variables pour pouvoir par la suite procéder à d’autres analyses statistiques, l’analyse factorielle peut alors servir de guide pour construire de nouvelles variables (échelles). En supposant que l’analyse factorielle a produit des facteurs qui sont interprétables et satisfaisants, la méthode de construction d’échelles la plus couramment utilisée consiste à construire \\(m\\) nouvelles variables, une par facteur. Pour un facteur donné, la nouvelle variable est simplement la moyenne des variables ayant des chargements élevés sur ce facteur. Une autre méthode, les scores factoriels, sera présentée plus loin. Il est important que les corrélations soient de même signe si on veut regrouper les variables dans les échelles pour que ce regroupement soit logique: certaines questions avec des échelles de Likert ont parfois un encodage inverse comme test d’attention.\nEst-il logique de calculer des échelles avec autre chose que des items de questionnaire ramenés sur une plage commune? Par forcément… Il faut aussi s’assurer que les variables ont la même plage ou étendue avant des les combiner, sinon certaines variables seront des poids plumes et seule la variable avec la plus grande étendue ressortira.\nLorsqu’on construit une échelle, il est important d’examiner sa cohérence interne. Ceci peut être fait à l’aide du coefficient alpha de Cronbach. Ce coefficient mesure à quel point chaque variable faisant partie d’une échelle est corrélée avec le total de toutes les variables pour cette échelle. Plus le coefficient est élevé, plus les variables ont tendance à être corrélées entre elles. L’alpha de Cronbach est \\[\\begin{align*}\n\\alpha=\\frac{k}{k-1} \\frac{S^2-\\sum_{i=1}^k S_i^2}{S^2},\n\\end{align*}\\] où \\(k\\) est le nombre de variables dans l’échelle, \\(S^2\\) est la variance empirique de la somme des variables et \\(S_i^2\\) est la variance empirique de la \\(i\\)e variable. En pratique, on voudra que ce coefficient soit au moins égal à 0.6 pour être satisfait de la cohérence interne de l’échelle.7\nLe paquet hecmulti contient une fonction, alphaC, pour faire l’estimation du \\(\\alpha\\) de Cronbach\n\n# Création des échelles\nech_service <- rowMeans(factor[,c(\"x4\",\"x8\",\"x11\")])\nech_produit <- rowMeans(factor[,c(\"x3\",\"x6\",\"x9\",\"x12\")])\nech_paiement <- rowMeans(factor[,c(\"x2\",\"x7\",\"x10\")])\nech_prix <- rowMeans(factor[,c(\"x1\",\"x5\")])\n\n# Cohérence interne (alpha de Cronbach)\nalphaC(factor[,c(\"x4\",\"x8\",\"x11\")])\nalphaC(factor[,c(\"x3\",\"x6\",\"x9\",\"x12\")])\nalphaC(factor[,c(\"x2\",\"x7\",\"x10\")])\nalphaC(factor[,c(\"x1\",\"x5\")])\n\n\n\n\n\nTableau 3.7:  Coefficient alpha de Cronbach pour les quatre échelles formées. \n \n  \n    service \n    produit \n    paiement \n    prix \n  \n \n\n  \n    0.781 \n    0.718 \n    0.727 \n    0.546 \n  \n\n\n\n\n\n\nAinsi, les \\(\\alpha\\) de Cronbach sont tous satisfaisants (plus grand que 0.6) sauf pour le facteur prix (0.546). Tout est donc cohérent. Les échelles provenant des facteurs service, produits et paiement, sont satisfaisantes. Ces facteurs sont identifiés à la fois dans la solution à quatre, mais aussi dans la solution à trois facteurs. Le facteur prix est celui qui apparaît en plus dans la solution à quatre facteurs. Il a une interprétation claire (c’est essentiellement x1), mais son faible \\(\\alpha\\) ferait en sorte qu’il serait discutable de travailler avec l’échelle prix dans d’autres analyses (du moins avec selon l’usage habituel du \\(\\alpha\\)) plutôt que d’utiliser directement la variable x1."
  },
  {
    "objectID": "02-analysefactorielle.html#compléments-dinformation",
    "href": "02-analysefactorielle.html#compléments-dinformation",
    "title": "3  Réduction de la dimension",
    "section": "3.6 Compléments d’information",
    "text": "3.6 Compléments d’information\n\n3.6.1 Variables ordinales\nThéoriquement, une analyse factorielle ne devrait être faite qu’avec des variables continues. Par contre, en pratique, on l’utilise souvent aussi avec des variables ordinales (comme pour l’exemple portant sur le questionnaire) et même avec des variables binaires (0-1).\nDans ce genre de situation, on peut aussi utiliser d’autres mesures d’associations au lieu du coefficient de corrélation linéaire de Pearson. Par exemple, on peut utiliser la corrélation polychorique, qui est une mesure de corrélation entre deux variables ordinales. La corrélation tétrachorique correspond au cas spécial de deux variables binaires.\nMa suggestion est d’utiliser la corrélation linéaire ordinaire avec des variables ordinales (même binaires). Si les résultats ne sont pas satisfaisants, on peut alors essayer avec d’autres mesures d’associations.\n\n\n3.6.2 Autres méthodes de rotation des facteurs\nJusqu’à présent, nous avons utilisé la méthode de rotation orthogonale varimax. Il existe de nombreuses autres méthodes de rotations orthogonales fournies dans le paquet psych. Rappelez-vous que le modèle d’analyse factorielle de base suppose que les facteurs sont non corrélés. Les rotations de type obliques permettent d’introduire de la corrélation entre les facteurs: quelquefois, une telle rotation facilitera davantage l’interprétation des facteurs qu’une rotation orthogonale. Notez qu’il faut être prudent lorsqu’on utilise une méthode de rotation oblique car il y aura trois matrices de chargements après rotation (coefficients de régression normalisés, corrélations semi-partielles ou corrélations). On suggère l’utilisation de la première, soit la représentation avec coefficients de régression normalisés. Il s’agit des coefficients de régression si on voulait prédire les variables à l’aide des facteurs. Ils indiquent donc à quel point chaque facteur est associé à chaque variable. Dans le cas d’une rotation orthogonale, ces trois matrices sont les mêmes et il s’agit de trois interprétations valides des chargements.\n\n\n3.6.3 Scores factoriels\nAvec les données de l’exemple, en nous basant sur les résultats de l’analyse factorielle, nous avons créé quatre nouvelles échelles (une par facteur) que l’on peut calculer pour chaque individu:\n\nservice = \\((X_4+X_8+X_{11})/3\\),\nproduit = \\((X_3+X_6+X_9+X_{12})/4\\),\npaiement = \\((X_2+X_7+X_{10})/3\\),\nprix = \\((X_1+X_5)/2\\).\n\nPar exemple, la variable prix peut donc être vu comme une combinaison linéaire des 12 variables où seulement \\(X_1\\) et \\(X_5\\) reçoivent un poids (égal) différent de zéro. Une autre façon de créer de nouvelles variables consiste à calculer des scores factoriels (un pour chaque facteur) pour chaque individu à partir de la matrice de données centrées et réduite \\(\\mathbf{Z}\\) (de telle sorte que la moyenne de chaque colonne soit 0 et la variance 1). Les score factoriel \\[\\begin{align*}\n\\widehat{F}_{i, k} &= \\left[\\mathbf{Z}\\mathbf{R}^{-1}\\widehat{\\boldsymbol{\\Gamma}}\\right]_{ik}\\\\&=\n\\widehat{\\beta}_{1, k} z_{i, 1} + \\cdots + \\widehat{\\beta}_{12, k}z_{i, 12}\n\\end{align*}\\] où \\(\\widehat{\\boldsymbol{\\Gamma}}\\) est la matrice \\(p \\times m\\) des chargements, \\(\\mathbf{R}\\) la matrice \\(p \\times p\\) des corrélation empirique et où \\(z_{i, 1}, \\ldots, z_{i, 12}\\) est la \\(i\\)e ligne (observation) de \\(\\mathbf{Z}\\). La matrice \\(p \\times m\\) de coefficients \\(\\boldsymbol{\\beta} = \\mathbf{R}^{-1}\\widehat{\\boldsymbol{\\Gamma}}\\).\nAinsi, chacune des 12 variables originales contribue au calcul du score factoriel. Les variables ayant des chargements plus élevés sur ce facteur auront tendance à avoir des poids (\\(\\widehat{\\gamma}\\)) plus élevés. Par contre, les scores factoriels ne sont pas uniques car ils dépendent des chargements utilisés (et donc à la fois de la méthode d’estimation et de la méthode de rotation). On peut également utiliser les scores factoriels au lieu des 12 variables originales dans des analyses subséquentes. Il est suggéré d’utiliser les nouvelles variables (échelles) obtenues en faisant les moyennes des variables identifiées comme faisant partie de chaque facteur pour les raisons suivantes:\n\nl’interprétation des scores factoriels est moins claire (chaque facteur dépend de toutes les variables)\nles scores factoriels ne sont pas uniques (ils dépendent de la méthode d’estimation et de rotation).\nles coefficients servant au calcul seront différents d’une étude à l’autre.\n\n\n\n\n\nTableau 3.8:  Coefficients du score (modèle de régression, maximum de vraisemblance) pour le modèle d’analyse factorielle à quatre facteurs. \n \n  \n      \n    facteur 1 \n    facteur 2 \n    facteur 3 \n    facteur 4 \n  \n \n\n  \n    x1 \n    0.03 \n    0.05 \n    0.04 \n    1.01 \n  \n  \n    x2 \n    -0.05 \n    0.01 \n    0.31 \n    0.01 \n  \n  \n    x3 \n    0.01 \n    0.45 \n    -0.02 \n    0.01 \n  \n  \n    x4 \n    0.30 \n    0.01 \n    0.01 \n    0.03 \n  \n  \n    x5 \n    0.00 \n    -0.01 \n    -0.01 \n    0.00 \n  \n  \n    x6 \n    -0.01 \n    0.17 \n    0.02 \n    0.00 \n  \n  \n    x7 \n    -0.01 \n    -0.02 \n    0.44 \n    0.02 \n  \n  \n    x8 \n    0.45 \n    -0.05 \n    0.04 \n    0.04 \n  \n  \n    x9 \n    -0.03 \n    0.27 \n    -0.02 \n    0.00 \n  \n  \n    x10 \n    0.02 \n    0.01 \n    0.30 \n    0.02 \n  \n  \n    x11 \n    0.30 \n    0.00 \n    -0.07 \n    0.02 \n  \n  \n    x12 \n    0.00 \n    0.24 \n    0.00 \n    0.01 \n  \n\n\n\n\n\n\nLes scores factoriels sont obtenus en spécifiant scores = \"regression\" dans les options de la fonction factanal. Les poids avec le modèle à quatre facteurs sont rapportés dans le Tableau 3.8. On remarque que\n\npour le premier facteur, trois variables ont des poids importants (\\(X_4\\), \\(X_8\\) et \\(X_{11}\\)). Il s’agit donc d’un facteur très proche du facteur service.\npour le deuxième facteur, les variables \\(X_3\\), \\(X_6\\), \\(X_9\\) et \\(X_{12}\\) ont des poids importants. Il s’agit donc d’un facteur très proche du facteur produits.\npour le troisième facteur, les variables \\(X_2\\), \\(X_7\\), \\(X_{10}\\) ont des poids importants. Il s’agit donc d’un facteur très proche du facteur paiement.\npour le quatrième facteur, seule la variable \\(X_1\\) a un poids important. On aurait pu s’attendre à ce que ce soit également le cas pour \\(X_5\\), en lien avec le facteur prix — ce facteur était moins clair selon le alpha de Cronbach.\n\nLes corrélations entre les échelles (construites avec les moyennes) et les scores factoriels sont données dans le Tableau 3.9. On remarque la forte corrélation entre le score factoriel et les échelles construites avec les moyennes pour les facteurs service, produits et paiement. Cela veut dire qu’utiliser les échelles ou les scores factoriels ne devrait pas faire de différence dans des analyses subséquentes. Par contre, cette corrélation est plus faible (0.82) pour le facteur prix.\n\n\n\n\nTableau 3.9:  Corrélation entre quatre premiers scores (modèle d’analyse factorielle à quatre facteurs) et échelles. \n \n  \n      \n    score 1 \n    score 2 \n    score 3 \n    score 4 \n  \n \n\n  \n    échelle 1 \n    0.99 \n    0.05 \n    0.03 \n    -0.05 \n  \n  \n    échelle 2 \n    0.05 \n    0.98 \n    0.03 \n    -0.04 \n  \n  \n    échelle 3 \n    0.03 \n    0.05 \n    0.99 \n    -0.07 \n  \n  \n    échelle 4 \n    -0.07 \n    -0.04 \n    -0.08 \n    0.82 \n  \n\n\n\n\n\n\n\n\n\n\n\n\nÉtapes de l’analyse factorielle exploratoire\n\n\n\n\nDéterminer les variables à utiliser dans l’analyse\nVérifier les prérequis\nSélectionner une méthode d’estimation et extraire les facteurs\nDéterminer le nombre de facteurs\nEffectuer une rotation des facteurs\nInterpréter les facteurs\nÉvaluer la validité du modèle\n\n\n\n\n\n\n\n\n\nEn résumé\n\n\n\n\nL’analyse factorielle exploratoire fournit un modèle pour la matrice de corrélation\n\nLa solution du problème n’est pas unique: on choisit celle qui permet de mieux séparer les variables.\nSeules les variables numériques pour lesquelles on suspecte une dimension commune sont incluses dans l’analyse.\nOn doit avoir beaucoup d’observations (au moins 100, 10 fois plus que de variables) pour estimer le modèle.\nOn estime le modèle à l’aide de la méthode des composantes principales (modèle toujours valide et moins coûteux en calcul, mais critères pour la sélection du nombre de facteurs arbitraires) ou du maximum de vraisemblance (optimisation numérique avec solutions fréquemment problématique, critères d’information pour choix du nombre de facteurs).\nLe nombre de facteurs retenu doit donner des regroupements logiques (facteur wow).\nOn utilise toujours une rotation orthogonale pour faciliter l’interprétation (varimax par défaut).\nL’interprétation se fait à partir des chargements (corrélation entre variables et facteurs).\nOn crée des échelles en prenant la moyenne des variables qui ont un chargement élevés en lien avec un facteur donné (de même signe).\nLes échelles sont cohérentes si le \\(\\alpha\\) de Cronbach est supérieur à 0.6, faute de quoi elles sont rejetées."
  },
  {
    "objectID": "04-selectionmodeles.html#introduction",
    "href": "04-selectionmodeles.html#introduction",
    "title": "4  Sélection de variables et de modèles",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nCe chapitre présente des principes, outils et méthodes très généraux pour choisir un « bon » modèle. Nous allons principalement utiliser la régression linéaire pour illustrer les méthodes en supposant que tout le monde connaît ce modèle de base. Les méthodes présentées sont en revanche très générales et peuvent être appliquées avec n’importe quel autre modèle (régression logistique, arbres de classification et régression, réseaux de neurones, analyse de survie, etc.)\nL’expression « sélection de variables » fait référence à la situation où l’on cherche à sélectionner un sous-ensemble de variables à inclure dans notre modèle à partir d’un ensemble de variables \\(X_1, \\ldots, X_p\\). Le terme variable ici inclut autant des variables distinctes que des transformations d’une ou plusieurs variables.\nPar exemple, supposons que les variables \\(\\texttt{age}\\), \\(\\texttt{sexe}\\) et \\(\\texttt{revenu}\\) soient trois variables explicatives disponibles. Nous pourrions alors considérer choisir entre ces trois variables. Mais aussi, nous pourrions considérer inclure \\(\\texttt{age}^2\\), \\(\\texttt{age}^3\\), \\(\\log(\\texttt{age})\\), etc. Nous pourrions aussi considérer des termes d’interactions entre les variables, comme \\(\\texttt{age} \\cdot \\texttt{revenu}\\) ou \\(\\texttt{age}\\cdot\\texttt{revenu}\\cdot\\texttt{sexe}\\). Le problème est alors de trouver un bon sous-ensemble de variables parmi toutes celles considérées.\nL’expression « sélection de modèle » est un peu plus générale. D’une part, elle inclut la sélection de variables car, pour une famille de modèles spécifiques (régression linéaire par exemple), choisir un sous-ensemble de variables revient à choisir un modèle. D’autre part, elle fait référence à la situation où l’on cherche à trouver le meilleur modèle parmi des modèles de natures différentes. Par exemple, on pourrait choisir entre une régression linéaire, un arbre de régression, une forêt aléatoire, un réseau de neurones, etc."
  },
  {
    "objectID": "04-selectionmodeles.html#sélection-de-variables-et-de-modèles-selon-les-buts-de-létude",
    "href": "04-selectionmodeles.html#sélection-de-variables-et-de-modèles-selon-les-buts-de-létude",
    "title": "4  Sélection de variables et de modèles",
    "section": "4.2 Sélection de variables et de modèles selon les buts de l’étude",
    "text": "4.2 Sélection de variables et de modèles selon les buts de l’étude\nNous disposons d’une variable réponse \\(Y\\) et d’un ensemble de variables explicatives \\(X_1, \\ldots, X_p\\). L’attitude à adopter dépend des buts de l’étude.\n\n1e situation: On veut développer un modèle pour faire des prédictions sans qu’il soit important de tester formellement les effets des paramètres individuels.\n\nDans ce cas, on désire seulement que notre modèle soit performant pour prédire des valeurs futures de \\(Y\\). On peut alors baser notre choix de variable (et de modèle) en utilisant des outils qui nous guiderons quant aux performances prédictives futures du modèle (voir \\(\\mathsf{AIC}\\), \\(\\mathsf{BIC}\\) et validation croisée plus loin). On pourra enlever ou rajouter des variables et des transformations de variables au besoin afin d’améliorer les performances prédictives. Les méthodes que nous allons voir concernent essentiellement ce contexte.\n\n2e situation: On veut développer un modèle pour estimer les effets de certaines variables sur notre \\(Y\\) et tester des hypothèses de recherche spécifiques concernant certaines variables.\n\nDans ce cas, il est préférable de spécifier le modèle dès le départ selon des considérations scientifiques et de s’en tenir à lui. Faire une sélection de variables dans ce cas est dangereux car on ne peut pas utiliser directement les valeurs-p des tests d’hypothèses (ou les intervalles de confiance sur les paramètres) concernant les paramètres du modèle final car elles ne tiennent pas compte de la variabilité due au processus de sélection de variables.\nUne bonne planification de l’étude est alors cruciale afin de collecter les bonnes variables, de spécifier le ou les bons modèles, et de s’assurer d’avoir suffisamment d’observations pour ajuster le ou les modèles désirés.\nSi procéder à une sélection de variables est quand même nécessaire dans ce contexte, il est quand même possible de le faire en divisant l’échantillon en deux. La sélection de variables pourrait être alors effectuée avec le premier échantillon. Une fois qu’un modèle est retenu, on pourrait alors réajuster ce modèle avec le deuxième échantillon (sans faire de sélection de variables cette fois-ci). L’inférence sur les paramètres (valeurs-p, etc.) sera alors valide. Le désavantage ici qu’il faut avoir une très grande taille d’échantillon au départ afin d’être en mesure de le diviser en deux."
  },
  {
    "objectID": "04-selectionmodeles.html#estimation-de-la-performance",
    "href": "04-selectionmodeles.html#estimation-de-la-performance",
    "title": "4  Sélection de variables et de modèles",
    "section": "4.3 Estimation de la performance",
    "text": "4.3 Estimation de la performance\nIl est préférable d’avoir un modèle un peu trop complexe qu’un modèle trop simple. Plaçons-nous dans le contexte de la régression linéaire et supposons que le vrai modèle est inclus dans le modèle qui a été ajusté. Il y a donc des variables en trop dans le modèle qui a été ajusté: ce dernier est dit surspécifié.\nPar exemple, supposons que le vrai modèle est \\(Y=\\beta_0+\\beta_1X_1+\\varepsilon\\) mais que c’est le modèle \\(Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\varepsilon\\) qui a été ajusté. Dans ce cas, règle générale, les estimateurs des paramètres et les prédictions provenant du modèle sont sans biais. Mais leurs variances estimées seront un peu plus élevées car on estime des paramètres pour des variables superflues.\nPour illustrer ce point, j’ai simulé des données avec deux variables explicatives corrélées et \\(\\beta_0 = 20\\), \\(\\beta_1=5\\) et \\(\\beta_2 = 0\\).\n\n\n\n\n\nTableau 4.1: Surspécification de modèle de régression linéaire pour des données simulées.\n\n\n\n\n\n(a) modèle correct\n \n  \n      \n    coefficient \n    borne inf. \n    borne sup. \n  \n \n\n  \n    (cst) \n    19.95 \n    19.28 \n    20.62 \n  \n  \n    X1 \n    2.74 \n    2.55 \n    2.94 \n  \n\n\n\n\n\n\n\n\n\n(b) modèle surspécifié\n \n  \n      \n    coefficient \n    borne inf. \n    borne sup. \n  \n \n\n  \n    (cst) \n    20.16 \n    19.88 \n    20.44 \n  \n  \n    X1 \n    1.93 \n    1.84 \n    2.03 \n  \n  \n    X2 \n    5.06 \n    4.74 \n    5.38 \n  \n\n\n\n\n\n\nUne fois qu’on a obtenu l’estimation des coefficients et les intervalles de confiance, on peut les comparer aux vraies valeurs (soit \\(\\beta_0 = 20\\), \\(\\beta_1=5\\) et \\(\\beta_2 = 0\\)) et vérifier si ces dernières se trouvent dans l’intervalle de confiance. Le Tableau 4.1 indique l’effet pour l’inférence de ces spécifications (avec des différences d’estimation, mais non de prédictions, qui sont dues à la colinéarité entre variables).\nSupposons à l’inverse qu’il manque des variables dans le modèle ajusté et que le modèle ajusté est sous-spécifié. Par exemple, supposons que le vrai modèle est \\(Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\varepsilon\\), mais que c’est le modèle \\(Y=\\beta_0+\\beta_1X_1+\\varepsilon\\) qui est ajusté. Dans ce cas, généralement, les estimateurs des paramètres et les prédictions sont biaisés. Le Tableau 4.1 montre les estimations du modèle: les vraies valeurs sont \\(\\beta_0=20\\), \\(\\beta_1 = 2\\) et \\(\\beta_2 = 5\\).\n\n\nTableau 4.2: Sous-spécification de modèle de régression linéaire pour des données simulées.\n\n\n\n\n\n(a) modèle sous-spécifié\n \n  \n      \n    coefficient \n    borne inf. \n    borne sup. \n  \n \n\n  \n    (cst) \n    20.03 \n    19.74 \n    20.31 \n  \n  \n    X1 \n    2.00 \n    1.91 \n    2.08 \n  \n\n\n\n\n\n\n\n\n\n(b) modèle correct\n \n  \n      \n    coefficient \n    borne inf. \n    borne sup. \n  \n \n\n  \n    (cst) \n    20.05 \n    19.77 \n    20.33 \n  \n  \n    X1 \n    1.92 \n    1.83 \n    2.02 \n  \n  \n    X2 \n    0.47 \n    0.14 \n    0.79 \n  \n\n\n\n\n\n\nAinsi, il est généralement préférable d’avoir un modèle légèrement surspécifié qu’un modèle sous-spécifié. Plus généralement, il est préférable d’avoir un peu trop de variables dans le modèle que de prendre le risque d’omettre une ou plusieurs variables importantes. Il faut faire attention et ne pas tomber dans l’excès et avoir un modèle trop complexe (avec trop de variables inutiles) car il pourrait souffrir de surajustement (over-fitting). Les exemples qui suivent illustreront ce fait.\n\n4.3.1 Surajustement\nCette section traite de l’optimisme de l’évaluation d’un modèle (trop beau pour être vrai) lorsqu’on utilise les mêmes données qui ont servies à l’ajuster pour évaluer sa performance. Un principe fondamental lorsque vient le temps d’évaluer la performance prédictive d’un modèle est le suivant : si on utilise les mêmes observations pour évaluer la performance d’un modèle que celles qui ont servi à l’ajuster (à estimer le modèle et ses paramètres), on va surestimer sa performance. Autrement dit, notre estimation de l’erreur que fera le modèle pour prédire des observations futures sera biaisée à la baisse. Ainsi, il aura l’air meilleur que ce qu’il est en réalité. C’est comme si on demandait à un cinéaste d’évaluer son dernier film. Comme c’est son film, il n’aura généralement pas un regard objectif. C’est pourquoi on aura tendance à se fier à l’opinion d’un critique.\nOn cherchera donc à utiliser des outils et méthodes qui nous donneront l’heure juste (une évaluation objective) quant à la performance prédictive d’un modèle.\n\n\n4.3.2 Principes généraux\nLes idées présentées ici seront illustrées à l’aide de la régression linéaire. Par contre, elles sont valides dans à peu près n’importe quel contexte de modélisation.\nPlaçons-nous d’abord dans un contexte plus général que celui de la régression linéaire. Supposons que l’on dispose de \\(n\\) observations indépendantes sur (\\(Y, X_1, \\ldots, X_p\\)) et que l’on a ajusté un modèle \\(\\widehat{f}(X_1, \\ldots, X_p)\\), avec ces données, pour prédire une variable continue \\(Y\\).\nCe modèle peut être un modèle de régression linéaire, \\[\\begin{align*}\n\\widehat{f}(X_1, \\ldots, X_p) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X_1 + \\cdots + \\widehat{\\beta}_pX_p\n\\end{align*}\\] mais il pourrait aussi avoir été construit selon d’autres méthodes (réseau de neurones, arbre de régression, forêt aléatoire, etc.) Une manière de quantifier la performance prédictive du modèle est l’erreur quadratique moyenne (mean squared error), \\[\\begin{align*}\n\\mathsf{EQM}=\\mathsf{E}\\left[\\left\\{(Y-\\widehat{f}(X_1, \\ldots, X_p)\\right\\}^2\\right]\n\\end{align*}\\] lorsque (\\(Y, X_1, \\ldots, X_p\\)) est choisi au hasard dans la population. Cette quantité mesure l’erreur théorique (la différence au carré entre la vraie valeur de \\(Y\\) et la valeur prédite par le modèle) que fait le modèle en moyenne pour l’ensemble de la population. Plus cette quantité est petite, meilleur est le modèle. Le problème est que l’on ne peut pas la calculer car on n’a pas accès à toute la population. Tout au plus peut-on essayer de l’estimer ou bien d’estimer une fonction qui, sans l’estimer directement, classifiera les modèles dans le même ordre qu’elle.\nUne première idée est d’estimer l’erreur quadratique moyenne de l’échantillon d’apprentissage (training mean squared error), \\[\\begin{align*}\n\\widehat{\\mathsf{EQM}}_a= \\frac{1}{n}\\sum_{i=1}^n \\left\\{Y_i-\\widehat{f}(X_{i1}, \\ldots, X_{ip})\\right\\}^2.\n\\end{align*}\\]\nMalheureusement, selon le principe fondamental de la section précédente, cette quantité n’est pas un bon estimateur de l’\\(\\mathsf{EQM}\\). En effet, comme on utilise les mêmes observations que celles qui ont estimé le modèle, l’\\(\\widehat{\\mathsf{EQM}}_a\\) aura tendance à toujours diminuer lorsqu’on augmente la complexité du modèle (par exemple, lorsqu’on augmente le nombre de paramètres). L’\\(\\widehat{\\mathsf{EQM}}_a\\) tend à surestimer la qualité du modèle en sous-estimant l’\\(\\mathsf{EQM}\\) et le modèle a l’air meilleur qu’il ne l’est en réalité.\n\n\n4.3.3 Présentation de l’exemple\nCet exemple simple sur le choix d’un modèle polynomial en régression linéaire servira à illustrer le fait qu’on ne peut utiliser directement les mêmes données qui ont servi à ajuster un modèle pour évaluer sa performance.\nNous disposons de 100 observations sur une variable cible \\(Y\\) et d’une seule variable explicative \\(X\\) dans la base de données selection1_train. Nous voulons considérer des modèles polynomiaux (en \\(X\\)) afin d’en trouver un bon pour prédire \\(Y\\). Un modèle polynomial est un modèle de la forme \\(Y=\\beta_0 + \\beta_1X+\\cdots+\\beta_kX^k+\\varepsilon\\). Le cas \\(k=1\\) correspond à un modèle linéaire simple, \\(k=2\\) à un modèle cubique, \\(k=3\\) à un modèle cubique, etc. Notre but est de déterminer l’ordre (\\(k\\)) du polynôme qui nous donnera un bon modèle. Voici d’abord le graphe de ces 100 observations de l’échantillon d’apprentissage et les valeurs ajustées de polynômes d’ordre 1, 4 et 10.\n\n\nWarning: Removed 28 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 4.1: Nuage de points de 100 observations simulées d’un modèle polynomial de degré inconnu (gauche) et ajustement de différents polynômes de degré variable (droite).\n\n\n\n\nCes données ont été obtenues par simulation et le vrai modèle sous-jacent (celui qui a généré les données) est le modèle cubique, c’est-à-dire le modèle d’ordre \\(k=3\\).\nJ’ai ajusté tour à tour à tour les modèles polynomiaux jusqu’à l’ordre 10, avec l’échantillon d’apprentissage de taille 100. C’est-à-dire, le modèle linéaire avec un polynôme d’ordre \\(k=1\\) (linéaire), \\(k=2\\) (quadratique), etc., jusqu’à \\(k=10\\). J’ai ensuite obtenu la valeur de l’erreur quadratique moyenne d’apprentissage pour chacun de ces modèles. En pratique, on ne pourrait pas calculer l’erreur quadratique moyenne de généralization puisqu’on ne connaît pas le vrai modèle. J’ai fait une approximation de cette dernière en simulant 100 000 observations du vrai modèle (selection1_test), en obtenant la prédiction pour chacune de ces 100 000 observations en utilisant le modèle d’ordre \\(k\\) ajusté sur les données d’apprentissage et en calculant l’erreur quadratique moyenne par la suite.\n\n\n\n\n\n\n\n\nFigure 4.2: erreur quadratique moyenne d’apprentissage (\\(\\widehat{\\mathsf{EQM}}_a\\)) et erreur quadratique moyenne théorique (\\(\\mathsf{EQM}\\)) en fonction de l’ordre (\\(k\\)) du polynôme ajusté.\n\n\n\n\nOn voit clairement dans la Figure 4.2 que l’\\(\\widehat{\\mathsf{EQM}}_a\\) diminue en fonction de l’ordre sur l’échantillon d’apprentissage: plus le modèle est complexe, plus l’erreur observée sur l’échantillon d’apprentissage est petite. La courbe \\(\\mathsf{EQM}\\) donne l’heure juste, car il s’agit d’une estimation de la performance réelle des modèles sur de nouvelles données. On voit que le meilleur modèle est donc le modèle cubique (\\(k=3\\)), ce qui n’est pas surprenant puisqu’il s’agit du modèle que utilisé pour générer les données. On peut aussi remarquer d’autres éléments intéressants. Premièrement, on obtient un bon gain en performance (\\(\\mathsf{EQM}\\)) en passant de l’ordre \\(2\\) à l’ordre \\(3\\). Ensuite, la perte de performance en passant de l’ordre \\(3\\) à \\(4\\), et ensuite à des ordres supérieurs n’est pas si sévère, même si elle est présente. Cela illustre empiriquement qu’il est préférable d’avoir un modèle un peu trop complexe que d’avoir un modèle trop simple. Il serait beaucoup plus grave pour la performance de choisir le modèle avec \\(k=2\\) que celui avec \\(k=4\\).\nEn pratique par contre, on n’a pas accès à la population : les 100 000 observations qui ont servi à estimer l’\\(\\mathsf{EQM}\\) théorique ne seront pas disponible. Si on a seulement l’échantillon d’apprentissage, soit 100 observations dans notre exemple, comment faire alors pour choisir le bon modèle? C’est ce que nous verrons à partir de la section suivante.\nMais avant cela, nous allons discuter un peu plus en détail au sujet de la régression linéaire et d’une mesure très connue, le coefficient de détermination (\\(R^2\\)). Supposons que l’on a ajusté un modèle de régression linéaire \\[\\begin{align*}\n\\widehat{f}(X_1, \\ldots, X_p) = \\widehat{Y}=\\widehat{\\beta}_0 + \\widehat{\\beta}_1X_1+ \\cdots + \\widehat{\\beta}_p X_p.\n\\end{align*}\\] La somme du carré des erreurs (\\(\\mathsf{SCE}\\)) pour notre échantillon est \\[\\begin{align*}\n\\mathsf{SCE}=\\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1X_1 - \\cdots - \\widehat{\\beta}_p X_p)^2 = \\sum_{i=1}^n (Y_i-\\widehat{Y}_i)^2.\n\\end{align*}\\] On peut démontrer que si on ajoute une variable quelconque au modèle, la valeur de la somme du carré des erreurs va nécessairement baisser. Il est facile de se convaincre de cela. En régression linéaire, les estimations sont obtenues par la méthode des moindres carrés qui consiste justement à minimiser la \\(\\mathsf{SCE}\\). Ainsi, en ajoutant une variable \\(X_{p+1}\\) au modèle, la \\(\\mathsf{SCE}\\) ne peut que baisser car, dans le pire des cas, le paramètre de la nouvelle variable sera \\(\\widehat{\\beta}_{p+1}=0\\) et on retombera sur le modèle sans cette variable. C’est pourquoi, la quantité \\(\\widehat{\\mathsf{EQM}}_a=\\mathsf{SCE}/n\\) ne peut être utilisée comme outil de sélection de modèles en régression linéaire.\nNous venons d’ailleurs d’illustrer cela avec notre exemple sur les modèles polynomiaux. En effet, augmenter l’ordre du polynôme de \\(1\\) revient à ajouter une variable. Le coefficient de détermination (\\(R^2\\)) est souvent utilisé, à tort, comme mesure de qualité du modèle. Il peut s’interpréter comme étant la proportion de la variance de \\(Y\\) qui est expliquée par le modèle.\nLe coefficient de détermination est \\[\\begin{align*}\nR^2=\\{\\mathsf{cor}(\\boldsymbol{y}, \\widehat{\\boldsymbol{y}})\\}^2 = 1-\\frac{\\mathsf{SCE}}{\\mathsf{SCT}},\n\\end{align*}\\] où \\(\\mathsf{SCT}=\\sum_{i=1}^n (Y_i-\\overline{Y})^2\\) est la somme des carrés totale calculée en centrant les observations. La somme des carrés totale, \\(\\mathsf{SCT}\\), ne varie pas en fonction du modèle. Ainsi, on voit que le \\(R^2\\) va méchaniquement augmenter lorsqu’on ajoute une variable au modèle (car la \\(\\mathsf{SCE}\\) diminue). C’est pourquoi on ne peut pas l’utiliser comme outil de sélection de variables.\nLe problème principal que nous avons identifié jusqu’à présent afin d’être en mesure de bien estimer la performance d’un modèle est le suivant : si on utilise les mêmes observations pour évaluer la performance d’un modèle que celles qui ont servi à l’ajuster, on va surestimer sa performance.\nIl existe deux grandes approches pour contourner ce problème lorsque le but est de faire de la sélection de variables ou de modèle :\n\nutiliser les données de l’échantillon d’apprentissage (en échantillon) et pénaliser la mesure d’ajustement (ici \\(\\widehat{\\mathsf{EQM}}_a\\)) pour tenir compte de la complexité du modèle (par exemple, à l’aide de critères d’informations).\ntenter d’estimer l’\\(\\mathsf{EQM}\\) directement sur d’autres données (hors échantillon) en utilisant des méthodes de rééchantillonnage, notamment la validation croisée ou la validation externe (division de l’échantillon).\n\n\n\n4.3.4 Pénalisation et critères d’information\nPlaçons-nous dans le contexte de la régression linéaire pour l’instant. Nous avons déjà utilisé les critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) en analyse factorielle. Il s’agit de mesures qui découlent d’une méthode d’estimation des paramètres, la méthode du maximum de vraisemblance.\nIl s’avère que les estimateurs des paramètres obtenus par la méthode des moindres carrés en régression linéaire sont équivalents à ceux provenant de la méthode du maximum de vraisemblance si on suppose la normalité des termes d’erreurs du modèle. Ainsi, dans ce cas, nous avons accès aux \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\), deux critères d’information définis pour les modèles dont la fonction objective est la vraisemblance (qui mesure la probabilité des observations sous le modèle postulé suivant une loi choisie par l’utilisateur). La fonction de vraisemblance \\(\\mathcal{L}\\) et la log-vraisemblance \\(\\ell\\) mesurent l’adéquation du modèle.\nSupposons que nous avons ajusté un modèle avec \\(p\\) paramètres en tout (incluant l’ordonnée à l’origine). En régression linéaire, le critère d’information d’Akaike, \\(\\mathsf{AIC}\\), est \\[\\begin{align*}\n\\mathsf{AIC} &=-2 \\ell(\\widehat{\\boldsymbol{\\beta}}, \\widehat{\\sigma}^2) +2p = n \\ln (\\mathsf{EQM}) + 2p + \\text{constante},\n\\end{align*}\\] tandis que le critère d’information bayésien de Schwartz, \\(\\mathsf{BIC}\\), est défini par \\[\\begin{align*}\n\\mathsf{BIC} &=-2 \\ell(\\widehat{\\boldsymbol{\\beta}}, \\widehat{\\sigma}^2) + p\\ln(n)=n \\ln (\\mathsf{EQM}) + p\\ln(n) + \\text{constante}.\n\\end{align*}\\] Plus la valeur du \\(\\mathsf{AIC}\\) (ou du \\(\\mathsf{BIC}\\)) est petite, meilleur est l’adéquation. Que se passe-t-il lorsqu’on ajoute un paramètre à un modèle? D’une part, la somme du carré des erreurs va méchaniquement diminuer tout comme l’erreur quadratique moyenne \\(\\textsf{EQM} = \\textsf{SCE}/n\\), donc la quantité \\(n \\ln (\\mathsf{EQM})\\) va diminuer. D’autre part, la valeur de \\(p\\) augmente de \\(1\\). Ainsi, le \\(\\mathsf{AIC}\\) peut soit augmenter, soit diminuer, lorsqu’on ajoute un paramètre; idem pour le \\(\\mathsf{BIC}\\). Par exemple, le \\(\\mathsf{AIC}\\) va diminuer seulement si la baisse de la somme du carré des erreurs est suffisante pour compenser le fait que le terme \\(2p\\) augmente à \\(2 (p+1)\\).\nCes critères pénalisent l’ajout de variables afin de se prémunir contre le surajustement. De plus, le \\(\\mathsf{BIC}\\) pénalise plus que le \\(\\mathsf{AIC}\\). Par conséquent, le critère \\(\\mathsf{BIC}\\) va choisir des modèles contenant soit le même nombre, soit moins de paramètres que le \\(\\mathsf{AIC}\\).\nLes critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) peuvent être utilisés comme outils de sélection de variables en régression linéaire mais aussi beaucoup plus généralement avec d’autres méthodes basées sur la vraisemblance (analyse factorielle, régression logistique, etc.) En fait, n’importe quel modèle dont les estimateurs proviennent de la méthode du maximum de vraisemblance produira ces quantités. Nous donnerons des formules générales pour le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\) dans le chapitre sur la régression logistique.\nLe critère \\(\\mathsf{BIC}\\) est le seul de ces critères qui est convergent. Cela veut dire que si l’ensemble des modèles que l’on considère contient le vrai modèle, alors la probabilité que le critère \\(\\mathsf{BIC}\\) choisissent le bon modèle tend vers 1 lorsque \\(n\\) tend vers l’infini. Il faut mettre cela en perspective : il est peu vraisemblable que \\(Y\\) ait été généré exactement selon un modèle de régression linéaire, car le modèle de régression n’est qu’une approximation de la réalité. Certains auteurs trouvent que le \\(\\mathsf{BIC}\\) est quelquefois trop sévère (il choisit des modèles trop simples) pour les tailles d’échantillons finies. Dans certaines applications, cette parcimonie est utile, mais il n’est pas possible de savoir d’avance lequel de ces deux critères (\\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\)) sera préférable pour un problème donné.\nIl est facile d’obtenir le \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) avec les méthodes AIC et BIC. On illustre ceci avec le modèle cubique:\n\ndata(polynome, package = \"hecmulti\")\n# Ajuster un polynôme de degré trois (modèle cubique)\nmod_cub <- lm(y ~ poly(x, 3),\n              data = polynome)\nsummary(mod_cub) # Tableau résumé des coefficients\nAIC(mod_cub)\nBIC(mod_cub)\n\nLe Tableau 4.3 résume ces quantités pour tous les modèles de l’ordre 1 à l’ordre 10.\n\n\n\n\nTableau 4.3:  Mesures de la qualité de l’ajustement d’un modèle polynomial aux données en fonction de l’ordre du polynôme. \n \n  \n      \n    \\(\\mathsf{EQM}\\) \n    \\(\\widehat{\\mathsf{EQM}}_a\\) \n    \\(R^2\\) \n    \\(\\mathsf{AIC}\\) \n    \\(\\mathsf{BIC}\\) \n    \\(\\mathsf{VC}_{10}\\) \n  \n \n\n  \n    1 \n    3191.29 \n    3674.20 \n    0.65 \n    1110.70 \n    1118.51 \n    3675.37 \n  \n  \n    2 \n    3132.67 \n    2879.24 \n    0.73 \n    1088.32 \n    1098.74 \n    2897.94 \n  \n  \n    3 \n    2697.40 \n    2620.05 \n    0.75 \n    1080.88 \n    1093.91 \n    2675.51 \n  \n  \n    4 \n    2766.68 \n    2581.70 \n    0.75 \n    1081.41 \n    1097.04 \n    2666.16 \n  \n  \n    5 \n    2771.05 \n    2580.86 \n    0.75 \n    1083.38 \n    1101.61 \n    2711.11 \n  \n  \n    6 \n    2779.66 \n    2577.60 \n    0.75 \n    1085.25 \n    1106.09 \n    2757.13 \n  \n  \n    7 \n    2780.21 \n    2577.49 \n    0.75 \n    1087.24 \n    1110.69 \n    2787.95 \n  \n  \n    8 \n    2797.35 \n    2531.00 \n    0.76 \n    1087.42 \n    1113.48 \n    2845.78 \n  \n  \n    9 \n    2811.07 \n    2527.85 \n    0.76 \n    1089.30 \n    1117.96 \n    2895.61 \n  \n  \n    10 \n    2848.81 \n    2519.14 \n    0.76 \n    1090.95 \n    1122.22 \n    2976.04 \n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: Critères d’information en fonction de l’ordre du polynôme.\n\n\n\n\nOn voit dans le Tableau 4.3 que l’erreur quadratique moyenne des données d’apprentissage, \\(\\widehat{\\mathsf{EQM}}_a\\), diminue toujours à mesure qu’on ajoute des variables (c’est-à-dire, qu’on augmente l’ordre du polynôme); ces valeurs sont représentées dans la Figure 4.2. Les critères d’information, \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\), ne sont pas sur la même échelle, mais le graphique de la Figure 4.3 illustre un comportement semblable à la vraie courbe de l’erreur quadratique moyenne théorique et suggèrent que le meilleur modèle est le modèle cubique (\\(k=3\\)), c’est-à-dire le vrai modèle. N’oubliez pas que ces critères sont calculés avec l’échantillon d’apprentissage (\\(n=100\\)), mais en pénalisant l’ajout de variables. On est ainsi en mesure de contrecarrer le problème provenant du fait qu’on ne peut pas utiliser directement le \\(\\widehat{\\mathsf{EQM}}_a\\).\nLe \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\) sont des critères très utilisés et très généraux. Ils sont disponibles dès qu’on utilise la méthode du maximum de vraisemblance comme méthode d’estimation.\n\n\n4.3.5 Validation externe\nLa deuxième grande approche après celle consistant à pénaliser le \\(\\widehat{\\mathsf{EQM}}_a\\) consiste à tenter d’estimer le \\(\\mathsf{EQM}\\) directement sans utiliser deux fois les mêmes données. Nous allons voir deux telles méthodes ici, la validation externe (division de l’échantillon) et la validation croisée (cross-validation).\nCes deux méthodes s’attaquent directement au problème qu’on ne peut utiliser (sans ajustement) les mêmes données qui ont servi à estimer les paramètres d’un modèle pour estimer sa performance. Pour ce faire, l’échantillon de départ est divisé en deux, ou plusieurs parties, qui vont jouer des rôles différents.\nL’idée de la validation externe est simple. Nous avons un échantillon de taille \\(n\\) que nous pouvons diviser au hasard en deux parties de tailles respectives \\(n_1\\) et \\(n_2\\) (\\(n_1+n_2=n\\)), soit\n\nun échantillon d’apprentissage (training) de taille \\(n_1\\) et\nun échantillon de validation (test) de taille \\(n_2\\).\n\nL’échantillon d’apprentissage servira à estimer les paramètres du modèle. L’échantillon de validation servira à estimer la performance prédictive (par exemple estimer l’\\(\\mathsf{EQM}\\)) du modèle. Comme cet échantillon n’a pas servi à estimer le modèle lui-même, il est formé de « nouvelles » observations qui permettent d’évaluer d’une manière réaliste la performance du modèle. Comme il s’agit de nouvelles observations, on n’a pas à pénaliser la complexité du modèle et on peut directement utiliser le critère de performance choisi, par exemple, l’erreur quadratique moyenne, c’est-à-dire, la moyenne des erreurs au carré pour l’échantillon de validation. Cette quantité est une estimation valable de l’\\(\\mathsf{EQM}\\) de ce modèle. On peut faire la même chose pour tous les modèles en compétition et choisir celui qui a la meilleure performance sur l’échantillon de validation.\nCette approche possède plusieurs avantages. Elle est facile à implanter. Elle est encore plus générale que les critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\). En effet, ces critères découlent de la méthode d’estimation du maximum de vraisemblance. Plusieurs autres types de modèles ne sont pas estimés par la méthode du maximum de vraisemblance (par exemple, les arbres, les forêts aléatoires, les réseaux de neurones, etc.) La performance de ces modèles peut toujours être estimée en divisant l’échantillon. Cette méthode peut donc servir à comparer des modèles de familles différentes. Par exemple, choisit-on un modèle de régression linéaire, une forêt aléatoire ou bien un réseau de neurones?\nCette approche possède tout de même un désavantage. Elle nécessite une grande taille d’échantillon au départ. En effet, comme on divise l’échantillon, on doit en avoir assez pour bien estimer les paramètres du modèle (l’échantillon d’apprentissage) et assez pour bien estimer sa performance (l’échantillon de validation).\nLa méthode consistant à diviser l’échantillon en deux (apprentissage et validation) afin de sélectionner un modèle est valide. Par contre, si on veut une estimation sans biais de la performance du modèle choisi (celui qui est le meilleur sur l’échantillon de validation), on ne peut pas utiliser directement la valeur observée de l’erreur de ce modèle sur l’échantillon de validation car elle risque de sous-évaluer l’erreur. En effet, supposons qu’on a 10 échantillons et qu’on ajuste 10 fois le même modèle séparément sur les 10 échantillons. Nous aurons alors 10 estimations différentes de l’erreur du modèle. Il est alors évident que de choisir la plus petite d’entre elles sous-estimerait la vraie erreur du modèle. C’est un peu ce qui se passe lorsqu’on choisit le modèle qui minimise l’erreur sur l’échantillon de validation. Le modèle lui-même est un bon choix, mais l’estimation de son erreur risque d’être sous-évaluée.\nUne manière d’avoir une estimation de l’erreur du modèle retenu consiste à diviser l’échantillon de départ en trois (plutôt que deux). Aux échantillons d’apprentissage et de validation, s’ajoute un échantillon « test ». Cet échantillon est laissé de côté durant tout le processus de sélection du modèle qui est effectué avec les deux premiers échantillons tel qu’expliqué plus haut. Une fois un modèle retenu (par exemple celui qui minimise l’erreur sur l’échantillon de validation), on peut alors évaluer sa performance sur l’échantillon test qui n’a pas encore été utilisé jusque là. L’estimation de l’erreur du modèle retenu sera ainsi valide. Il est évident que pour procéder ainsi, on doit avoir une très grande taille d’échantillon au départ.\n\n\n4.3.6 Validation croisée\nSi la taille d’échantillon n’est pas suffisante pour diviser l’échantillon en deux et procéder comme nous venons de l’expliquer, la validation croisée est une bonne alternative. Cette méthode permet d’imiter le processus de division de l’échantillon.\nVoici les étapes à suivre pour faire une validation croisée à \\(K\\) groupes (\\(K\\)-fold cross-validation) :\n\nDiviser l’échantillon au hasard en \\(K\\) parties \\(P_1, P_2, \\ldots, P_K\\) contenant toutes à peu près le même nombre d’observations.\nPour \\(j = 1\\) à \\(K\\),\n\nEnlever la partie \\(j\\).\nEstimer les paramètres du modèle en utilisant les observations des \\(K-1\\) autres parties combinées.\nCalculer la mesure de performance (par exemple la somme du carré des erreurs) de ce modèle pour le groupe \\(P_j\\).\n\nCombiner les \\(K\\) estimations de performance pour obtenir une mesure de performance finale.1\n\nPour l’erreur quadratique moyenne, cette dernière étape revient à additionner la somme du carré des erreurs avant de diviser par la taille de l’échantillon totale.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 4.4: Illustration de la validation croisée: on scinde l’échantillon d’apprentissage en cinq groupes (abcisse) et à chaque étape, une portion différente des données est mise de côté et ne sert que pour la validation.\n\n\n\n\nLa validation croisée est coûteuse parce qu’on doit ajuster \\(K\\) fois le modèles. On recommande habituellement de prendre \\(K=\\min\\{n^{1/2}, 10\\}\\) groupes (le choix de cinq ou 10 groupes sont ceux qui revient le plus souvent en pratique). Si on prend \\(K=10\\) groupes, alors chaque modèle est estimé avec 90% des données et on prédit ensuite le 10% restant. Comme on passe en boucle les 10 parties, chaque observation est prédite une et une seule fois à la fin. Il est important de souligner que les groupes sont formés de façon aléatoire et donc que l’estimé que l’on obtient peut être très variable, surtout si la taille de l’échantillon d’apprentissage est petite. Il arrive également que le modèle ajusté sur un groupe ne puisse pas être utilisé pour prédire les observations mises de côté, notamment si des variables catégorielles sont présentes mais qu’une modalité n’est présente que dans un des groupes; ce problème se présente en pratique si certaines classes ont peu d’observations. Un échantillonnage stratifié permet de pallier à cette lacune et de s’assurer d’une répartition plus homogène des variables catégorielles.\n\n# Validation croisée avec k groupes\nlmkfold <- function(formula, data, k, ...){\n   # Créer un accumulateur pour le calcul de l'EQM\n   accu <- 0\n   k <- as.integer(k) # nombre de groupes\n   n <- nrow(data) # nombre d'observations\n   # Permuter les indices des observatoins\n   gp <- sample.int(n, n, replace = FALSE)\n   # Créer une liste de k éléments avec les nos d'observations\n   folds <- split(gp, cut(seq_along(gp), k, labels = FALSE))\n   for(i in seq_len(k)){\n      # Extraire les indices des observations de la portion validation\n      g <- as.integer(unlist(folds[i]))\n      # Ajuster le modèles à toutes les données, moins celles de la portion validation\n      fitlm <- lm(formula, data = data[-g,])\n      # ajouter l'erreur quadratique du pli de validation\n      accu <- accu + sum((data[g, all.vars(formula)[1]] -predict(fitlm, newdata=data[g,]))^2)\n   }\n   # Diviser par la taille de l'échantillon \n   # pour obtenir la moyenne\n   return(accu/n)\n}\n\n# Le paquet 'caret' a une fonction \n# pour faire la validation croisée\ncv_caret <- \n  caret::train(form = formula, \n             data = data, \n             method = \"lm\",\n             trControl = caret::trainControl(\n               method = \"cv\",\n               number = 10))\neqm_cv <- cv_caret$results$RMSE^2\n\nLe cas particulier \\(K=n\\) (en anglais leave-one-out cross validation, ou \\(\\mathsf{LOOCV}\\)) consiste à enlever une seule observation, à estimer le modèle avec les \\(n-1\\) autres et à valider à l’aide de l’observation laissée de côté: on répète cette procédure pour chaque observation. Pour les modèles linéaires, il existe des formules explicites qui nous permettent d’éviter d’ajuster \\(n\\) régressions par moindre carrés. Cette forme de validation croisée tend à être trop optimiste.\nIl faut garder en tête que le résultat de la validation croisée est aléatoire parce que la séparation des données en plis l’est également. La figure Figure 4.5, obtenue en répétant 100 fois la procédure et en calculant à chaque fois la performance de différents modèles polynomiaux, montre la variabilité des estimations. Plutôt que de répéter le calcul, si on a un nombre de groupes \\(K\\) suffisamment grand et assez d’observations par pli, on pourrait estimer la variabilité de la procédure directement. Posons \\(\\widehat{\\mathsf{EQM}}_{\\text{VC}, k}\\) (\\(k=1, \\ldots, K\\)) calculer l’erreur quadratique moyenne de chaque pli. On peut estimer l’écart-type empirique de cette moyenne via \\[\\begin{align*}\n\\mathsf{sd}(\\widehat{\\mathsf{EQM}}_{\\text{VC}}) = \\frac{1}{K-1} \\sum_{k=1}^{K} (\\widehat{\\mathsf{EQM}}_{\\text{VC}, k}-\\widehat{\\mathsf{EQM}}_{\\text{VC}})^2.\n\\end{align*}\\]\nRevenons à notre exemple où une seule variable explicative est disponible et où l’on cherche à déterminer un bon modèle polynomial. La dernière colonne de Tableau 4.3, \\(\\mathsf{VC}_{10}\\), donne les moyennes de 100 réplications de estimations de l’\\(\\mathsf{EQM}\\) obtenues avec la validation croisée à 10 groupes. Notez que si vous exécutez le programme, vous n’obtiendrez pas les mêmes valeurs car il y a un élément aléatoire dans ce processus.\nLe modèle cubique (ordre 3) est aussi choisi par la validation croisée, en moyenne (comme il l’était par le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\)). Le graphe qui suit trace les valeurs de l’estimation par validation croisée (courbe de validation croisée) et aussi le \\(\\mathsf{EQM}\\). On voit que l’estimation par validation croisée suit assez bien la forme du \\(\\mathsf{EQM}\\) (qu’il est supposé estimer). Les boîtes à moustache de la Figure 4.5 permettent d’apprécier la variabilité des estimés de l’erreur quadratique moyenne telles qu’estimée par validation croisée avec 10 groupes.\n\n\n\n\n\nFigure 4.5: Boîtes-à-moustaches des 100 réplications des valeurs de l’erreur quadratique moyenne estimées par validation croisée à 10 plis pour chaque ordre du polynôme.\n\n\n\n\nIl arrive que la performance soit très similaire pour plusieurs modèles, auquel cas on pourrait être tenté de prendre le modèle le plus parsimonieux (c’est-à-dire, celui qui a le moins de paramètres). Si on a calculé la performance avec la validation croisée et qu’on a obtenu une mesure d’incertitude pour notre performance, on peut utiliser la règle du « 1 écart-type». Cette dernière veut qu’on choisisse le modèle le plus simple parmi un ensemble \\(\\mathcal{M}_0 \\subset\\cdots \\subset \\mathcal{M}_m\\) qui satisfasse \\[\\begin{align*}\n\\widehat{\\mathsf{EQM}}_{\\text{VC}}(\\mathcal{M}_i) \\leq \\min_{m = i+1}^M \\widehat{\\mathsf{EQM}}_{\\text{VC}}(\\mathcal{M}_m) + \\mathsf{sd}\\{\\widehat{\\mathsf{EQM}}_{\\text{VC}}(\\mathcal{M}_m)\\}.\n\\end{align*}\\] Autrement dit, on trouve le modèle qui minimise notre critère d’erreur et on choisit ensuite le modèle le plus simple qui soit à au plus un écart-type de ce modèle. On verra ainsi souvent des barres d’erreurs à \\(\\pm\\) un écart-type, comme dans la Figure 4.9."
  },
  {
    "objectID": "04-selectionmodeles.html#présentation-des-données",
    "href": "04-selectionmodeles.html#présentation-des-données",
    "title": "4  Sélection de variables et de modèles",
    "section": "4.4 Présentation des données",
    "text": "4.4 Présentation des données\nNous allons présenter un exemple classique de commercialisation de bases de données qui nous servira à illustrer la sélection de modèles, la régression logistique et la gestion de données manquantes. Le but est de cibler les clients pour l’envoi d’un catalogue.\nLe contexte est le suivant : une entreprise possède une grande base de données client. Elle désire envoyer un catalogue à ses clients mais souhaite maximiser les revenus d’une telle initiative. Il est évidemment possible d’envoyer le catalogue à tous les clients mais ce n’est possiblement pas optimal. La stratégie envisagée est la suivante :\n\nEnvoyer le catalogue à un échantillon de clients et attendre les réponses. Le coût de l’envoi d’un catalogue est de 10$.\nConstruire un modèle avec cet échantillon afin de décider à quels clients (parmi les autres) le catalogue devrait être envoyé, afin de maximiser les revenus.\n\nPlus précisément, on s’intéresse aux clients de 18 ans et plus qui ont au moins un an d’historique avec l’entreprise et qui ont effectué au moins un achat au cours de la dernière année. Dans un premier lieu, on a envoyé le catalogue à un échantillon de 1000 clients. Un modèle sera construit avec ces 1000 clients afin de cibler lesquels des clients restants seront choisis pour recevoir le catalogue.\nPour les 1000 clients de l’échantillon d’apprentissage, les deux variables cibles suivantes sont disponibles :\n\nyachat, une variable binaire qui indique si le client a acheté quelque chose dans le catalogue égale à 1 si oui et 0 sinon.\nymontant, le montant de l’achat si le client a acheté quelque chose.\n\nLes 10 variables suivantes sont disponibles pour tous les clients et serviront de variables explicatives pour les deux variables cibles. Il s’agit de :\n\nx1: sexe de l’individu, soit homme (0) ou femme (1);\nx2: l’âge (en année);\nx3: variable catégorielle indiquant le revenu, soit moins de 35 000$ (1), entre 35 000$ et 75 000$ (2) ou plus de 75 000$ (3);\nx4: variable catégorielle indiquant la région où habite le client (de 1 à 5);\nx5: couple : la personne est elle en couple (0=non, 1=oui);\nx6: nombre d’année depuis que le client est avec la compagnie;\nx7: nombre de semaines depuis le dernier achat;\nx8: montant (en dollars) du dernier achat;\nx9: montant total (en dollars) dépensé depuis un an;\nx10: nombre d’achats différents depuis un an.\n\nLes données se trouvent dans le fichier dbm. Voici d’abord des statistiques descriptives pour l’échantillon d’apprentissage.\n\ndata(dbm, package = \"hecmulti\")\nstr(dbm)\n\ntibble [101,000 × 13] (S3: tbl_df/tbl/data.frame)\n $ x1      : int [1:101000] 1 1 0 0 1 1 0 0 0 1 ...\n $ x2      : num [1:101000] 42 59 52 32 38 63 35 32 26 32 ...\n $ x3      : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 3 1 2 2 2 1 3 1 ...\n $ x4      : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 3 5 1 5 5 1 3 1 5 ...\n $ x5      : int [1:101000] 1 1 1 0 0 1 1 0 0 0 ...\n $ x6      : num [1:101000] 8.6 8.6 1.4 10.7 9.1 9.4 10.6 4.8 4 10.3 ...\n $ x7      : num [1:101000] 8 9 9 42 5 1 6 5 48 9 ...\n $ x8      : num [1:101000] 49 70 120 31 30 28 59 70 73 55 ...\n $ x9      : num [1:101000] 159 123 434 110 55 102 593 298 83 90 ...\n $ x10     : num [1:101000] 5 5 8 3 3 8 10 6 2 3 ...\n $ yachat  : int [1:101000] 0 0 0 0 0 0 0 1 1 1 ...\n $ ymontant: num [1:101000] NA NA NA NA NA NA NA 52 79 77 ...\n $ test    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\n\nTableau 4.4:  Tableaux de fréquence pour les variables catégorielles de la base de données marketing. \n\n  \n    \n\n\n \n  \n    sexe \n    décompte \n  \n \n\n  \n    0 \n    534 \n  \n  \n    1 \n    466 \n  \n\n\n\n \n    \n\n\n \n  \n    revenu \n    décompte \n  \n \n\n  \n    1 \n    397 \n  \n  \n    2 \n    337 \n  \n  \n    3 \n    266 \n  \n\n\n\n \n    \n\n\n \n  \n    couple \n    décompte \n  \n \n\n  \n    0 \n    575 \n  \n  \n    1 \n    425 \n  \n\n\n\n \n    \n\n\n \n  \n    région \n    décompte \n  \n \n\n  \n    1 \n    216 \n  \n  \n    2 \n    185 \n  \n  \n    3 \n    216 \n  \n  \n    4 \n    191 \n  \n  \n    5 \n    192 \n  \n\n\n\n \n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Histogrammes des variables continues de la base de données dbm pour les 1000 clients par intention d’achat.\n\n\n\n\nIl y a 46.6% de femmes parmi les 1000 clients de l’échantillon. De plus, 39.7% ont un revenu de moins de 35 000$, 33.7% sont entre 35 000$ et 75 000$ et 26.6% ont plus de 75 000$. 42.5% de ces clients qui ont un conjoint.\n\n\n\n\nTableau 4.5:  Statistiques descriptives des variables numériques de la base de données marketing. \n \n  \n    variable \n    description \n    moyenne \n    écart-type \n    min \n    max \n  \n \n\n  \n    x2 \n    âge \n    37.06 \n    9.27 \n    20 \n    70 \n  \n  \n    x6 \n    nombre d’année comme client \n    6.01 \n    2.92 \n    1 \n    11 \n  \n  \n    x7 \n    nombre de semaines depuis le dernier achat \n    9.97 \n    9.34 \n    1 \n    52 \n  \n  \n    x8 \n    montant du dernier achat \n    48.41 \n    28.27 \n    20 \n    252 \n  \n  \n    x9 \n    montant total dépensé sur un an \n    229.27 \n    173.97 \n    22 \n    1407 \n  \n  \n    x10 \n    nombre d'achats différents sur un an \n    5.64 \n    2.31 \n    1 \n    14 \n  \n\n\n\n\n\n\nLe nombre d’achats différents depuis un an par ces clients varie entre 1 et 14. Un peu plus de la moitié (51.4%) ont fait cinq achats ou moins. Parmi les 1000 clients de l’échantillon d’apprentissage, 210 ont acheté quelque chose dans le catalogue. La variable yachat sera l’une des variables que nous allons chercher à modéliser en vue d’obtenir des prédictions.\nL’âge des 1000 clients de l’échantillon d’apprentissage varie entre 20 et 70 avec une moyenne de 37.1 ans. En moyenne, ces clients ont acheté pour 229.30$ depuis un an. Le dernier achat de ces clients remonte, en moyenne, à 10 semaines.\nDans cette section, nous modéliserons le montant d’achat, ymontant. Seuls 210 clients ont acheté quelque chose dans le catalogue et les statistiques rapportées correspondent seulement à ces derniers, car la variable ymontant est manquante si le client n’a rien acheté dans le catalogue. On pourrait également remplacer ces valeurs par des zéros et les modéliser, mais nous aborderons cet aspect ultérieurement. Les clients qui ont acheté quelque chose ont dépensé en moyenne 67.3$, et au minimum 25$. La Figure 4.6 présente les histogrammes de quelques unes de ces variables.\nIl y a plusieurs façons d’utiliser l’échantillon d’apprentissage afin de mieux cibler les clients à qui envoyer le catalogue et maximiser les revenus. En voici quelques unes.\n\nOn pourrait développer un modèle afin d’estimer la probabilité qu’un client achète quelque chose si on lui envoie un catalogue. Plus précisément, on peut développer un modèle pour \\(\\Pr(\\texttt{yachat}=1)\\). Comme la variable yachat est binaire, un modèle possible est la régression logistique, que nous décrirons au chapitre suivant. Ainsi, en appliquant le modèle aux 100 000 clients restant, on pourra cibler les clients susceptibles d’acheter (ceux avec une probabilité élevée).\nUne autre façon serait de tenter de prévoir le montant d’argent dépensé. Nous venons de voir la distribution de la variable ymontant. Il y a deux situations, ceux qui ont acheté et ceux qui n’ont pas achetés. En conditionnant sur le fait d’avoir acheté quelque chose, il est possible de décomposer le problème de la manière suivante :\n\n\\[\\begin{align*}\n\\mathsf{E}(\\texttt{ymontant}) &= \\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1) \\mathsf{P}(\\texttt{yachat}=1) \\\\& \\quad +\n\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=0) \\mathsf{P}(\\texttt{yachat}=0) \\\\\n&= \\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1) \\mathsf{P}(\\texttt{yachat}=1),\n\\end{align*}\\] puisque le terme \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=0)\\) est zéro: les gens qui n’ont pas acheté n’ont rien dépensé.\nOn peut donc estimer \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) et \\(\\mathsf{P}(\\texttt{yachat}=1)\\), pour ensuite les combiner et avoir une estimation de \\(\\mathsf{E}(\\texttt{ymontant})\\). Le développement du modèle pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) peut se faire avec la régression linéaire, en utilisant seulement les clients qui ont acheté dans l’échantillon d’apprentissage, car \\(\\texttt{ymontant}\\) est une variable continue dans ce cas. Le développement du modèle pour \\(\\mathsf{P}(\\texttt{yachat}=1)\\) peut se faire avec la régression logistique, tel que mentionné plus haut, en utilisant tous les 1000 clients de l’échantillon d’apprentissage. En fait, nous verrons plus loin qu’il est possible d’estimer conjointement les deux modèles avec un modèle Tobit. En appliquant le modèle aux 100 000 clients restants, on pourra cibler les clients qui risquent de dépenser un assez grand montant.\nComme nous n’avons pas encore vu la régression logistique, nous allons nous limiter à illustrer les méthodes qui restent à voir dans ce chapitre avec la régression linéaire en cherchant à développer un modèle pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\), le montant d’argent dépensé par les clients qui ont acheté quelque chose.\nLa base de donnée contient deux variables explicatives catégorielles. Il s’agit de revenu (x3) et région (x4). Il faut coder d’une manière appropriée afin de pouvoir les incorporer dans les modèles. La manière habituelle est de créer des variables indicatrices (binaires) qui indiquent si la variable prend ou non une valeur particulière dans R est de transformer la variable en facteur (factor). En général, si une variable catégorielle possède \\(K\\) valeurs possibles, il est suffisant de créer \\(K-1\\) indicatrices, en laissant une modalité comme référence. Par exemple, pour x3, nous allons créer deux variables,\n\nx31: variable binaire égale à 1 si x3 égale 1 et 0 sinon,\nx32: variable binaire égale à 1 si x3 égale 2 et 0 sinon.\n\nAinsi, la valeur 3 est celle de référence. Ces deux indicatrices sont suffisantes pour récupérer toute l’information comme le démontre le Tableau 4.6.\n\n\nTableau 4.6: Valeur des indicateurs en fonction du niveau de la variable catégorielle\n\n\nx3\nx31\nx32\n\n\n\n\n1\n1\n0\n\n\n2\n0\n1\n\n\n3\n0\n0\n\n\n\n\nIl est important de noter que, si le modèle qui inclut toutes les modalités (ordonnée à l’origine, x31 et x32) possibles ne dépend pas de la catégorie de référence, ce ne sera plus le cas si on permet lors de la sélection de variables de ne conserver que certains niveaux de la variable catégorielle. Par exemple, si on inclut uniquement x31 comme variable explicative, l’ordonnée à l’origine englobera toutes les autres valeurs de x3, à savoir \\(\\{2, 3\\}\\).2\n\n\n\n\n\n\nDanger de surajustement avec variables catégorielles\n\n\n\nLa principale cause de mauvaise performance est le surajustement sélectif. Dans l’exemple que l’on considère avec la base de données marketing, la plupart des modalités des variables catégorielles semblent à première vues suffisantes pour estimer des coefficients. Si on s’intéresse par contre aux interactions, on se rendra rapidement compte qu’il y a trop peu de valeurs pour certaines combinaisons (par exemple, x3*x5) pour estimer de manière fiable l’effet combiné. Si on a une valeur aberrante dans un groupe avec de faibles modalités, les indicateurs donneront systématiquement préférence à l’inclusion d’un terme pour l’accomoder (au détriment de la généralisation). Cela a pour effet de fausser la sélection et donner une grande erreur quadratique moyenne de validation. Si certaines modalités ont des effectifs trop petits, on peut envisager de les regrouper avec d’autres similaires."
  },
  {
    "objectID": "04-selectionmodeles.html#sélection-de-variables",
    "href": "04-selectionmodeles.html#sélection-de-variables",
    "title": "4  Sélection de variables et de modèles",
    "section": "4.5 Sélection de variables",
    "text": "4.5 Sélection de variables\n\n4.5.1 Recherche exhaustive (meilleurs sous-ensembles)\nLorsque nous voulons comparer un petit nombre de modèles, il est relativement aisé d’obtenir les critères (\\(\\mathsf{AIC}\\), \\(\\mathsf{BIC}\\) ou autre) pour tous les modèles et de choisir le meilleur. C’était le cas dans l’exemple du choix de l’ordre du polynôme où il y avait seulement 10 modèles en compétitions. Mais lorsqu’il y a plusieurs variables en jeu, le nombre de modèles potentiel augmente très rapidement.\nEn fait, supposons qu’on a \\(p\\) variables distinctes disponibles. Avant même de considérer les transformations des variables et les interactions entre elles, il y a déjà trop de modèles possibles. En effet, chaque variable est soit incluse ou pas (deux possibilités) et donc il y a \\(2^p=2\\times 2 \\times \\cdots \\times 2\\) (\\(p\\) fois) modèles en tout à considérer. Ce nombre augmente très rapidement comme en témoigne le Tableau 4.7.\n\n\n\n\nTableau 4.7:  Nombres de modèles en fonction du nombre de paramètres. \n \n  \n    \\(p\\) \n    nombre de paramètres \n  \n \n\n  \n    5 \n    32 \n  \n  \n    10 \n    1024 \n  \n  \n    15 \n    32768 \n  \n  \n    20 \n    1048576 \n  \n  \n    25 \n    33554432 \n  \n  \n    30 \n    1073741824 \n  \n\n\n\n\n\n\nAinsi, si le nombre de variables est restreint, il est possible de comparer tous les modèles potentiels et de choisir le meilleur (selon un critère). II existe même des algorithmes très efficaces qui permettent de trouver le meilleur modèle sans devoir examiner tous les modèles possibles. Le nombre de variables qu’il est possible d’avoir dépend de la puissance de calcul et augmente d’année en année. Par contre, dans plusieurs applications, il ne sera pas possible de comparer tous les modèles et il faudra effectuer une recherche limitée. Faire une recherche exhaustive parmi tous les modèles possibles s’appelle sélection de tous les sous-ensembles (best subsets).\nOn veut trouver un bon modèle pour prévoir la valeur de ymontant des clients qui ont acheté quelque chose. On a vu qu’il y a 210 clients qui ont acheté dans l’échantillon d’apprentissage. Nous allons chercher à développer un « bon » modèle avec ces 210 clients. Dans ce premier exemple, nous allons seulement utiliser les 10 variables explicatives de base (14 variables avec les indicatrices).\nPour un nombre de variables fixé, le meilleur modèle selon le \\(R^2\\) est aussi le meilleur selon les critères d’information \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\), pour ce nombre fixé de variables. Pour vous convaincre de cette affirmation, fixons le nombre de variables et restreignons-nous seulement aux modèles avec ce nombre de variables. Comme \\(R^2=1 - \\mathsf{SCE}/\\mathsf{SCT}\\) et que \\(\\mathsf{SCT}\\) est une constante indépendante du modèle, le modèle avec le plus grand coefficient de détermination, \\(R^2\\), est aussi celui avec la plus petite somme du carré des erreurs (\\(\\mathsf{SCE}\\)). Comme \\(\\mathsf{AIC}=n \\ln (\\mathsf{EQM}) + 2p\\), ce sera aussi celui avec le plus petit \\(\\mathsf{AIC}\\) car la pénalité \\(2p\\) est la même si on fixe le nombre de variables; la même remarque est valide pour le \\(\\mathsf{BIC}\\).\nAinsi, pour trouver le meilleur modèle globalement (sans fixer le nombre de variables), il suffit de trouver le modèle à \\(k\\) variables explicatives ayant le coefficient de détermination le plus élevé pour tous les nombres de variables fixés et d’ensuite de trouver celui qui minimise le \\(\\mathsf{AIC}\\) (ou le \\(\\mathsf{BIC}\\)) parmi ces modèles. Ainsi, le modèle linéaire simple qui a le plus grand \\(R^2\\) est celui qui inclut l’indicateur de couple (x5). Le meilleur modèle (selon le \\(R^2\\)) parmi tous les modèles avec deux variables est celui avec x5 et x6.\n\n\n\n\nTableau 4.8:  Modèle parmi les candidats ayant la plus grande valeur de coefficient de détermination selon le nombre de régresseurs, avec valeurs des critères d’informations. \n \n  \n    variables \n    BIC \n    AIC \n  \n \n\n  \n    x5 \n    -95.96 \n    -102.65 \n  \n  \n    x5 x6 \n    -196.41 \n    -206.45 \n  \n  \n    x31 x5 x6 \n    -311.09 \n    -324.48 \n  \n  \n    x31 x5 x6 x10 \n    -351.05 \n    -367.79 \n  \n  \n    x1 x31 x5 x6 x10 \n    -369.33 \n    -389.41 \n  \n  \n    x1 x31 x5 x6 x7 x10 \n    -387.09 \n    -410.52 \n  \n  \n    x1 x31 x5 x6 x7 x8 x10 \n    -392.18 \n    -418.95 \n  \n  \n    x1 x31 x44 x5 x6 x7 x8 x10 \n    -390.68 \n    -420.80 \n  \n  \n    x1 x2 x31 x44 x5 x6 x7 x8 x10 \n    -390.05 \n    -423.53 \n  \n  \n    x1 x2 x31 x44 x5 x6 x7 x8 x9 x10 \n    -386.94 \n    -423.75 \n  \n  \n    x1 x2 x31 x43 x44 x5 x6 x7 x8 x9 x10 \n    -382.86 \n    -423.03 \n  \n  \n    x1 x2 x31 x41 x42 x43 x44 x5 x6 x7 x8 x10 \n    -378.39 \n    -421.91 \n  \n  \n    x1 x2 x31 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10 \n    -374.76 \n    -421.62 \n  \n\n\n\n\n\n\nUn algorithme par séparation et évaluation permet d’effectuer cette recherche de manière efficace sans essayer tous les candidats pour ces sous-ensembles. Dans l’exemple, on voit que le modèle avec les variables x1 x2 x31 x44 x5 x6 x7 x8 x9 et x10 est celui qui minimise le \\(\\mathsf{AIC}\\) globalement (\\(\\mathsf{AIC}\\) de -423.7539583). Le modèle choisi par le \\(\\mathsf{BIC}\\) contient seulement sept variables explicatives (plutôt que 10), soit x1 x31 x5 x6 x7 x8 x10.\n\ndata(dbm, package = \"hecmulti\")\ndbm_a <- dbm |>\n  dplyr::filter(test == 0,\n                !is.na(ymontant))\n# Conserver données d'entraînement (test == 0)\n# des personnes qui ont acheté ymontant > 0\n   \nrec_ex <- leaps::regsubsets(\n  x = ymontant ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, \n  nvmax = 13L,\n  method = \"exhaustive\",\n  data = dbm_a)\nresume_rec_ex <- summary(rec_ex,\n                         matrix.logical = TRUE)\n# Trouver le modèle avec le plus petit BIC\nmin_BIC <- which.min(resume_rec_ex$bic)\n# Nom des variables dans le modèle retenu\nrec_ex$xnames[resume_rec_ex$which[min_BIC,]]\n# Coefficients\n# coef(rec_ex, id = min_BIC)\n\nNous avons seulement inclus les variables de base pour ce premier essai. Il est possible qu’ajouter des variables supplémentaires améliore la performance du modèle. Pour cet exemple, nous allons considérer les variables suivantes3:\n\nles variables continues au carré, comme \\(\\texttt{age}^2\\).\ntoutes les interactions d’ordre deux entre les variables de base, comme \\(\\texttt{sexe}\\cdot\\texttt{age}\\).\n\nAux variables de base (10 variables explicatives, mais 14 avec les indicatrices pour les variables catégorielles), s’ajoutent ainsi 90 autres variables. Il y a donc 104 variables explicatives potentielles si on inclut les interactions et les termes quadratiques. Notez qu’il y a des interactions entre chacune des variables indicatrices et chacune des autres variables, mais il ne sert à rien de calculer une interaction entre deux indicatrices d’une même variable (car une telle variable est zéro pour tous les individus). De même, il ne sert à rien de calculer le carré d’une variable binaire codée \\(\\{0, 1\\}\\).\nDans la mesure où on aura un ratio d’environ un paramètre pour deux observations,Le modèle à 104 variables servira uniquement à illustrer le surajustement. Pensez à la taille de votre échantillon comme à un budget et aux paramètres comme à un nombre d’items: plus vous achetez d’items, moins votre budget est élevé pour chacun et leur qualité en pâtira. Réalistement, un modèle avec plus d’une vingtaine de variables ici serait difficilement estimable de manière fiable et l’inclusion d’interactions et de termes quadratiques sert surtout à augmenter la flexibilité et les possibilités lors de la sélection de variables.\n\n# (...)^2 crée toutes les interactions d'ordre deux\n# I(x^2) permet de créer les termes quadratiques\nformule <- \n  formula(ymontant ~ \n          (x1 + x2 + x3 + x4 + x5 + \n             x6 + x7 + x8 + x9 + x10)^2 + \n            I(x2^2) + I(x6^2) + I(x7^2) +\n            I(x8^2) + I(x9^2) + I(x10^2))\nmod_complet <- lm(formule, data = dbm_a)\n\nLancer une sélection exhaustive de tous les sous-modèles avec 104 variables risque de prendre un temps énorme. Que faire alors? Il y a plusieurs possibilités. Nous pourrions faire une recherche limitée avec les méthodes que nous allons voir à partir de la section suivante. Nous pourrions aussi combiner les deux approches. Supposons que notre ordinateur permet de faire une recherche exhaustive de tous les sous-modèles avec 40 variables. Nous pourrions alors commencer avec une recherche limitée pour trouver un sous-ensemble de 40 « bonnes » variables et faire une recherche exhaustive, mais en se restraignant à ces 40 variables.\n\n\n4.5.2 Méthodes séquentielles de sélection\nLes méthodes de sélection ascendante, descendante et séquentielle sont des algorithmes gloutons. Elles ont été développées à une époque où la puissance de calcul était bien moindre, et où il était impossible de faire une recherche exhaustive des sous-modèles. La procédure leaps::regsubsets permet une sélection de modèle avec une approche séquentielle, ascendante ou descendante en choisissant le meilleur modèle (côté ajustement) avec \\(k\\) variables \\((k=1, \\ldots, k_{\\text{max}})\\). La procédure MASS::stepAIC permet de faire cette sélection en utilisant un critère d’information.\nL’idée de la sélection ascendante est d’ajouter à chaque étape au modèle précédent la variable qui améliore le plus l’ajustement. Le modèle de départ est celui qui n’inclut que l’ordonnée à l’origine (aucune variable explicative). À chaque étape, on ajoute la variable qui améliore le plus le critère d’ajustement jusqu’à ce qu’aucune amélioration ne soit résultante.\nUn algorithme glouton résoud un problème d’optimisation étape par étape: après \\(k\\) étapes, le modèle construit par la procédure n’est pas nécessairement le meilleur modèle (si on essayait toutes les combinaisons). Si on commence avec \\(p\\) variables, on regarde \\(p\\) choix à la première étape de la procédure ascendante, puis on choisit nue variable parmi les \\(p-1\\) restantes à la deuxième étape, etc. La procédure exhaustive essaiera toutes les \\(\\binom{p}{2}\\) combinaisons possibles4: puisque plus de modèles sont essayés, la solution finale est nécessairement meilleure côté performance évaluée sur l’échantillon d’apprentissage.\nLa sélection descendante est similaire, sauf qu’on part avec le modèle qui inclut toutes les variables explicatives. À chaque étape, on retire la variable qui contribue le moins à l’ajustement jusqu’à ce que le critère d’ajustement ne puisse plus être amélioré ou jusqu’à ce qu’on recouvre le modèle sans variables explicatives, selon le scénario. C’est l’inverse de la méthode ascendante: on va tester le retrait de chaque variable individuellement et retirer celle qui est la moins significative.\nLa méthode de sélection séquentielle est un hybride entre les méthodes de sélection ascendantes et descendante. On débute la recherche à partir du modèle ne contenant que l’ordonnée à l’origine. À chaque étape, on fait une étape ascendante suivie de une (ou plusieurs) étapes descendantes. On continue ainsi tant que le modèle retourné par l’algorithme n’est pas identique à celui de l’étape précédente (dépendant de notre critère). Le dernier modèle est celui retenu.\nAvec la méthode séquentielle, une fois qu’on entre une variable (étape ascendante), on fait autant d’étapes descendante afin de retirer toutes les variables qui satisfont le critère de sortie (il peut ne pas y en avoir). Une fois cela effectué, on refait une étape ascendante pour voir si on peut ajouter une nouvelle variable.\nAvec la méthode ascendante, une fois qu’une variable est dans le modèle, elle y reste. Avec la méthode descendante, une fois qu’une variable est sortie du modèle, elle ne peut plus y entrer. Avec la méthode séquentielle, une variable peut entrer dans le modèle et sortir plus tard dans le processus. Par conséquent, parmi les trois, la méthode séquentielle est généralement préférable aux méthodes ascendante et descendante, car elle inspecte potentiellement un plus grand nombre de modèles.\n\n# Cette procédure séquentielle retourne\n# la liste de modèles de 1 variables à\n# nvmax variables.\nrec_seq <- \n  leaps::regsubsets(\n    x = formule, \n    data = dbm_a,\n    method = \"seqrep\", \n    nvmax = length(coef(mod_complet)))\nwhich.min(summary(rec_seq)$bic)\n\n# Alternative avec procédure séquentielle\n# qui utilise le critère AIC pour déterminer \n# l'inclusion ou l'exclusion de variables\n# \n# Procédure plus longue à rouler\n# (car les modèles linéaires sont ajustés)\n# \n# On ajoute ou retire la variable qui\n# améliore le plus le critère de sélection\n# à chaque étape.\nseq_AIC <- MASS::stepAIC(\n  lm(ymontant ~ 1, data = dbm_a), \n  # modèle initial sans variables explicative\n    scope = formule, # modèle maximal possible\n    direction = \"both\", #séquentielle\n    trace = FALSE, # ne pas imprimer le suivi\n    keep = function(mod, AIC, ...){ \n      # autres sorties des modèles à conserver\n      list(bic = BIC(mod), \n           coef = coef(mod))},\n    k = 2) #\n# Remplacer k=2 par k = log(nrow(dbm_a)) pour BIC\n\n# L'historique des étapes est disponible via\n# seq_AIC$anova\n\nLa procédure exhaustive est préférable aux méthodes séquentielles si le nombre de variables n’est pas trop élevé. S’il y a trop de variables, rien ne nous empêche de combiner plusieurs méthodes: on pourrait par exemple faire une procédure descendante pour ne conserver que 40 variables. En utilisant seulement ce sous-ensemble de variables, on choisit le meilleur modèle selon le \\(\\mathsf{AIC}\\) ou le \\(\\mathsf{BIC}\\) en faisant une recherche exhaustive de tous les sous-modèles. On pourrait également faire une recherche séquentielle avec le \\(\\mathsf{AIC}\\) et choisir le modèle parmi l’historique avec le plus petit \\(\\mathsf{BIC}\\).\n\n\n\n\n\n\n\n\nFigure 4.7: Critères d’information et estimation de l’erreur quadratique moyenne de validation externe et de validation croisée (10 groupes) pour les 40 premiers modèles de la procédure descendante, selon le nombre de termes inclus dans la régression linéaire. Les traitillés verticaux indiquent le nombre de terme du modèle avec la meilleure valeur du critère pour chaque méthode.\n\n\n\n\nOn peut voir sur la Figure 4.7 l’historique des valeurs de AIC et BIC à mesure qu’on augmente le nombre de variables dans le modèle obtenu par une procédure séquentielle: les mêmes variables sont enlevées à chaque étape, mais la valeur optimale du critère est différente pour la sélection finale. Sur l’axe des abscisses, j’ai ajouté l’erreur quadratique moyenne de l’échantillon de validation pour les clients avec ymontant positif. Cet exemple n’est pas réaliste puisqu’on regarde la solution, mais il permet de nous comparer et de voir à quel point ici le critère d’information bayésien suit la même tendance que l’erreur quadratique moyenne de validation. L’erreur quadratique moyenne obtenue par validation croisée est trop optimiste (mais aléatoire!), comme le AIC. Pour éviter le surentraînement dans une région où le critère est quasi constant, on peut utiliser la règle d’une erreur-type. Puisque on a plusieurs réplications, on peut estimer ce dernier avec la validation croisée en même temps que l’EQM et choisir le modèle le plus simple à distance une erreur-type du modèle avec la plus petite erreur de validation croisée.\n\n\n4.5.3 Méthodes de régression avec régularisation\nUne façon d’éviter le surajustement est d’ajouter une pénalité sur les coefficients: ce faisant, on introduit un biais dans nos estimés, mais dans l’espoir de réduire leur variabilité et ainsi d’obtenir une meilleur erreur quadratique moyenne.\nL’avantage des moindres carrés est que les valeurs ajustées et les prédictions ne changent pas si on fait une transformation affine (de type \\(Z = aX+b\\)). Peu importe le choix d’unité (par exemple, exprimer une distance en centimètres plutôt qu’en mètres, ou la température en Farenheit plutôt qu’en Celcius), on obtient le même ajustement. En revanche, une fois qu’on introduit un terme de pénalité, notre solution dépendra de l’unité de mesure, d’où l’importance d’utiliser les données centrées et réduites pour que la solution reste la même.\nLes estimateurs des moindres carrés ordinaires pour la régression linéaire représentent la combinaison qui minimise la somme du carré des erreurs, \\[\\begin{align*}\n\\mathsf{SCE} = \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^pX_{ij}\\beta_{j}\\right)^2.\n\\end{align*}\\] On peut ajouter à cette fonction objective \\(\\mathsf{SCE}\\) un terme additionnel de pénalité qui va contraindre les paramètres à ne pas être trop grand. On considère une pénalité additionnelle pour la valeur absolue des coefficients, \\[\\begin{align*}\nq_1(\\lambda) = \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\end{align*}\\] Pour chaque valeur de \\(\\lambda\\) donnée, on obtiendra une solution différente pour les estimés car on minimisera désormais \\(\\mathsf{SCE} + q_1(\\lambda)\\). On ne pénalise pas l’ordonnée à l’origine \\(\\beta_0\\), parce que ce coefficient sert à recentrer les observations et a une signification particulière: si on standardise les données, de manière à ce que leur moyenne empirique soit zéro et leur écart-type un, alors \\(\\widehat{\\beta}_0 = \\overline{y}\\).\nLa pénalité \\(q_1(\\lambda)\\) a un rôle particulier parce qu’elle a deux effets: elle réduit la taille des paramètres, mais elle force également certains paramètres très proches de zéro à être exactement égaux à zéro, ce qui fait que la régression pénalité agit également comme outil de sélection de variables. Des algorithmes efficaces permettent de trouver la solution du problème d’optimisation \\[\\begin{align*}\n\\min_{\\boldsymbol{\\beta}} \\{\\mathsf{SCE} + q_1(\\lambda)\\} = \\min_{\\boldsymbol{\\beta}}  \\left\\{\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^pX_{ij}\\beta_{j}\\right)^2 +\n\\lambda \\sum_{j=1}^p |\\beta_j|\\right\\}\n\\end{align*}\\] laquelle est appelée LASSO. La Figure 4.8 montre la fonction objective dans le cas où on a deux paramètres, \\(\\beta_1\\) et \\(\\beta_2\\). La solution des moindres carrés ordinaires, qui minimisent l’erreur quadratique moyenne, est au centre des ellipses de contour et correspond à la solution du modèle avec \\(\\lambda=0\\). À mesure que l’on augmente la pénalité \\(\\lambda\\), les coefficients rétrécissent vers \\((0, 0)\\). On peut interpréter la pénalité \\(l_1\\) comme une contraire budgétaire: les coefficients estimés pour une valeur de \\(\\lambda\\) donnée sont ceux qui minimisent la somme du carré des erreurs, mais doivent être à l’intérieur d’un budget alloué (losange). La forme de la région fait en sorte que la solution, qui se trouve sur la bordure du losange, intervient dans un coin avec certaines coordonnées nulles.\n\n\n\n\n\nFigure 4.8: Courbes de contour du critère de l’erreur quadratique moyenne (ellipses) et fonction de pénalité (losanges) pour différentes valeurs de \\(\\lambda\\). Les points dénotent des solutions différentes et intersectent les contours du losange.\n\n\n\n\nPlusieurs variantes existent dans la littérature qui généralisent le modèle à des contextes plus compliqués. Le choix des variables à inclure dans la sélection dépend du choix de la pénalité \\(\\lambda\\), qui est règle générale estimée par validation croisée à cinq ou 10 groupes.\n\n# Sélection par LASSO\nlibrary(glmnet)\n# Paramètre de pénalité déterminé par \n# validation croisée à partir d'un vecteur\n# de valeurs candidates\nlambda_seq <- seq(from = 0.1,\n                  to = 10, \n                  by = 0.01)\ncv_output <- \n  glmnet::cv.glmnet(x = as.matrix(dbm_a[,1:10]), \n            y = dbm_a$ymontant, \n            alpha = 1, \n            lambda = lambda_seq)\nplot(cv_output)\n\n# On réestime le modèle avec la pénalité\nlambopt <- cv_output$lambda.min # ou lambda.1se\nlasso_best <- \n  glmnet::glmnet(\n    x = as.matrix(dbm_a[,1:10]),\n    y = dbm_a$ymontant,\n    alpha = 1, \n    lambda = lambopt)\n# Prédictions et calcul de l'EQM\n# On pourrait remplacer `newx` par \n# d'autres données (validation externe)\npred <- predict(lasso_best, \n                s = lambopt, \n                newx = as.matrix(dbm_a[,-1]))\neqm_lasso <- mean((pred - dbm_a$ymontant)^2)\n\nLe graphique de la Figure 4.9 montre l’évolution de l’erreur quadratique moyenne estimée en fonction du logarithme naturel de la pénalité (axe des abscisses). Comme plusieurs pénalités sont dans la marge d’erreurs, on choisit le première modèle à un erreur-type de la valeur minimale.\n\n\n\n\n\nFigure 4.9: Estimation de l’erreur quadratique moyenne (validation croisée à 10 groupes) pour les modèles avec pénalité LASSO en fonction de la pénalité (échelle log).\n\n\n\n\n\n\n4.5.4 Moyenne de modèles\nIl est souvent préférable de combiner plusieurs modèles plutôt que d’en choisir un seul. La technique des forêts aléatoires (random forests) est une des meilleures techniques de prédiction disponibles de nos jours. Elle est basée sur cette idée, en combinant plusieurs arbres de classification (ou de régression) individuels. C’est une des techniques de base en exploitation de données.\nIci, nous allons voir comment cette idée peut être appliquée à notre contexte. Toutes les méthodes que nous avons vues jusqu’à maintenant font une sélection « rigide » de variables, dans le sens que chaque variable est soit sélectionnée pour faire partie du modèle, soit elle ne l’est pas. C’est donc tout ou rien pour chaque variable. Il y a beaucoup de variabilité associée à une telle forme de sélection. Une variable peut avoir été très près d’être choisie, mais elle ne l’a pas été et est éliminée complètement. Construire plusieurs modèles et en faire la moyenne permet d’adoucir le processus de sélection car une variable peut alors être partiellement sélectionnée.\nSupposons qu’on dispose de deux échantillons et qu’on fasse une sélection de variables séparément pour les deux échantillons, avec l’une des approches que nous avons vues jusqu’à maintenant. Il est alors très probable qu’on ne va pas avoir exactement les mêmes variables sélectionnées pour les deux échantillons. Supposons ensuite qu’on fasse la moyenne des coefficients pour les deux modèles. Si une variable, disons \\(X_1\\), a été choisie les deux fois, alors la moyenne des deux coefficients devrait estimer en quelque sorte un effet global pour cette variable. Si une autre variable, disons \\(X_2\\), n’a pas été choisie du tout pour les deux échantillons, alors la moyenne de ses deux coefficients est nulle. Mais si une variable, disons, \\(X_3\\), a été choisie pour seulement l’un des deux échantillons, alors la moyenne de ses deux coefficients est la moitié du coefficient pour le modèle dans lequel elle a été choisie (car l’autre est zéro). Ainsi, cette variable est donc représentée par une « moitié » d’effet dans la moyenne des modèles. Donc au lieu d’être totalement là ou totalement absente, elle est présente en fonction de sa probabilité d’être sélectionnée. Ceci diminue de beaucoup la variabilité engendrée par une sélection « rigide » de variables et permet souvent de produire un modèle fort raisonnable.\nLe problème est que l’on n’a pas plusieurs échantillons mais un seul. Une solution possible est de générer nous-mêmes des échantillons différents à partir de l‘échantillon original. Cela peut être fait avec l’autoamorçage (bootstrap). Un échantillon d’autoamorçage est tout simplement un échantillon choisi au hasard et avec remise dans l’échantillon original. Ainsi, une même observation peut être sélectionnée plus d’une fois tandis qu’une autre peut ne pas être sélectionnée du tout.\nL’idée est alors la suivante :\n\nGénérer plusieurs échantillons par autoamorçage nonparamétrique à partir de l‘échantillon original.\nFaire une sélection de variables pour chaque échantillon.\nFaire la moyenne des paramètres de ces modèles.\n\n\n# Moyenne de modèles\nmoyenne_modeles <- function(\n    data, \n    form, \n    aic = FALSE, \n    B = 100L,\n    ks = 2){\n  B <- as.integer(B)\n  stopifnot(is.logical(aic),\n            length(aic) == 1L,\n            inherits(form, \"formula\"),\n            B > 1,\n            ks >= 0,\n            inherits(data, \"data.frame\"))\n  N <- nrow(data)\n  # Faire une expansion pour obtenir colonnes\n  matmod <- model.matrix(form, data = data)\n  # Nombre de variables explicatives\n  p <- ncol(matmod) - 1L\n  # Formule du modèle complet\n  fmod <- formula(paste0(\"y ~\", paste0(\"x\", seq_len(p), collapse = \"+\")))\n  # Sauvegarder les noms\n  noms <- colnames(matmod)\n  xnoms <- paste0(\"x\", seq_len(p))\n  # Extraire le nom de la variable réponse\n  nom_reponse <- all.vars(form)[attr(terms(form), \"response\")]\n  # Créer une base de données avec la réponse\n  # moins l'ordonnée à l'origine\n  matmod <- data.frame(cbind(\n    y = get(nom_reponse, data), \n    matmod[,-1]))\n  colnames(matmod) <- c(\"y\", xnoms)\n    # Contenant pour params/ nb de sélections\n  params <- nselect <- rep(0, p + 1)\n  names(params) <- \n    names(nselect) <- \n    c(\"(Intercept)\", xnoms)\n\n  # Boucle\n  for(b in seq_len(B)){\n    # Procédure de sélection avec AIC ou BIC\n  modselect <- MASS::stepAIC(\n    # Valeurs de départ\n    object = lm(formula = y ~ 1,\n   # Rééchantillonner données (avec remplacement)\n       data = matmod[sample.int(n = N, \n                             size = N, \n                             replace = TRUE),]),\n    # Modèle maximal additif considéré\n    scope = fmod, \n    # pénalité pour critère d'information\n    # k = ifelse(aic, 2, log(N)), \n    direction = \"both\", \n    trace = FALSE,\n   keep = function(mod, AIC, ...){ \n      # autres sorties des modèles à conserver\n      list(IC = AIC(mod, k = ifelse(aic, 2, log(N))),\n           coef = coef(mod))},\n   k = ks)\n  min_IC <- which.min(unlist(modselect$keep['IC',]))\n  coefsv <- modselect$keep[2,min_IC]$coef\n  # Trouver quelles colonnes représentent\n  #  un coefficient non-nul\n  colind <- match(names(coefsv),\n                  names(params))\n  # Incrémenter paramètres non-nuls\n  params[colind] <- params[colind] +\n    as.numeric(coefsv)\n  nselect[colind] <- nselect[colind] + 1L\n  }\n  \n  names(nselect) <- noms\n  names(params) <- noms\n  return(list(coefs = params / B,\n              nselect = nselect[-1] / B))\n}\n\n\n# Moyenne de modèles\n#  procédure séquentielle ascendante (AIC) \n#  sélection de modèle selon BIC\nmmodeles <- \n  moyenne_modeles(\n    data = dbm_a, \n    form = formule,\n    B = 10L,\n    aic = FALSE)\n\n# Proportion des variables \n# sélectionnées dans plus d'un modèle\nsum(mmodeles$nselect > 0)\n# Nombre moyen de coefficients\nsum(mmodeles$nselect)\n# variables retenues plus de 20% du temps\nnames(which(mmodeles$nselect > 0.2))\n# moyenne des coefficients\nmmodeles$coefs\n\nChaque modèle est construit à l’aide d’un échantillon aléatoire avec remise. Utilisez set.seed pour fixer le générateur de nombre aléatoire et permettre la reproductibilité\nToutes les méthodes employées jusqu’à maintenant utilise une méthode de pénalisation pour déterminer le meilleur modèle. Une alternative avec serait de répéter la sélection en utilisant directement l’erreur quadratique moyenne estimée à l’aide de la validation croisée comme critère de sélection: pour cela, il faudrait ajuster l’ensemble des modèles candidats retournés par une procédure exhaustive ou séquentielle."
  },
  {
    "objectID": "04-selectionmodeles.html#évaluation-de-la-performance",
    "href": "04-selectionmodeles.html#évaluation-de-la-performance",
    "title": "4  Sélection de variables et de modèles",
    "section": "4.6 Évaluation de la performance",
    "text": "4.6 Évaluation de la performance\nLa direction de la compagnie a décidé de passer outre vos recommandations et d’envoyer le catalogue aux 100 000 clients restants; nous pouvons donc faire un post-mortem afin de voir ce que chaque modèle aurait donné comme profit, comparativement à la stratégie de référence. Les 100 000 autres clients serviront d’échantillon de validation pour évaluer la performance des modèles et, plus précisément, afin d’évaluer les revenus (ou d’autres mesures de performance) si ces modèles avaient été utilisés. L’échantillon de validation nous donnera donc l’heure juste quant aux mérites des différentes approches que nous allons comparer. En pratique, nous ne pourrions pas faire cela car la valeur de la variable cible ne serait pas connue pour ces clients et nous utiliserions plutôt les modèles pour obtenir des prédictions pour déterminer quels clients cibler avec l’envoi. Parmi, les 100 000 clients restants, il y en a 23 179 qui auraient acheté quelque chose si on leur avait envoyé le catalogue. Ces 23 179 observations vont nous servir pour estimer l’erreur quadratique moyenne (théorique) des modèles retenus par nos critères.\nCommençons par l’estimation de l’erreur quadratique moyenne (moyenne des carrés des erreurs) pour les deux modèles retenus par le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\) avec les variables de base. Le Tableau 4.9 contient aussi l’estimation de l’erreur quadratique moyenne si on utilise toutes les variables (14 en incluant les indicatrices) sans faire de sélection. On voit que le modèle choisi par le \\(\\mathsf{BIC}\\) est le meilleur des trois. Ces deux méthodes font mieux que le modèle qui inclut toutes les variables sans faire de sélection, mais nous verrons que leur performance est exécrable: les variables de base ne permettent pas de capturer les effets présents dans les données et ce manque de flexibilité coûte cher.\n\n\nTableau 4.9: Estimation de l’erreur quadratique moyenne sur l’échantillon test avec les variables de base. Les meilleurs modèles selon les critères d’informations découlent d’une recherche exhaustive de tous les sous-ensembles.\n\n\n\n\n\n\n\nnombre de variables\n\\(\\mathsf{EQM}\\)\nméthode\n\n\n\n\n15\n25.69\ntoutes les variables\n\n\n12\n25.53\nexhaustive - \\(\\mathsf{AIC}\\)\n\n\n10\n25.04\nexhaustive - \\(\\mathsf{BIC}\\)\n\n\n\n\n\n\nTableau 4.10: Comparaison des méthodes selon l’erreur quadratique moyenne avec les variables de base, les interactions et les termes quadratiques.\n\n\nnombre de variables\n\\(\\mathsf{EQM}\\)\nméthode\n\n\n\n\n104\n19.63\ntoutes les variables\n\n\n21\n12\nséquentielle ascendante, choix selon \\(\\mathsf{AIC}\\)\n\n\n15\n12.31\nséquentielle ascendante, choix selon \\(\\mathsf{BIC}\\)\n\n\n23\n12.75\nséquentielle ascendante avec critère \\(\\mathsf{AIC}\\)\n\n\n20\n12.4\nséquentielle descendante avec critère \\(\\mathsf{BIC}\\)\n\n\n30\n12\nLASSO, validation croisée avec 10 groupes\n\n\n\n\nLe Tableau 4.10 présente la performance de toutes les méthodes avec les autres variables. On voit d’abord qu’utiliser toutes les 104 variables sans faire de sélection fait mieux (\\(\\mathsf{EQM}\\) de 19.63) que les modèles précédents basés sur les 10 variables originales. Mais faire une sélection permet une amélioration très importante de la performance (\\(\\mathsf{EQM}\\) jusqu’à 12 dans l’exemple). Utiliser les 104 variables mène à du surajustement (over-fitting).\nLes méthodes séquentielles avec un critère d’information (qui pénalisent davantage que les tests d’hypothèse classique) mènent à des modèles plus parcimonieux qui ont une erreur quadratique moyenne de validation plus faible. Le LASSO performe très bien dans ce cas de figure. Les coefficients sont tous rétrécis vers zéro (donc le nombre de coefficients non-nuls n’est pas évocateur), ce qui engendre du biais et peut affecter négativement la performance si le rapport signal-bruit est élevé.\nIl faut bien comprendre qu’il ne s’agit que d’un seul exemple: il ne faut surtout pas conclure que la méthode séquentielle sera toujours la meilleure. En fait, il est impossible de prévoir quelle méthode donnera les meilleurs résultats.\nIl y aurait plusieurs autres approches/combinaisons qui pourraient être testées. Le but de ce chapitre était simplement de présenter les principes de base en sélection de modèles et de variables ainsi que certaines approches pratiques. Il y a d’autres approches intéressantes, tels le filet élastique. Ces méthodes sont dans la même mouvance moderne que celle qui consiste à faire la moyenne de plusieurs modèles, en performant à la fois une sélection de variables et en permettant d’avoir des parties d’effet par le rétrécissement (shrinkage). De récents développements théoriques permettent aussi de corriger les valeurs-p pour faire de l’inférence post-sélection avec le LASSO.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nEn présence de nombreuses variables explicatives, choisir un modèle prédictif est compliqué: le nombre de modèles possibles augmente rapidement avec le nombre de prédicteurs, \\(p\\).\nSi un modèle est mal spécifié (variables importantes manquantes), alors les estimations sont biaisées. Si le modèle est surspécifié, les coefficients correspondants aux variables superflues incluses sont en moyenne nuls, mais contribuent à l’augmentation de la variance (compromis biais/variance).\nLa taille du modèle (\\(p\\), le nombre de variables explicatives) est restreinte par le nombre d’observations disponibles, \\(n\\).\n\nEn général, il faut s’assurer d’avoir suffisamment d’observations pour estimer de manière fiable les coefficients (le rapport \\(n/p\\) donne le budget moyen par paramètre).\nPorter une attention particulière aux variables binaires et aux interactions avec ces dernières: si les effectifs de certaines modalités sont faibles, il y a possible de surajustement.\n\nLe principal critère pour juger de la qualité d’un modèle linéaire est l’erreur quadratique moyenne.\n\nL’estimation de l’erreur quadratique moyenne obtenue à partir de l’échantillon d’apprentissage (qui sert à estimer les paramètres) est trompeuse et mène au surajustement:\nplus le modèle est compliqué, plus cette erreur décroît.\ncette performance n’est pas répétée sur de nouvelles données.\n\nCritères de sélection: Plusieurs stratégies existent pour pallier à cet excès d’optimisme\n\nvalidation externe: diviser le jeu de données aléatoirement au préalable en deux ou trois. Nécessite une grande base de données, potentiellement sous-optimal.\nvalidation croisée: diviser aléatoirement le jeu de données en plis et varier les échantillons d’apprentissage en conservant un pli en réserve à chaque fois comme validation. Plus coûteux en calcul (il faut réajuster plusieurs fois les modèles), applicable avec des petites bases de données.\npénalisation a posteriori: ajouter une pénalité fonction du nombre de paramètres qui compense pour l’augmentation constante de l’ajustement (par ex., critères d’information).\nrétrécissement des coefficients: inclure dans la fonction objective qui est maximisée une pénalité qui contraint les paramètres et les force à demeurer petit. Cela introduit du biais pour réduire la variance.\nUne pénalité particulière (LASSO) contraint certains paramètres à être exactement nuls, ce qui correspond implicitement à une sélection de variables.\n\nEn pratique, on cherche à essayer plusieurs modèle pour trouver un choix optimal de variables.\n\nUne recherche exhaustive garantie le survol du plus grand nombre de modèles possibles, mais est coûteuse et limitée à moins de 50 variables.\nLes algorithmes gloutons de recherche séquentielle sont sous-optimaux, mais rapides\n\nOn applique le critère de sélection sur la liste de modèles candidats pour retenir celui qui donne la meilleure performance.\nPour éviter une sélection rigide, on peut perturber les données et répéter la procédure pour calculer une moyenne de modèles. Cette approche est très coûteuse en calcul."
  },
  {
    "objectID": "05-reglogistique.html#introduction",
    "href": "05-reglogistique.html#introduction",
    "title": "5  Régression logistique",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nEn régression linéaire, on cherche à expliquer le comportement d’une variable quantitative \\(Y\\) que l’on peut traiter comme étant continue (elle peut prendre suffisamment de valeurs différentes).\nSupposons à présent que l’on veut expliquer le comportement d’une variable \\(Y\\) prenant seulement deux valeurs que l’on va noter 0 et 1.\nExemples :\n\nEst-ce qu’un client potentiel va répondre favorablement à une offre promotionnelle?\nEst-ce qu’un client est satisfait du service après-vente?\nEst-ce qu’un client va faire faillite ou non au cours des trois prochaines années.\n\nEn général, on cherchera à expliquer le comportement d’une variable binaire \\(Y\\) en utilisant un modèle basé sur \\(p\\) variables explicatives \\(\\mathrm{X}_1, \\ldots, \\mathrm{X}_p\\).\nNotre but sera de faire de l’inférence, de la prédiction, ou les deux à la fois, soit\n\nInférence : comprendre comment et dans quelles mesures les variables \\(\\mathbf{X}\\) influencent \\(Y\\) (ou bien la probabilité que \\(Y=1\\)).\nPrédiction : développer un modèle pour prévoir des valeurs de \\(Y\\) futures à partir des variables \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "05-reglogistique.html#modèle-de-régression-logistique",
    "href": "05-reglogistique.html#modèle-de-régression-logistique",
    "title": "5  Régression logistique",
    "section": "5.2 Modèle de régression logistique",
    "text": "5.2 Modèle de régression logistique\nAvec une variable réponse continue, le modèle de régression linéaire, \\[\\begin{align*}\nY = \\beta_0 + \\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p \\mathrm{X}_p + \\varepsilon,\n\\end{align*}\\] avec \\(\\mathsf{E}(\\varepsilon \\mid \\mathbf{X})=0\\) et \\(\\mathsf{Va}(\\varepsilon \\mid \\mathbf{X})=\\sigma^2\\), peut être écrit de manière équivalente comme \\(\\mathsf{E}(Y \\mid \\mathbf{X}) = \\beta_0 + \\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p\\mathrm{X}_p\\) et \\(\\mathsf{Va}(Y \\mid \\mathbf{X})=\\sigma^2.\\)\nSi \\(Y\\) est binaire (0/1), on peut facilement vérifier que \\[\\begin{align*}\n\\mathsf{E}(Y \\mid \\mathbf{X}) = \\Pr(Y=1 \\mid  \\mathbf{X}),\n\\end{align*}\\] soit la probabilité que \\(Y\\) égale 1 étant donné les valeurs des variables explicatives. Pour simplifier la notation, posons la probabilité de succès \\(p = \\Pr(Y=1 \\mid \\mathbf{X})\\) en se rappelant que \\(p\\) est une fonction des variables explicatives.\nÀ première vue, on peut se demander pourquoi ne pas utiliser le même modèle que la régression linéaire, c’est-à-dire \\[\\begin{align*}\n\\eta=\\beta_0 + \\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p \\mathrm{X}_p.\n\\end{align*}\\]\n\n\n\n\n\nFigure 5.1: Données de la réserve de Boston sur l’approbation de prêts hypothécaires (1990); données tirées de Stock et Watson (2007).\n\n\n\n\nLa Figure 5.1 montre le modèle de régression linéaire (bleu) et le modèle logistique. La pente pour la ligne bleu correspond à l’augmentation (réputée constante) de la probabilité d’approbation de crédit, de l’ordre de 11% par augmentation de 0.1 du rapport paiements hypothécaires sur revenu.\nIl y a quelques problèmes avec le modèle linéaire. D’abord, les données binaires ne respectent pas le postulat d’égalité des variances, ce qui rend les tests d’hypothèses caducs. Le problème principal est que \\(p\\) est une probabilité. Par conséquent \\(p\\) prend seulement des valeurs entre 0 et 1 alors que rien n’empêche \\(\\eta\\) de prendre des valeurs dans \\(\\mathbb{R}=(-\\infty, \\infty)\\): par exemple, on voit que la droite de la Figure 5.1 retourne des prédictions négatives dès que le ratio paiements/revenus est en dessous de 0.094: on peut évidemment tronquer ces prédictions à zéro, mais cela sous-tend que la probabilité d’acceptation est nulle, alors que certaines personnes dans l’échantillon ont reçu un prêt.\nUne façon de résoudre ce problème consiste à appliquer une transformation à \\(p\\) de telle sorte que la quantité transformée puisse prendre toutes les valeurs entre \\(-\\infty\\) et \\(\\infty\\). Le modèle de régression logistique est défini à l’aide de la transformation \\(\\textrm{logit}\\), \\[\\begin{align*}\n\\textrm{logit}(p) = \\ln\\left( \\frac{p}{1-p}\\right)=\\eta=\\beta_0 + \\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p \\mathrm{X}_p,\n\\end{align*}\\] où \\(\\ln\\) est le logarithme naturel.\nEn régression linéaire, on suppose que l’espérance de \\(Y\\) étant donné les valeurs des variables explicatives est une combinaison linéaire de ces dernières. En régression logistique, on suppose que le logit de la probabilité de succès est une combinaison linéaire des variables explicatives.\nUne simple manipulation algébrique permet d’exprimer ce modèle en terme de la probabilité \\(p\\), \\[\\begin{align*}\np &= \\textrm{expit}(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\n= \\frac{1}{1+\\exp(-\\eta)}.\n\\end{align*}\\] On peut voir qu’à mesure que le prédicteur linéaire \\(\\eta=\\beta_0+\\beta_1\\mathrm{X}_1 + \\cdots + \\beta_p\\mathrm{X}_p\\) augmente, la probabilité augmente. Si le coefficient \\(\\beta_j\\) est négatif, \\(p\\) diminuera à mesure que \\(\\mathrm{X}_j\\) augmente.\n\n\n\n\n\nFigure 5.2: Valeurs ajustées du modèle de régression logistique en fonction du prédicteur linéaire \\(\\eta\\).\n\n\n\n\nPour une variable binaire \\(Y\\), le rapport \\(p/(1-p)\\) est appelé cote et représente le ratio de la probabilité de succès (\\(Y=1\\)) sur la probabilité d’échec (\\(Y=0\\)), \\[\\begin{align*}\n\\mathsf{cote}(p) = \\frac{p}{1-p} = \\frac{\\Pr(Y=1 \\mid \\mathbf{X})}{\\Pr(Y=0 \\mid \\mathbf{X})}.\n\\end{align*}\\]\nPar exemple, une cote de 4 veut dire qu’il y a 4 fois plus de chance que \\(Y\\) soit égale à \\(1\\) par rapport à \\(0\\). Une cote de 0.25 veut dire le contraire, il y a 4 fois moins de chance que \\(Y=1\\) par rapport à \\(0\\) ou bien, de manière équivalente, il y a 4 fois plus de chance que \\(Y=0\\) par rapport à \\(1\\). Le Tableau 5.1 donne un aperçu de cotes pour quelques probabilités \\(p\\).\n\n\n\n\nTableau 5.1:  Cote et probabilité de succès \n \n  \n    \\(\\Pr(Y=1)\\) \n    0.1 \n    0.2 \n    0.3 \n    0.4 \n    0.5 \n    0.6 \n    0.7 \n    0.8 \n    0.9 \n  \n \n\n  \n    cote \n    0.11 \n    0.25 \n    0.43 \n    0.67 \n    1 \n    1.5 \n    2.3 \n    4 \n    9 \n  \n  \n     \n    \\(\\frac{1}{9}\\) \n    \\(\\frac{1}{4}\\) \n    \\(\\frac{3}{7}\\) \n    \\(\\frac{2}{3}\\) \n    \\(1\\) \n    \\(\\frac{3}{2}\\) \n    \\(\\frac{7}{3}\\) \n    \\(4\\) \n    \\(9\\) \n  \n\n\n\n\n\n\n\n5.2.1 Estimation et interprétation des paramètres\nSupposons qu’on dispose d’un échantillon de taille \\(n\\) sur les variables \\((Y, \\mathrm{X}_1, \\ldots, \\mathrm{X}_p)\\). À l’aide de ces observations, on peut estimer les paramètres \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1 ,\\ldots, \\beta_p)\\) du modèle de régression logistique \\[\\begin{align*}\n\\textrm{logit}(p) = \\ln \\left( \\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\mathrm{X}_1 + \\cdots + \\beta_p\\mathrm{X}_p.\n\\end{align*}\\] On obtient ainsi les estimés des paramètres \\(\\widehat{\\boldsymbol{\\beta}}\\), desquels découle une estimation de \\(\\Pr(Y=1)\\) pour les valeurs \\(\\mathrm{X}_1=x_1, \\ldots, \\mathrm{X}_p=x_p\\) d’un individu donné, \\[\\begin{align*}\n\\widehat{p} = \\textrm{expit}(\\widehat{\\beta}_0 + \\cdots + \\widehat{\\beta}_p\\mathrm{X}_p).\n\\end{align*}\\]\nUn modèle ajusté peut ensuite être utilisé pour faire de la classification (prédiction) pour de nouveaux individus pour lesquels la variable réponse \\(Y\\) n’est pas observée. Pour ce faire, on choisit un point de coupure \\(c\\) (souvent \\(c=0.5\\) mais pas toujours) et on classifie les observations en deux groupes:\n\nSi \\(\\widehat{p}< c\\), alors \\(\\widehat{Y}=0\\) (c’est-à-dire, on assigne cette observation à la catégorie 0 ou échec).\nSi \\(\\widehat{p} \\geq c\\), alors \\(\\widehat{Y}=1\\) (c’est-à-dire, on assigne cette observation à la catégorie 1 ou succès).\n\nOn reviendra en détail sur cet aspect dans une section suivante.\nLa méthode d’estimation des paramètres usuelle est la méthode du maximum de vraisemblance. Pour les applications, il est suffisant de savoir manipuler trois quantités importantes: la log-vraisemblance, le \\(\\mathsf{AIC}\\) et le \\(\\mathsf{BIC}\\). Les deux critères d’information, que nous avons couvert dans les chapitres précédents, servent à la sélection de modèles tandis que la log-vraisemblance \\(\\ell\\) servira à construire un test d’hypothèse.\n\n\n5.2.2 Méthode du maximum de vraisemblance\nCette sous-section est facultative. Elle donne plus de détails sur la méthode du maximum de vraisemblance et les quantités en découlant, soit \\(\\mathsf{AIC}\\), \\(\\mathsf{BIC}\\) et \\(\\ell(\\widehat{\\boldsymbol{\\beta}})\\).\nLa méthode du maximum de vraisemblance (maximum likelihood) est possiblement la méthode d’estimation la plus utilisée en statistique. En général, pour un échantillon donné et un modèle avec des paramètres inconnus \\(\\boldsymbol{\\theta}\\), on peut calculer la « probabilité » d’avoir obtenu les observations de notre échantillon selon les paramètre. Si on traite cette « probabilité » comme étant une fonction des paramètres du modèle, \\(\\boldsymbol{\\theta}\\), on l’appelle alors la vraisemblance (likelihood). La méthode du maximum de vraisemblance consiste à trouver les valeurs des paramètres qui maximisent la vraisemblance. On cherche donc les estimations qui sont les plus vraisemblables étant donné nos observations.\nEn pratique, il est habituellement plus simple de chercher à maximiser le log de la vraisemblance (ce qui revient au même car le logarithme naturel est une fonction croissante) et on nomme cette fonction la log-vraisemblance (log-likelihood).\nVous connaissez déjà des exemples d’estimateurs du maximum de vraisemblance. La moyenne d’un échantillon est l’estimateur du maximum de vraisemblance pour la moyenne de la population \\(\\mu\\) si les observations représentent un échantillon aléatoire simple tiré d’une loi normale.\nDans le cas d’un modèle de régression linéaire multiple de la forme \\(Y_i \\sim \\mathsf{No}(\\beta_0 + \\sum_{j=1}^p \\beta_j\\mathrm{X}_{ij}, \\sigma^2)\\) des termes indépendants et de même loi, la log-vraisemblance du modèle pour un échantillon de taille \\(n\\) est \\[\\begin{align*}\n\\ell(\\boldsymbol{\\beta}, \\sigma^2) =- \\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (Y_i- \\beta_0 - \\beta_1 \\mathrm{X}_{1i} - \\cdots - \\beta_p\\mathrm{X}_{ip})^2.\n\\end{align*}\\] Puisque le premier terme ne dépend pas des paramètres \\(\\boldsymbol{\\beta}\\), il est clair que maximiser cette fonction de \\(\\boldsymbol{\\beta}\\) revient à minimiser \\(\\sum_{i=1}^n (Y_i- \\beta_0 - \\beta_1 \\mathrm{X}_{1i} - \\cdots - \\beta_p\\mathrm{X}_{ip})^2\\): ce critère est exactement le même que celui des moindres carrés. Par conséquent, les estimations des paramètres \\(\\boldsymbol{\\beta}\\) provenant de la méthode des moindres carrés peuvent être vues comme étant des estimateurs du maximum de vraisemblance sous l’hypothèse de normalité et d’homoscédasticité des observations; il est même possible d’obtenir une formule explicite pour le calcul dest estimateurs.\nDans le cas de la régression logistique, la fonction de log-vraisemblance s’écrit \\[\\begin{align*}\n\\ell(\\boldsymbol{\\beta}) &= \\sum_{i=1}^n Y_i ( \\beta_0 + \\beta_1 \\mathrm{X}_{i1} + \\cdots + \\beta_p \\mathrm{X}_{ip}) \\\\&- \\sum_{i=1}^n \\ln\\left\\{1+\\exp(\\beta_0 + \\cdots + \\beta_p\\mathrm{X}_{ip})\\right\\}\n\\end{align*}\\]\nContrairement au cas de la régression linéaire, on ne peut trouver une solution explicite pour les valeurs des paramètres qui maximisent cette fonction. Des méthodes numériques doivent être utilisées pour l’optimisation. Une fois la maximisation accomplie, on obtient les estimés du maximum de vraisemblance, \\(\\widehat{\\boldsymbol{\\beta}}\\). On peut alors calculer la valeur maximale (numérique) de la log-vraisemblance, \\(\\ell(\\widehat{\\boldsymbol{\\beta}})\\). Par analogie avec la régression linéaire la valeur de la log-vraisemblance évaluée à \\(\\widehat{\\boldsymbol{\\beta}}\\), \\(\\ell(\\widehat{\\boldsymbol{\\beta}})\\), augmente toujours lorsqu’on ajoute des régresseurs et c’est pourquoi on ne pourra pas l’utiliser comme outil de sélection de variables.\nLes critères d’information sont des fonctions de la log-vraisemblance, mais incluent une pénalité pour le nombre de coefficients \\(\\boldsymbol{\\beta}\\), \\[\\begin{align*}\n\\mathsf{AIC} & = -2 \\ell(\\widehat{\\boldsymbol{\\beta}}) + 2(p+1)\\\\\n\\mathsf{BIC} & = -2 \\ell(\\widehat{\\boldsymbol{\\beta}}) + \\ln(n)(p+1)\n\\end{align*}\\]\nCes définitions sont utilisables dans plusieurs situations lorsque le modèle est ajusté par la méthode du maximum de vraisemblance. Tout comme en régression linéaire et en analyse factorielle, ces deux critères pourront être utilisés pour faire de la sélection de modèles si on calcule les estimateurs du maximum de vraisemblance.\n\n\n5.2.3 Exemple du Professional Rodeo Cowboys Association\nL’exemple suivant est inspiré de l’article\n\nDaneshvary, R. et Schwer, R. K. (2000) The Association Endorsement and Consumers’ Intention to Purchase. Journal of Consumer Marketing 17, 203-213.\n\nDans cet article, les auteurs cherchent à voir si le fait qu’un produit soit recommandé par le Professional Rodeo Cowboys Association (PRCA) a un effet sur les intentions d’achats. On dispose de 500 observations sur les variables suivantes:\n\n\\(Y\\): seriez-vous intéressé à acheter un produit recommandé par le PRCA\n\n\\(\\texttt{0}\\): non\n\\(\\texttt{1}\\): oui\n\n\\(\\mathrm{X}_1\\): quel genre d’emploi occupez-vous?\n\n\\(\\texttt{1}\\): à la maison\n\\(\\texttt{2}\\): employé\n\\(\\texttt{3}\\): ventes/services\n\\(\\texttt{4}\\): professionnel\n\\(\\texttt{5}\\): agriculture/ferme\n\n\\(\\mathrm{X}_2\\): revenu familial annuel\n\n\\(\\texttt{1}\\): moins de 25 000\n\\(\\texttt{2}\\): 25 000 à 39 999\n\\(\\texttt{3}\\): 40 000 à 59 999\n\\(\\texttt{4}\\): 60 000 à 79 999\n\\(\\texttt{5}\\): 80 000 et plus\n\n\\(\\mathrm{X}_3\\): sexe\n\n\\(\\texttt{0}\\): homme\n\\(\\texttt{1}\\): femme\n\n\\(\\mathrm{X}_4\\): avez-vous déjà fréquenté une université?\n\n\\(\\texttt{0}\\): non\n\\(\\texttt{1}\\): oui\n\n\\(\\mathrm{X}_5\\): âge (en années)\n\\(\\mathrm{X}_6\\): combien de fois avez-vous assisté à un rodéo au cours de la dernière année?\n\n\\(\\texttt{1}\\): 10 fois ou plus\n\\(\\texttt{2}\\): entre six et neuf fois\n\\(\\texttt{3}\\): cinq fois ou moins\n\n\nLe but est d’examiner les effets de ces variables sur l’intentions d’achat (\\(Y\\)). Les données se trouvent dans la base de données logit1.\n\n\n5.2.4 Modèle avec une seule variable explicative\nFaisons tout d’abord une analyse en utilisant seulement \\(\\mathrm{X}_5\\) (âge) comme variable explicative. L’ajustement du modèle de régression incluant uniquement \\(\\mathrm{X}_5\\) sera effectuée en exécutant le programme\n\ndata(logit1, package = \"hecmulti\")\n# Nombre d'observations par groupe\nwith(logit1, table(y))\n# Ajustement du modèle avec une\n#  seule variable explicative\nmodele1 <- glm(formula = y ~ x5,\n            family = binomial(link = \"logit\"),\n            data = logit1)\n# Tableau résumé avec coefficients\nsummary(modele1)\n# Cote\ncote <- exp(modele1$coefficients)\n# Intervalles de confiance profilés\n#  pour les paramètres betas\nconfbeta <- confint(modele1) \n# Intervalles de confiance pour la cote\nexp(confbeta) \n# Tester la significativité globale \n#  à l'aide du rapport de vraisemblance\nanova(modele1, test = 'Chisq')\n# Critères d'information\nnp <- length(coef(modele1))\nn <- nrow(logit1)\nAIC(modele1) \n# -2*logLik(modele1) + 2*np\nBIC(modele1) \n# -2*logLik(modele1) + log(n)*np\n\nPar défaut, pour des variables \\(0/1\\), le modèle décrit la probabilité de succès. On peut transformer la variable réponse en facteur (factor) et changer la catégorie de référence via relevel pour obtenir le modèle \\(\\Pr(Y=0 \\mid \\mathrm{X}_5)\\).\n\nnlogit1 <- logit1 |> \n  dplyr::mutate(y = relevel(factor(y), \"1\"))\nglm(formula = y ~ x5,\n    family = binomial(link = \"logit\"),\n    data = nlogit1)\n\nQuelques observations sur l’échantillon et les données:\n\nOn voit qu’il y a 272 personnes (\\(\\texttt{0}\\)) qui ne sont pas intéressées à acheter un produit recommandé par le PRCA et 228 personnes (\\(\\texttt{1}\\)) qui le sont.\nLes estimés des paramètres sont \\(\\widehat{\\beta}_0 = -3.05\\) et \\(\\widehat{\\beta}_{\\texttt{age}}=0.0749\\).\nUn intervalle de confiance de niveau 95% pour l’effet de l’âge est [\\(0.0465; 0.1043\\)].\nLe modèle ajusté est \\(\\textrm{logit}\\{\\Pr(Y=1 \\mid \\mathrm{X}_5=x_5)\\} = -3.05 + 0.0749 x_5\\). On peut également exprimer ce modèle directement en terme de la probabilité de succès, \\[\\begin{align*}\n\\Pr(Y=1 \\mid \\mathrm{X}_5=x_5) &= \\textrm{expit}(-3.05 + 0.0749 x_5) \\\\&= \\frac{1}{1+\\exp(-3.05 - 0.0749 x_5)}\n\\end{align*}\\] Le graphe de cette fonction (Figure 5.3) pour \\(\\mathrm{X}_5\\) allant de 18 à 59 ans, respectivement les valeurs minimales et maximales observées dans l’échantillon, montre que le lien entre l’âge et \\(p\\) est presque linéaire entre 20 et 60 ans. On décèle tout de même la forme sigmoide de la fonction \\(\\textrm{logit}\\) aux deux extrémités.\n\n\n\n\n\n\nFigure 5.3: Probabilité de suivre les recommendations selon l’âge.\n\n\n\n\n\nLa valeur-\\(p\\) pour \\(\\widehat{\\beta}_{\\texttt{age}}\\) correspondant au test des hypothèses \\(\\mathscr{H}_0: \\beta_{\\texttt{age}}=0\\) versus \\(\\mathscr{H}_1: \\beta_{\\texttt{age}} \\neq 0\\), est plus petite que \\(10^{-4}\\) et donc l’effet de la variable âge est statistiquement différent de zéro. Plus l’âge augmente, plus la probabilité d’être intéressé à acheter un produit recommandé par le PRCA augmente.\n\n\n\n5.2.5 Interprétation du paramètre\nSi une variable est modélisée à l’aide d’un seul paramètre (pas de terme quadratique et pas d’interaction avec d’autre covariables), une valeur positive du paramètre indique une association positive avec \\(p\\) alors qu’une valeur négative indique le contraire.\nAinsi, le signe du paramètre donne le sens de l’association. Si le coefficient \\(\\beta_j\\) de la variable \\(\\mathrm{X}_j\\) est positif, alors plus la variable augmente, plus \\(\\Pr(Y=1)\\) augmente. Inversement, si le coefficient \\(\\beta_j\\) est négatif, plus la variable augmente, plus \\(\\Pr(Y=1)\\) diminue.\nEn régression linéaire, l’interprétation de coefficient \\(\\beta_j\\) est simple: lorsque la variable \\(\\mathrm{X}_j\\) augmente de un, la variable \\(Y\\) augmente en moyenne de \\(\\beta_j\\), toute chose étant égale par ailleurs. Cette interprétation ne dépend pas de la valeur de \\(\\mathrm{X}_j\\). En régression logistique, comme le modèle est nonlinéaire en fonction de \\(\\Pr(Y=1)\\) (courbe sigmoide), l’augmentation ou la dimininution de \\(\\Pr(Y=1\\mid \\mathbf{X})\\) pour un changement d’une unité de \\(\\mathrm{X}_j\\) dépend de la valeur de cette dernière. C’est pourquoi il est parfois plus utile d’utiliser la cote pour interpréter globalement l’effet d’une variable.\nDans notre exemple, on peut exprimer le modèle ajusté en termes de cote, \\[\\begin{align*}\n\\frac{\\Pr(Y=1 \\mid \\mathrm{X}_5=x_5)}{\\Pr(Y=0 \\mid \\mathrm{X}_5=x_5)} = \\exp(-3.05)\\exp(0.0749x_5).\n\\end{align*}\\] Ainsi, lorsque \\(\\mathrm{X}_5\\) augmente d’une année, la cote est multipliée par \\(\\exp(0.0749) = 1.078\\) peut importe la valeur de \\(x_5\\). Pour deux personnes dont la différence d’âge est un an, la cote de la personne plus âgée est 7.8% plus élevée. On peut aussi quantifier l’effet d’une augmentation d’un nombre d’unités quelconque. Par exemple, pour chaque augmentation de 10 ans de \\(\\mathrm{X}_5\\), la cote est multiplié par \\(1.078^{10} = 2.12\\), soit une augmentation de 112%.\nL’interprétation des coefficients du modèle logistique se fait au niveau du rapport de cote, à savoir \\(\\exp(\\beta)\\).\nUn des avantages d’utiliser la vraisemblance comme fonction objective est que les intervalles de confiance et les estimateurs basés sur la vraisemblance (profilée) sont invariant aux reparamétrisations: l’intervalle de confiance à niveau 95% pour \\(\\exp(\\beta_{\\texttt{age}})\\) est obtenu en prenant l’exponentielle des bornes de l’intervalle pour \\(\\beta_{\\texttt{age}}\\), [\\(\\exp(0.0465); \\exp(0.1043)\\)], soit [\\(1.048; 1.110\\)] tel que rapporté dans la sortie. Ce n’est pas le cas des intervalles usuels de Wald qui ont la forme \\(\\widehat{\\beta} \\pm 1.96 \\mathrm{se}(\\widehat{\\beta})\\).\nComme l’exponentielle est une transformation monotone croissante, on a \\(\\beta>0\\) si et seulement si \\(\\exp(\\beta)>1\\), etc. On peut ainsi utiliser les intervalles de confiance pour tester l’hypothèse \\(\\mathscr{H}_0: \\beta_j=0\\) ou de façon équivalente \\(\\mathscr{H}_0: \\exp(\\beta_j)=1\\) à niveau 95%.\nAjustons à présent le modèle avec toutes les variables explicatives. Rappelez-vous que la variable \\(\\mathrm{X}_1\\) (quel genre d’emploi occupez-vous) a cinq catégories, \\(\\mathrm{X}_2\\) (revenu familial annuel) a cinq catégories, et \\(\\mathrm{X}_6\\) (combien de fois avez-vous assisté à un rodéo au cours de la dernière année) a trois catégories. Notez qu’on pourrait aussi traiter \\(\\mathrm{X}_2\\) comme continue car elle est ordinale et possède tout de même cinq modalités, mais on la traitera comme variable nominale.\nLes variables de type factor sont modélisées par défaut à l’aide d’un ensemble de variables indicatrices, la catégorie de référence étant celle qui apparaît en dernier en ordre alphabétique.\n\nstr(logit1)\nmodele2 <- glm(\n  y ~ x1 + x2 + x3 + x4 + x5 + x6,\n  data = logit1,\n  family = binomial(link = \"logit\")\n)\nsummary(modele2)\nic <- confint(modele2)\n# Tests de rapport de vraisemblance\ncar::Anova(modele2, type = \"3\")\n\n\n\n\n\n\n\nTableau 5.2:  Coefficients (cote), intervalles de confiance profilée de 95 pourcent et valeurs-p pour le test de rapport de vraisemblance pour le modèle logistique avec toutes les variables catégorielles. \n  \n  \n    \n      variables\n      cote1\n      IC 95%1\n      valeur-p\n    \n  \n  \n    x1\n\n\n0.4\n        1\n—\n—\n\n        2\n0.44\n0.18, 1.06\n\n        3\n0.51\n0.21, 1.21\n\n        4\n0.51\n0.21, 1.25\n\n        5\n0.70\n0.27, 1.80\n\n    x2\n\n\n<0.001\n        1\n—\n—\n\n        2\n0.83\n0.38, 1.82\n\n        3\n0.57\n0.25, 1.31\n\n        4\n0.09\n0.03, 0.25\n\n        5\n0.26\n0.08, 0.84\n\n    x3\n\n\n<0.001\n        0\n—\n—\n\n        1\n3.85\n2.34, 6.50\n\n    x4\n\n\n<0.001\n        0\n—\n—\n\n        1\n6.24\n3.53, 11.4\n\n    x5\n1.12\n1.08, 1.16\n<0.001\n    x6\n\n\n<0.001\n        1\n—\n—\n\n        2\n0.25\n0.13, 0.49\n\n        3\n0.09\n0.04, 0.18\n\n  \n  \n  \n    \n      1 cote = rapport de cote, IC = intervalle de confiance\n    \n  \n\n\n\n\n\n\n\n\n\n\nFigure 5.4: Intervalles de confiance profilés de niveau 95% pour les coefficients du modèle logistique (échelle exponentielle).\n\n\n\n\n\n\n\n\nTableau 5.3:  Mesures d’ajustement du modèle avec toutes les variables explicatives, et du modèle nul (pour lequelle la probabilité de succès est la proportion de 1). \n \n  \n      \n    AIC \n    BIC \n    log.vrais. \n  \n \n\n  \n    modèle ajusté \n    544.20 \n    603.20 \n    -258.1 \n  \n  \n    modèle nul \n    691.27 \n    695.48 \n    -258.1 \n  \n\n\n\n\n\n\nL’interprétation se fait comme en régression linéaire multiple puisqu’il n’y a pas ni terme quadratique, ni interaction. Les paramètres estimés représentent donc l’effet de la variable correspondante sur le logit une fois que les autres variables sont dans le modèle, et demeurent fixes.\nPrenons le coefficient associé à l’âge (\\(\\mathrm{X}_5\\)) comme exemple. Le paramètre estimé est \\(\\widehat{\\beta}_{\\texttt{age}}=0.109\\) et il est significativement différent de zéro. Ainsi, plus l’âge augmente, plus \\(\\Pr(Y=1\\mid \\mathbf{X})\\) augmente, toutes autres choses étant égales par ailleurs. Pour chaque augmentation d’un an de \\(\\mathrm{X}_5\\), la cote est multipliée par \\(\\exp(0.109)=1.116\\), lorsque les autres variables demeurent fixes.\nN’oubliez pas la nuance suivante concernant l’interprétation d’un test lorsque plusieurs variables explicatives font partie du modèle. Si un paramètre n’est pas significativement différent de zéro, cela ne veut pas dire qu’il n’y a pas de lien entre la variable correspondante et \\(Y\\). Cela veut seulement dire qu’il n’y a pas de lien significatif une fois que les autres variables sont dans le modèle.\nL’interprétation des variables catégorielles est analogue à celle faite en régression linéaire. On peut aussi interpréter individuellement les paramètres des indicatrices: pour \\(\\mathrm{I}(\\mathrm{X}_6=2)\\), lorsque les autres variables demeurent fixes, les personnes ayant assisté entre six et 10 fois à un rodéo au cours de la dernière année voient leur cote multipliée par \\(\\exp(-1.370)=0.255\\) par rapport aux personnes ayant assisté plus de di fois. Ce paramètre est significativement différent de zéro car sa valeur-\\(p\\) est négligeable; l’intervalle de confiance à 95% pour le rapport de cotes, basé sur la vraisemblance profilée, est [0.13; 0.49] et la valeur 1 (qui correspond à un rapport de cote constant) n’est pas dans l’intervalle. Ainsi, il y a une différence significative entre les gens qui ont assisté à 10 rodéos ou plus et les gens qui ont assisté à 5 rodéos ou moins, pour ce qui est de l’intérêt à acheter un produit recommandé par le PRCA.\nSi on désire comparer les deux modalités \\(\\mathrm{X}_6=2\\) et \\(\\mathrm{X}_6=3\\), il suffit de changer la modalité de référence de x6 et d’exécuter le modèle à nouveau. Une alternative est de calculer le rapport de cotes pour ces deux modalités.\n\n\n5.2.6 Test du rapport de vraisemblance\nLes tests rapportés d’ordinaire dans le tableau avec les coefficients (et correspondants aux valeurs-\\(p\\)) sont des tests de Wald, à savoir \\(W = \\widehat{\\beta}/\\widehat{\\mathsf{se}}(\\widehat{\\beta})\\). Ces tests feront l’affaire dans la plupart des applications. Par contre, il existe un autre test qui est généralement plus puissant, c’est-à-dire qu’il sera meilleur pour détecter que \\(\\mathscr{H}_0\\) n’est pas vraie lorsque c’est effectivement le cas. Ce test est le test du rapport de vraisemblance (likelihood ratio test). Il découle de la méthode d’estimation du maximum de vraisemblance et est donc généralement applicable lorsqu’on estime les paramètres avec cette méthode. Il est basé sur la quantité \\(\\ell\\) que nous avons vue plus tôt.\nLa procédure consiste à ajuster deux modèles emboîtés:\n\nLe premier modèle, le modèle complet, contient tous les paramètres et l’estimateur du maximum de vraisemblance \\(\\widehat{\\boldsymbol{\\beta}})\\).\nLe deuxième modèle correspondant à l’hypothèse nulle \\(\\mathscr{H}_0\\), le modèle réduit, contient tous les paramètres avec les restrictions imposées sous \\(\\mathscr{H}_0\\); on dénote l’estimateur du maximum de vraisemblance \\(\\widehat{\\boldsymbol{\\beta}}_0\\)\n\nLe test est basé sur la statistique \\[\\begin{align*}\nD = -2\\{\\ell(\\widehat{\\boldsymbol{\\beta}}_0)-\\ell(\\widehat{\\boldsymbol{\\beta}})\\}.\n\\end{align*}\\] Cette différence \\(D\\), lorsque l’hypothèse \\(\\mathscr{H}_0\\) est vraie suit approximativement une loi khi-deux avec un nombre de degrés de liberté égal au nombre de paramètre testé (le nombre de restrictions sous \\(\\mathscr{H}_0\\)). On peut donc calculer la valeur-\\(p\\) en utilisant la distribution du khi-deux.\nPrenons comme exemple le test de la significativité de \\(\\mathrm{X}_6\\), qui est modélisée à l’aide deux variables binaires et dont les paramètres correspondants sont \\(\\beta_{6_{\\texttt{2}}}\\) et \\(\\beta_{6_{\\texttt{3}}}\\). Pour effectuer le test du rapport de vraisemblance, il suffit de retirer la variable \\(\\mathrm{X}_6\\) et de réajuster le modèle à nouveau avec toutes les autres variables.\n\n# Ajuster modèle sous H0 (sans X6)\nmodeleH0 <- update(modele2, formula. =  \". ~ . - x6\")\nanova(modeleH0, modele2, test = \"LRT\")\n# Le modèle 'modeleH0' est équivalent à \n# glm(y ~ x1 + x2 + x3 + x4 + x5,\n#  data = logit1, \n#  family = binomial(link = \"logit\"))\n\n## Calculer statistique du test manuellement\n## Deviance = -2*log vraisemblance\nrvrais <- modeleH0$deviance - modele2$deviance\n# Valeur-p\npchisq(rvrais, df = 2, lower.tail = FALSE)\n\nConsidérons maintenant la variable \\(\\mathrm{X}_6\\), qui représente le nombre de fois où l’individu a assisté à un rodéo au cours de la dernière année. Cette variable est modélisée à l’aide de deux variables indicatrices, \\(\\mathrm{I}(\\mathrm{X}_6=2)\\) égale à un si \\(\\mathrm{X}_6=2\\) et zéro autrement, et \\(\\mathrm{I}(\\mathrm{X}_6=3)\\) égale à un si \\(\\mathrm{X}_6=3\\) et zéro sinon. La catégorie de référence est \\(\\mathrm{X}_6=1\\), c’est-à-dire les personnes ayant assisté plus de 10 fois à un rodéo au cours de la dernière année. Pour tester la significativité globale d’une variable catégorielle qui est modélisée avec plusieurs indicatrices, il faut utiliser un test qui compare l’ajustement du modèle avec ou sans toutes les variables binaires associées à \\(\\mathrm{X}_6\\); l’hypothèse nulle \\(\\mathscr{H}_0: \\beta_{6_{\\texttt{2}}}=\\beta_{6_{\\texttt{3}}}=0\\) versus l’alternative qu’au moins un de ces deux paramètres est différent de zéro. La statistique du test de rapport de vraisemblance \\(D\\) de 50.251 et la valeur-\\(p\\) peut-être obtenue de la loi du khi-deux avec 2 degrés de liberté via le code suivant permet d’imprimer la valeur-\\(p\\), qui est \\(1.22 \\times 10^{-11}\\).\n\n\n5.2.7 Multicolinéarité\nRappelez-vous que le terme multicolinéarité fait référence à la situation où les variables explicatives sont très corrélées entre elles ou bien, plus généralement, à la situation où une (ou plusieurs) variable(s) explicative(s) est (sont) très corrélée(s) à une combinaison linéaire des autres variables explicatives.\nL’effet potentiellement néfaste de la multicolinéarité est le même qu’en régression linéaire, c’est-à-dire, elle peut réduire la précision des estimations des paramètres (augmenter leurs écarts-types estimés).\nEn pratique, le problème est qu’il devient difficile de départager l’effet individuel d’une variable explicative lorsqu’elle est fortement corrélée avec d’autres variables explicatives.\nComme la multicolinéarité est une propriété des variables explicatives (le \\(Y\\) n’intervient pas) on peut utiliser les mêmes outils qu’en régression linéaire pour tenter de la détecter, par exemple, le facteur d’inflation de la variance (variance inflation factor), accessible via car::vif pour un modèle de régression. Cette quantité ne dépend que des variables explicatives \\(\\mathbf{X}\\), pas du modèle ou de la variable réponse.\nLa multicolinéarité est surtout un problème lorsque vient le temps d’interpréter et tester l’effet des paramètres individuels. Si le but est seulement de faire de la classification (prédiction) et que l’interprétation des paramètres individuels n’est pas cruciale alors il n’y a pas lieu de se soucier de la multicolinéarité. Il faut alors plutôt comparer correctement la performance de classification des modèles en utilisant des méthodes permettant d’obtenir un bon modèle tout en se protégeant contre le surajustement. Certaines de ces méthodes (division de l’échantillon, validation croisée) ont déjà été présentées.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nUne régression logistique sert à modéliser la moyenne de variables catégorielles, typiquement binaires.\nC’est un cas particulier d’un modèle de régression linéaire généralisée.\nLe modèle est interprétable à l’échelle de la cote, qui donne dans le cas binaire le rapport probabilité de réussite (1) sur probabilité d’échec (0)\nEn l’absence d’interactions, on interprète les coefficients en terme de pourcentage d’augmentation si \\(\\exp(\\widehat{\\beta}) > 1\\), avec \\(\\exp(\\widehat{\\beta})-1\\) ou en terme de pourcentage de diminution si \\(\\exp(\\widehat{\\beta}) < 1\\), avec \\(1-\\exp(\\widehat{\\beta})\\)\nL’estimation est faite par maximum de vraisemblance: on a accès aux critères d’information et aux tests d’hypothèses omnibus pour comparer des modèles emboîtés.\nLes intervalles de confiance de vraisemblance profilée rapportés sont invariants aux reparamétrisations."
  },
  {
    "objectID": "05-reglogistique.html#classification-et-prédiction",
    "href": "05-reglogistique.html#classification-et-prédiction",
    "title": "5  Régression logistique",
    "section": "5.3 Classification et prédiction",
    "text": "5.3 Classification et prédiction\nLa finalité du modèle de régression logistique est fréquemment l’obtention de prédictions. Une fois qu’on a ajusté un modèle, on peut l’utiliser pour prévoir la valeur de \\(Y\\) pour de nouvelles observations. Ceci consiste à assigner une classe (\\(0\\) ou \\(1\\)) à ces observations (pour lesquels \\(Y\\) est inconnue) à partir des valeurs prises par \\(\\mathrm{X}_1, \\ldots, \\mathrm{X}_p\\).\nLe modèle ajusté nous fournit une estimation de \\(\\Pr(Y=1 \\mid \\mathbf{X}=\\boldsymbol{x})\\) pour des valeurs \\(\\mathrm{X}_1=x_1, \\ldots, \\mathrm{X}_p=x_p\\) données. Cet estimé est \\[\\begin{align*}\n\\widehat{p} = \\frac{1}{1+ \\exp\\{- ( \\widehat{\\beta}_0 + \\widehat{\\beta}_1x_1 + \\cdots + \\widehat{\\beta}_p x_p)\\}}.\n\\end{align*}\\]\nClassification de base: pour classifier des observations, il suffit de choisir un point de coupure \\(c\\), souvent \\(c=0.5\\), et de classifier une observation de la manière suivante:\n\nSi \\(\\widehat{p} < c\\), on assigne cette observation à la catégorie zéro et \\(\\widehat{Y}=0\\).\nSi \\(\\widehat{p} \\geq c\\), on assigne cette observation à la catégorie un et \\(\\widehat{Y}=1\\).\n\nSi on prend \\(c=0.5\\) comme point de coupure, cela revient à assigner l’observation à la classe (catégorie) la plus probable, un choix fort raisonnable. Nous verrons dans une section suivante que, lorsque les conséquences de faussement classifier une observation (succès, mais échec prédit et vice-versa) ne sont pas les mêmes, il peut être avantageux d’utiliser un autre point de coupure.\nDans un cadre de prédiction, il nous faudra un critère pour juger de la qualité de l’ajustement du modèle. Rappelez-vous que pour une réponse continue, nous avons utilisé l’erreur quadratique moyenne, \\(\\mathsf{EQM} = \\mathsf{E}\\{(Y-\\widehat{Y})^2\\}\\), où \\(\\widehat{Y} = \\mathsf{E}(Y \\mid \\mathbf{X})\\), pour juger de la performance d’un modèle. Comme la réponse \\(Y\\) est binaire ici, nous allons utiliser des critères différents.\nVoyons d’abord un premier critère pour juger de la qualité d’un modèle de prédiction. Soit \\(Y\\) la vraie valeur de la réponse binaire et \\(\\widehat{Y}\\) (soit 0 ou 1) la valeur de \\(Y\\) prédite par un modèle pour une observation choisie au hasard dans la population. Un premier critère pour juger de la performance d’un modèle est le taux de mauvaise classification, un estimé de la probabilité de mal classifier une observation choisie au hasard dans la population, \\(\\Pr(Y \\neq\\widehat{Y})\\). Plus \\(\\Pr(Y \\neq\\widehat{Y})\\) est petite, meilleure est la capacité prédictive du modèle.\nTout comme l’erreur quadratique moyenne, on ne peut qu’estimer \\(\\Pr(Y \\neq\\widehat{Y})\\). Pour les raisons vues au chapitre précédent, l’estimer en calculant le taux de mauvaise classification des observations ayant servi à l’ajustement du modèle sans aucune correction n’est pas une bonne approche. Les approches couvertes dans le dernier chapitre pour l’estimation de l’erreur quadratique moyenne, telles la validation-croisée et la division de l’échantillon, peuvent être utilisées pour estimer le taux de mauvaise classification \\(\\Pr(Y \\neq \\widehat{Y})\\).\nCette utilisation d’un modèle de régression logistique sera illustrée avec l’exemple que nous avons traité au chapitre précédent: notre objectif final est de construire un modèle avec les 1000 clients de l’échantillon d’apprentissage et cibler ensuite lesquels des 100 000 clients restants seront choisis pour recevoir le catalogue. Les variables cibles sont:\n\nyachat: variable binaire égale à un si le client a acheté quelque chose dans le catalogue et zéro sinon.\nymontant: le montant de l’achat si le client a acheté quelque chose\n\nLes 10 variables suivantes sont disponibles pour tous les clients et serviront de variables explicatives,\n\nx1: sexe de l’individu, soit homme (0) ou femme (1);\nx2: l’âge (en année);\nx3: variable catégorielle indiquant le revenu, soit moins de 35 000$ (1), entre 35 000$ et 75 000$ (2) ou plus de 75 000$ (3);\nx4: variable catégorielle indiquant la région où habite le client (de 1 à 5);\nx5: conjoint : le client a-t-il un conjoint, soit oui (1) ou non (0);\nx6: nombre d’année depuis que le client est avec la compagnie;\nx7: nombre de semaines depuis le dernier achat;\nx8: montant (en dollars) du dernier achat;\nx9: montant total (en dollars) dépensé depuis un an;\nx10: nombre d’achats différents depuis un an.\n\nDans le chapitre précédent, nous avons cherché à développer un modèle pour prévoir ymontant, le montant dépensé, étant donné que le client achète quelque chose. Cette fois-ci, nous allons travailler avec la variable yachat, qui est binaire, à l’aide de la régression logistique.\nAfin d’introduire différentes notions, nous allons, dans un premier temps, utiliser les 10 variables de base. À partir de la section suivante, nous chercherons à optimiser le modèle en considérant les interactions d’ordre deux.\n\ndata(dbm, package = \"hecmulti\")\nformule <- formula(\"yachat ~ x1 + x2 + x3 +\n                x4 + x5 + x6 + x7 + x8 + x9 + x10\")\ndbm_class <- dbm |>\n  dplyr::filter(test == 0) |>\n  dplyr::mutate(yachat = factor(yachat))\nset.seed(202209)\npredprob <- hecmulti::predvc(\n  modele = glm(formula = formule, \n               data = dbm_class, \n               family = binomial),\n  K = 10, #nombre de plis\n  nrep = 1,\n  type = \"response\")\nclassif <- with(dbm, yachat[test == 0])\n# Tableau de la performance\nhecmulti::perfo_logistique(\n  prob = predprob,\n  resp = classif)\n\nLe modèle utilise seulement les 10 variables de base. Des prévisions pour les clients restants seront exportées dans le fichier. La méthode predict permet d’obtenir les prédictions des probabilités et la fonction maison hecmulti::perfo_logistique retourne un tableau de classification. Tel que nous l’avons vu au chapitre précédent, il y a 210 clients qui ont acheté quelque chose parmi les 1000.\n\n\n\n\n\nFigure 5.5: Répartition des probabilités de succès prédites par validation croisée à \\(n\\) groupes.\n\n\n\n\n\n\n\n\nTableau 5.4:  Tableau de classification avec le nombre de vrais positifs (VP), vrais négatifs (VN), faux positifs (FP) et faux négatifs (FN), le taux de bonne classification, la sensibilité, la spécificité, le taux de vrais positifs (TVP) et le taux de vrais négatifs (TVN). \n \n  \n    coupe \n    VP \n    VN \n    FP \n    FN \n    correct (%) \n    sensibilité (%) \n    spécificité (%) \n    TVP \n    TVN \n  \n \n\n  \n    0.02 \n    210 \n    209 \n    581 \n    0 \n    41.9 \n    100.0 \n    26.5 \n    26.5 \n    100.0 \n  \n  \n    0.04 \n    207 \n    320 \n    470 \n    3 \n    52.7 \n    98.6 \n    40.5 \n    30.6 \n    99.1 \n  \n  \n    0.06 \n    201 \n    398 \n    392 \n    9 \n    59.9 \n    95.7 \n    50.4 \n    33.9 \n    97.8 \n  \n  \n    0.08 \n    199 \n    451 \n    339 \n    11 \n    65.0 \n    94.8 \n    57.1 \n    37.0 \n    97.6 \n  \n  \n    0.10 \n    193 \n    480 \n    310 \n    17 \n    67.3 \n    91.9 \n    60.8 \n    38.4 \n    96.6 \n  \n  \n    0.12 \n    191 \n    512 \n    278 \n    19 \n    70.3 \n    91.0 \n    64.8 \n    40.7 \n    96.4 \n  \n  \n    0.14 \n    184 \n    547 \n    243 \n    26 \n    73.1 \n    87.6 \n    69.2 \n    43.1 \n    95.5 \n  \n  \n    0.16 \n    176 \n    572 \n    218 \n    34 \n    74.8 \n    83.8 \n    72.4 \n    44.7 \n    94.4 \n  \n  \n    0.18 \n    172 \n    598 \n    192 \n    38 \n    77.0 \n    81.9 \n    75.7 \n    47.3 \n    94.0 \n  \n  \n    0.20 \n    164 \n    611 \n    179 \n    46 \n    77.5 \n    78.1 \n    77.3 \n    47.8 \n    93.0 \n  \n  \n    0.22 \n    162 \n    626 \n    164 \n    48 \n    78.8 \n    77.1 \n    79.2 \n    49.7 \n    92.9 \n  \n  \n    0.24 \n    158 \n    639 \n    151 \n    52 \n    79.7 \n    75.2 \n    80.9 \n    51.1 \n    92.5 \n  \n  \n    0.26 \n    153 \n    645 \n    145 \n    57 \n    79.8 \n    72.9 \n    81.6 \n    51.3 \n    91.9 \n  \n  \n    0.28 \n    150 \n    657 \n    133 \n    60 \n    80.7 \n    71.4 \n    83.2 \n    53.0 \n    91.6 \n  \n  \n    0.30 \n    143 \n    667 \n    123 \n    67 \n    81.0 \n    68.1 \n    84.4 \n    53.8 \n    90.9 \n  \n  \n    0.32 \n    138 \n    679 \n    111 \n    72 \n    81.7 \n    65.7 \n    85.9 \n    55.4 \n    90.4 \n  \n  \n    0.34 \n    134 \n    695 \n    95 \n    76 \n    82.9 \n    63.8 \n    88.0 \n    58.5 \n    90.1 \n  \n  \n    0.36 \n    130 \n    699 \n    91 \n    80 \n    82.9 \n    61.9 \n    88.5 \n    58.8 \n    89.7 \n  \n  \n    0.38 \n    126 \n    708 \n    82 \n    84 \n    83.4 \n    60.0 \n    89.6 \n    60.6 \n    89.4 \n  \n  \n    0.40 \n    120 \n    715 \n    75 \n    90 \n    83.5 \n    57.1 \n    90.5 \n    61.5 \n    88.8 \n  \n  \n    0.42 \n    115 \n    723 \n    67 \n    95 \n    83.8 \n    54.8 \n    91.5 \n    63.2 \n    88.4 \n  \n  \n    0.44 \n    112 \n    731 \n    59 \n    98 \n    84.3 \n    53.3 \n    92.5 \n    65.5 \n    88.2 \n  \n  \n    0.46 \n    109 \n    736 \n    54 \n    101 \n    84.5 \n    51.9 \n    93.2 \n    66.9 \n    87.9 \n  \n  \n    0.48 \n    106 \n    739 \n    51 \n    104 \n    84.5 \n    50.5 \n    93.5 \n    67.5 \n    87.7 \n  \n  \n    0.50 \n    100 \n    744 \n    46 \n    110 \n    84.4 \n    47.6 \n    94.2 \n    68.5 \n    87.1 \n  \n  \n    0.52 \n    98 \n    748 \n    42 \n    112 \n    84.6 \n    46.7 \n    94.7 \n    70.0 \n    87.0 \n  \n  \n    0.54 \n    92 \n    750 \n    40 \n    118 \n    84.2 \n    43.8 \n    94.9 \n    69.7 \n    86.4 \n  \n  \n    0.56 \n    87 \n    753 \n    37 \n    123 \n    84.0 \n    41.4 \n    95.3 \n    70.2 \n    86.0 \n  \n  \n    0.58 \n    83 \n    761 \n    29 \n    127 \n    84.4 \n    39.5 \n    96.3 \n    74.1 \n    85.7 \n  \n  \n    0.60 \n    80 \n    766 \n    24 \n    130 \n    84.6 \n    38.1 \n    97.0 \n    76.9 \n    85.5 \n  \n  \n    0.62 \n    77 \n    769 \n    21 \n    133 \n    84.6 \n    36.7 \n    97.3 \n    78.6 \n    85.3 \n  \n  \n    0.64 \n    74 \n    771 \n    19 \n    136 \n    84.5 \n    35.2 \n    97.6 \n    79.6 \n    85.0 \n  \n  \n    0.66 \n    68 \n    772 \n    18 \n    142 \n    84.0 \n    32.4 \n    97.7 \n    79.1 \n    84.5 \n  \n  \n    0.68 \n    62 \n    774 \n    16 \n    148 \n    83.6 \n    29.5 \n    98.0 \n    79.5 \n    83.9 \n  \n  \n    0.70 \n    54 \n    775 \n    15 \n    156 \n    82.9 \n    25.7 \n    98.1 \n    78.3 \n    83.2 \n  \n  \n    0.72 \n    51 \n    777 \n    13 \n    159 \n    82.8 \n    24.3 \n    98.4 \n    79.7 \n    83.0 \n  \n  \n    0.74 \n    49 \n    778 \n    12 \n    161 \n    82.7 \n    23.3 \n    98.5 \n    80.3 \n    82.9 \n  \n  \n    0.76 \n    46 \n    778 \n    12 \n    164 \n    82.4 \n    21.9 \n    98.5 \n    79.3 \n    82.6 \n  \n  \n    0.78 \n    41 \n    781 \n    9 \n    169 \n    82.2 \n    19.5 \n    98.9 \n    82.0 \n    82.2 \n  \n  \n    0.80 \n    35 \n    783 \n    7 \n    175 \n    81.8 \n    16.7 \n    99.1 \n    83.3 \n    81.7 \n  \n  \n    0.82 \n    33 \n    783 \n    7 \n    177 \n    81.6 \n    15.7 \n    99.1 \n    82.5 \n    81.6 \n  \n  \n    0.84 \n    32 \n    783 \n    7 \n    178 \n    81.5 \n    15.2 \n    99.1 \n    82.1 \n    81.5 \n  \n  \n    0.86 \n    28 \n    784 \n    6 \n    182 \n    81.2 \n    13.3 \n    99.2 \n    82.4 \n    81.2 \n  \n  \n    0.88 \n    25 \n    786 \n    4 \n    185 \n    81.1 \n    11.9 \n    99.5 \n    86.2 \n    80.9 \n  \n  \n    0.90 \n    21 \n    787 \n    3 \n    189 \n    80.8 \n    10.0 \n    99.6 \n    87.5 \n    80.6 \n  \n  \n    0.92 \n    18 \n    787 \n    3 \n    192 \n    80.5 \n    8.6 \n    99.6 \n    85.7 \n    80.4 \n  \n  \n    0.94 \n    14 \n    788 \n    2 \n    196 \n    80.2 \n    6.7 \n    99.7 \n    87.5 \n    80.1 \n  \n  \n    0.96 \n    6 \n    788 \n    2 \n    204 \n    79.4 \n    2.9 \n    99.7 \n    75.0 \n    79.4 \n  \n  \n    0.98 \n    2 \n    790 \n    0 \n    208 \n    79.2 \n    1.0 \n    100.0 \n    100.0 \n    79.2 \n  \n\n\n\n\n\n\nLe Tableau 5.4 contient des estimations de plusieurs quantités intéressantes rattachées à la classification, en faisant varier le point de coupure. Pour chaque point de coupure, ces estimations ont été obtenues par validation croisée à \\(n\\) groupes (en anglais, leave-one-out cross-validation, ou LOOCV). Ainsi, ces estimations sont meilleures que les estimés sans ajustement aucun car elles ne sont pas obtenues en utilisant les mêmes observations que celles qui ont servi à estimer le modèle.\nLa colonne Correct donne le taux de bonne classification, \\(\\Pr(Y = \\widehat{Y})\\): avec un point de coupure de \\(0\\), on classifie toutes les observations à la classe achat (\\(1\\)), car \\(\\widehat{p}\\) est forcément plus grande que zéro. Le taux de bonne classification dans ce cas de figure sera de \\(21\\)%, puisque 210 individus ont acheté un produit dans le catalogue dans l’échantillon d’apprentissage. L’autre extrême, avec un point de coupure \\(c=1\\), donne un taux de bonne classification de \\(79\\)%.\nOn peut chercher dans le tableau les points de coupure qui donnent le meilleur taux de bonne classification. Ce dernier, à savoir 84.6%, est atteint par trois points de coupure, soit 0.52, soit 0.6, soit 0.62. Une recherche plus fine donne 0.465 comme point de coupure optimal, avec un taux de mauvaise classification de 15.3%.\nAvec une variable réponse binaire, il y a deux classifications possibles et le tableau de confusion contient, en partant du coin supérieur gauche et dans le sens des aiguilles d’une montre, le nombre de vrai positif (\\(Y=1\\), \\(\\widehat{Y}=1\\)), de faux positif (\\(Y=0\\), \\(\\widehat{Y}=1\\)), de vrai négatif (\\(Y=0\\), \\(\\widehat{Y}=0\\)) et finalement de faux négatif (\\(Y=1\\), \\(\\widehat{Y}=0\\)). La matrice de confusion, qui compare les vraies valeurs avec les prédictions, peut être construite à partir des colonnes VP, VN, FP et FN. Ces nombres proviennent de la validation croisée à \\(n\\) groupes et ne sont pas ceux qu’on obtiendrait si on appliquait directement le modèle ajusté à notre échantillon. Le taux de mauvaise classification est \\((\\mathsf{FP}+\\mathsf{FN})/n\\); une estimation plus fiable serait obtenue en utilisant la validation croisée à 10 groupes.\n\n\n\n\nTableau 5.5:  Matrice de confusion avec point de coupure 0.465. \n \n  \n      \n    \\(Y=1\\) \n    \\(Y=0\\) \n  \n \n\n  \n    \\(\\widehat{Y}=1\\) \n    109 \n    52 \n  \n  \n    \\(\\widehat{Y}=0\\) \n    101 \n    738 \n  \n\n\n\n\n\n\nQuatre autres quantités, dérivées à partir de la matrice de confusion, sont parfois utilisées:\n\nla sensibilité (sensitivity), \\(\\Pr(\\widehat{Y}=1 \\mid Y=1)\\), ou \\(\\mathsf{VP}/(\\mathsf{VP}+\\mathsf{FN})\\);\nla spécificité (specificity), \\(\\Pr(\\widehat{Y}=0 \\mid Y=0)\\), ou \\(\\mathsf{VN}/(\\mathsf{VN}+\\mathsf{FP})\\);\nle taux de vrais positifs, \\(\\Pr(Y=1 \\mid \\widehat{Y}=1)\\), ou \\(\\mathsf{VP}/(\\mathsf{VP}+\\mathsf{FP})\\);\nle taux de vrais négatifs, \\(\\Pr(Y=0 \\mid \\widehat{Y}=0)\\), ou \\(\\mathsf{VN}/(\\mathsf{VN}+\\mathsf{FN})\\).\n\nLes estimés empiriques sont simplement obtenus en calculant les rapports du nombre d’observations dans chaque classe.\nLa sensibilité mesure à quel point notre modèle est performant pour détecter un vrai positif (classe 1). La spécificité mesure à quel point notre modèle est performant pour détecter un résultat négatif (classe 0). Plus le point de coupure augmente, plus la sensibilité et le taux de faux positifs diminuent mais plus la spécificité et le taux de faux négatifs augmentent.\n\n5.3.1 Fonction d’efficacité du récepteur\nLa fonction d’efficacité du récepteur, parfois appelée courbe ROC (receiver operating characteristic) est parfois utilisée pour représenter globalement la performance du modèle. Il s’agit du graphe de la sensibilité en fonction de un moins la spécificité, en faisant varier le point de coupure. Un modèle parfait aurait une sensibilité et une spécificité égales à 1 (correspondant au coin supérieur gauche de la fonction d’efficacité du récepteur). Ainsi, plus le couple (\\(1-\\)spécificité, sensibilité) est près de (\\(0\\), \\(1\\)), meilleur est le modèle. Par conséquent, plus la courbe ROC tend vers (\\(0\\), \\(1\\)) meilleur est le pouvoir prévisionnel des variables. L’aire sous la courbe (area under the curve, ou AUC) est souvent utilisée en parallèle comme mesure de la qualité: comme son nom l’indique, c’est l’aire sous la courbe de la fonction d’efficacité du récepteur.\nLa fonction courbe_roc permet de tracer la courbe et de calculer l’aire sous la courbe. Plus cette valeur est près de 1, mieux c’est: une probabilité de 0.5 correspond à une allocation aléatoire, représentée sur la fonction d’efficacité du récepteur par la ligne diagonale.\n\n\n\n\n\nFigure 5.6: Fonction d’efficacité du récepteur avec probabilités de succès issues de la validation croisée à \\(n\\) groupes.\n\n\n\n\n\n# Fonction d'efficacité du récepteur\nroc <- hecmulti::courbe_roc(\n  resp = classif,\n  prob = predprob,\n  plot = TRUE)\nprint(roc)\n## Pour extraire l'aire sous la courbe, \n# roc$aire\n\n\n\n5.3.2 Classification avec une matrice de gain\nUtiliser le taux de mauvaise classification \\(\\Pr(Y \\neq \\widehat{Y})\\), comme critère de performance, revient au même que d’utiliser le taux de bonne classification \\(\\Pr(Y=\\widehat{Y})\\), car \\(\\Pr(Y \\neq \\widehat{Y}) = 1-\\Pr(Y=\\widehat{Y})\\). On veut un modèle avec un haut taux de bonne classification (ou un faible taux de mauvaise classification).\nLorsqu’on utilise \\(\\Pr(Y \\neq \\widehat{Y})\\) comme critère pour juger de la qualité d’un modèle prévisionnel, on fait l’hypothèse que le gain associé à bien classifier une observation dans la catégorie 0 lorsqu’elle est réellement dans la catégorie 0 est le même que celui associé à classifier une observation dans la catégorie 1 lorsqu’elle est réellement dans la catégorie 1: cela correspond à la matrice de gain.\n\n\nTableau 5.6: Matrice de gain correspondant au taux de bonne classification\n\n\n\n\nobservation\n\n\n\n\n\n\ngain\n\\(Y=1\\)\n\\(Y=0\\)\n\n\nprédiction\n\\(\\widehat{Y}=1\\)\n\\(1\\)\n\\(0\\)\n\n\n\n\\(\\widehat{Y}=0\\)\n\\(0\\)\n\\(1\\)\n\n\n\n\nLe gain vaut 1 lorsque la prévision est bonne (les deux cas sur la diagonale) et 0 lorsque le modèle se trompe (les deux autres cas). L’unité de mesure du gain n’est pas importante pour l’instant. Le gain total est\n\\[\\begin{align*}\n\\text{gain} &= 1 \\Pr(\\widehat{Y}=1, Y=1) + 1 \\Pr(\\widehat{Y}=0, Y=0)\n\\\\ &\\quad + 0 \\Pr(\\widehat{Y}=1, Y=0)  + 0 \\Pr(\\widehat{Y}=0, Y=1)\n\\\\& = \\Pr(Y = \\widehat{Y}).\n\\end{align*}\\] Maximiser le gain total revient donc à maximiser le taux de bonne classification.\nDans certaines situations, les gains (ou la perte si le gain est négatif) associés aux bonnes décisions et aux erreurs ne sont pas équivalents.\nSupposons que le gain de classer une observation à \\(i\\) (\\(i \\in \\{0,1\\}\\)) lorsqu’elle vaut \\(j\\) (\\(j \\in \\{0,1\\}\\)) en réalité est de \\(c_{ij}\\). La matrice de gain est alors\n\n\nTableau 5.7: Matrice de gain pondérée en fonction d’un coût\n\n\n\n\nobservation\n\n\n\n\n\n\ngain\n\\(Y=1\\)\n\\(Y=0\\)\n\n\nprédiction\n\\(\\widehat{Y}=1\\)\n\\(c_{11}\\)\n\\(c_{10}\\)\n\n\n\n\\(\\widehat{Y}=0\\)\n\\(c_{01}\\)\n\\(c_{00}\\)\n\n\n\n\nEn pratique, l’une de ces quatre quantités peut être fixée à 1 car seulement les poids relatifs (les ratios) des gains sont importants. Dans ce cas, le gain moyen est \\[\\begin{align*}\n\\text{gain} &= c_{11} \\Pr(\\widehat{Y}=1, Y=1) + c_{00}\\Pr(\\widehat{Y}=0, Y=0)\n\\\\ &\\quad + c_{10} \\Pr(\\widehat{Y}=1, Y=0)  + c_{01} \\Pr(\\widehat{Y}=0, Y=1)\n\\end{align*}\\]\nLe meilleur modèle est alors celui qui maximise le gain moyen.\nNous allons encore une fois seulement utiliser les 10 variables de base. Mais nous allons intégrer des revenus et coûts afin de trouver le meilleur point de coupure. Rappelez-vous que le coût de l’envoi d’un catalogue est de 10$. Le tableau des variables descriptives qui suit montre que, pour les 210 clients qui ont acheté quelque chose, le revenu moyen est de 67.29$ (moyenne de la variable ymontant).\n\n\n\n\nTableau 5.8:  Statistiques descriptives des montants d’achats pour la base de données marketing (échantillon d’apprentissage). \n \n  \n    n \n    moyenne \n    écart-type \n    minimum \n    maximum \n  \n \n\n  \n    210 \n    67.29 \n    13.24 \n    25 \n    109 \n  \n\n\n\n\n\n\nNous allons travailler en termes de revenu net. Nous pouvons donc spécifier la matrice de gain du Tableau 5.9 pour notre problème. Si on n’envoie pas de catalogue, notre gain est nul. Si on envoie le catalogue à un client qui n’achète pas, on perd 10$ (le coût de l’envoi). En revanche, notre revenu net est de 57$ (revenu moyen moins coût de l’envoi).\n\n\nTableau 5.9: Matrice de gain pour l’envoi de catalogue\n\n\n\n\nobservation\n\n\n\n\n\n\ngain\n\\(Y=1\\)\n\\(Y=0\\)\n\n\nprédiction\n\\(\\widehat{Y}=1\\)\n57\n-10\n\n\n\n\\(\\widehat{Y}=0\\)\n0\n0\n\n\n\n\nOn peut calculer la performance du modèle et le gain moyen en faisant varier le point de coupure. Pour avoir une mesure fidèle, on utilise la validation croisée à \\(K=10\\) groupes (la mesure affichée correspondant à la moyenne de 10 réplications)\n\ndata(dbm, package = \"hecmulti\")\ndonnees <- dbm |> \n  dplyr::filter(test == 0)\nformule = formula(yachat ~ x1 + x2 + x3 +\n                    x4 + x5 + x6 + x7 + \n                    x8 + x9 + x10)\nmodele <- glm(formule, \n              family = binomial, \n              data = donnees)\ncoupe <- hecmulti::select_pcoupe(\n  modele = modele, \n  c00 = 0, \n  c01 = 0, \n  c10 = -10, \n  c11 = 57,\n  plot = TRUE)\ncoupe\n\nPoint de coupure optimal: 0.01 \n\n\n\n\n\nFigure 5.7: Estimation du gain moyen en fonction du point de coupure pour l’exemple de base de données marketing.\n\n\n\n\nLa fonction select_pcoupe donne l’estimation du gain moyen (gain) pour différents points de coupures (pcoupe). Cette estimation provient d’une validation-croisée avec \\(K\\) groupes (ncv) dans la fonction), répétée nrep fois. On a effectué ici la validation croisée avec 10 groupes et fait la moyenne des 10 répétitions afin d’avoir plus de précision.\nOn voit dans la Figure 5.7 que le meilleur point de coupure, celui qui maximise le gain est 0.01. Avec ce point de coupure, et selon le Tableau 5.4, on estime que le taux de bonne classification est de 70.3 et que la sensibilité est de 90.95. Ainsi, on estime qu’on va détecter environ 91% des clients qui achètent.\nComme il est très coûteux de rater un client qui aurait acheté quelque chose, il est préférable d’envoyer le catalogue à plus de clients, quitte à ce que plusieurs d’entre eux n’achètent rien. Bien que le point de coupure de 0.5 donne un meilleur taux de bonne classification, il correspond à un gain moyen plus faible car on rate trop de clients qui achètent (la sensibilité est de seulement 47.62%). Travailler avec la matrice de gain permet de trouver le point de coupure optimal en incorporant des notions de coûts et profits.\n\n\n5.3.3 Courbe lift\nUn autre type de graphe qui est souvent utilisé dans des contextes de gestion est la courbe lift (sic) (en anglais, lift chart). Cette courbe est obtenue en ordonnant les probabilités de succès estimées par le modèle, \\(\\widehat{p}\\), en ordre croissant et en regardant quelle pourcentage de ces derniers seraient bien classifiés (le nombre de vrais positifs sur le nombre de succès).\n\ntab_lift <- hecmulti::courbe_lift(\n  prob = predprob, # probabilité de succès (Y=1)\n  resp = classif, # variable binaire réponse 0/1\n  plot = TRUE)\ntab_lift\n\n\n\n\n\n\nFigure 5.8: Courbe lift\n\n\n\n\n\n\n\n\nTableau 5.10:  Tableau du lift (déciles). \n \n  \n      \n    pourcent \n    hasard \n    modele \n    lift \n  \n \n\n  \n    10% \n    10 \n    21 \n    78 \n    3.714286 \n  \n  \n    20% \n    20 \n    42 \n    120 \n    2.857143 \n  \n  \n    30% \n    30 \n    63 \n    157 \n    2.492063 \n  \n  \n    40% \n    40 \n    84 \n    180 \n    2.142857 \n  \n  \n    50% \n    50 \n    105 \n    195 \n    1.857143 \n  \n  \n    60% \n    60 \n    126 \n    201 \n    1.595238 \n  \n  \n    70% \n    70 \n    147 \n    208 \n    1.414966 \n  \n  \n    80% \n    80 \n    168 \n    210 \n    1.250000 \n  \n  \n    90% \n    90 \n    189 \n    210 \n    1.111111 \n  \n\n\n\n\n\n\nLe Tableau 5.10 présente les 10 déciles. Si on classifiait comme acheteurs les 10% qui ont la plus forte probabilité estimée d’achat, on détecterait 81 des 210 clients (37.6%). En comparaison, on s’attend que 21 clients soient sélectionnés en moyenne si on prend un échantillon aléatoire de 100 personnes. Le ratio 81/21 (dernière colonne) est le lift du modèle: il permet de détecter 3.86 fois plus de succès que le hasard.\nLa Figure 5.8 présente le pourcentage d’observations bien classées parmi les variables (pourcentage des probabilités prédites qui correspondent à un succès parmi les \\(k\\) plus susceptibles selon le modèle). La référence est la ligne diagonale, qui correspond à une détection aléatoire.\n\n\n5.3.4 Calibration du modèle et détection du surajustement\nIl peut être intéressant de vérifier la calibration de notre modèle, et une statistique simple proposée par Spiegelhalter (1986) peut être utile à cette fin. Pour une variable binaire \\(Y \\in \\{0,1\\}\\), l’erreur quadratique moyenne s’écrit \\[\\begin{align*}\n\\overline{B} &= \\frac{1}{n} \\sum_{i=1}^n (Y_i-p_i)^2\n=\\frac{1}{n} \\sum_{i=1}^n(Y_i-p_i)(1-2p_i) + \\frac{1}{n} \\sum_{i=1}^n p_i(1-p_i).\n\\end{align*}\\] Le premier terme représente le manque de calibration du modèle, tandis que le deuxième correspond à la séparation entre variables. Si notre modèle était parfaitement calibré, alors \\(\\mathsf{E}_0(Y_i)=p_i\\) et \\(\\mathsf{Va}_0(Y_i) = p_i(1-p_i)\\). On peut utiliser ce fait pour construire une statistique de test de la forme \\(Z = \\{\\overline{B} - \\mathsf{E}_0(\\overline{B})\\}/\\sqrt{\\mathsf{Va}_0(\\overline{B})}\\), où \\[\\begin{align*}\n\\mathsf{E}_0(\\overline{B})&= \\frac{1}{n} \\sum_{i=1}^n p_i(1-p_i) \\\\\n\\mathsf{Va}_0(\\overline{B})&= \\frac{1}{n^2} \\sum_{i=1}^n p_i(1-p_i)(1-2p_i)^2\n\\end{align*}\\]\n\n\n\nSous l’hypothèse nulle de calibration parfaite, \\(Z \\sim \\mathsf{No}(0,1)\\) en grand échantillon. Pour le modèle simple avec toutes les covariables, la valeur-\\(p\\) approximative calculée avec les probabilités de succès obtenues par validation-croisée et les données de l’échantillon d’apprentissage est 0.22 et il n’y a pas de preuve ici que le modèle est mal calibré. Cette technique est utile pour vérifier s’il n’y a pas de surajustement (auquel cas le modèle tend à retourner des probabilités très près de 0/1, mais qui ne correspondent pas à la réalité).\nIci, nous avons ajusté un seul modèle, celui contenant uniquement les 10 variables de base et nous nous sommes attardés au choix du point de coupure pour l’assignation aux classes. Il est possible qu’un autre modèle, contenant par exemple des termes d’interactions, des termes quadratiques ou d’autres transformations des variables, soit supérieur à celui-ci. Le choix du modèle de prévision se fait donc souvent en deux étapes:\n\nchoisir les variables explicatives\nsélectionner un point de coupure.\n\nNous avons déjà vu des méthodes de sélections de variables au chapitre précédent. La section suivante reviendra sur ces méthodes dans le contexte de la régression logistique.\n\n\n5.3.5 Sélection de variables en régression logistique\nLes principes généraux, concernant la sélection de variables et de modèles, que nous avons vus au chapitre précédent sont toujours valides. Les critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) sont toujours disponibles puisqu’on estime le modèle par maximum de vraisemblance et les techniques générales de division de l’échantillon et de validation-croisée sont toujours valides. La principale différence est le coût d’estimation numérique des différents modèles: parce qu’il n’y a pas de solution explicite pour les estimateurs du maximum de vraisemblance du modèle logistique, ajuster chacun de ces modèles est coûteux.\nÀ la section précédente, nous avons inclus les 10 variables de base dans notre exemple d’envoi ciblé. Nous allons ici faire une recherche de type exhaustive parmi ces variables. La fonction glmbb du paquet éponyme fait une recherche à l’aide de l’algorithme de recherche arborescente dite par méthode de séparation et d’évaluation, qui ne nécessite pas de tester tous les modèles emboîtés. La sortie inclut les modèles qui sont à distance au plus cutoff du modèle optimal en ordre décroissant du critère d’information, avec une pondération associée qui peut servir comme succédané au mélange de modèle. La fonction permet de choisir entre les critères \\(\\mathsf{AIC}\\) et \\(\\mathsf{BIC}\\) et inclut toutes les modalités des variables explicatives pour les facteurs.\n\ndata(dbm, package = \"hecmulti\")\ndonnees <- dbm |> dplyr::filter(test == 0)\nformule <- formula(yachat ~ x1 + x2 + x3 +\n               x4 + x5 + x6 + x7 + \n               x8 + x9 + x10)\nselect_BIC <-\n  glmbb::glmbb(formule, \n             data = donnees,\n             criterion = \"BIC\", \n             family = binomial(link = \"logit\"))\nresultat_BIC <- summary(select_BIC)\n# Formule du meilleur modèle\nresultat_BIC$results$formula\n# Valeurs de BIC des modèles\nresultat_BIC$results$criterion\n\nCeci n’est qu’un exemple de stratégie de sélection de modèle parmi tant d’autre: le code qui suit explorera d’autres alternatives. Nous allons évaluer la performance de ces différentes stratégies avec comme critère de performance le revenu net de la stratégie si elle était appliquée aux 100 000 clients restants. Pour chacun des 100 000 clients à catégoriser, nous allons calculer la quantité suivante :\n\nSi le client n’est pas ciblé pour l’envoi d’un catalogue par le modèle, alors le revenu est nul.\nSi le client est ciblé pour l’envoi d’un catalogue par le modèle et qu’il n’achète rien, le revenu est de \\(-10\\)$ (le coût de l’envoi).\nSi le client est ciblé pour l’envoi d’un catalogue par le modèle et qu’il achète quelque chose, le revenu est de (ymontant\\(-10\\))$, c’est-à-dire, le montant qu’il dépense moins le \\(10\\)$ du coût de l’envoi.\n\nPour une stratégie donnée, chaque individu n’appartient qu’à une seule des catégories. Le revenu net de la stratégie est la somme des revenus pour les 100 000 clients. Parmi ces derniers, 23 179 auraient acheté si on leur avait envoyé le catalogue et ces clients auraient généré des revenus de 1 601 212$. Si on enlève le coût des envois (100 000 X 10$ = 1 000 000$), on obtient que la stratégie de référence permet un revenu net de 601 212$.\nDans ce cas, nous allons estimer la probabilité d’achat avec un modèle de régression logistique. Nous allons ensuite trouver le meilleur point de coupure, avec une matrice de gain adéquatement choisie, afin d’avoir une règle d’assignation optimale. Nous avons déterminé des modèles potentiels à la section précédente. De plus, nous avons déjà vu comment trouver le meilleur point de coupure en spécifiant une matrice de gain, afin de maximiser le gain moyen à partir de la matrice de gain du Tableau 5.9. Nous allons donc trouver le meilleur point de coupure pour quelques-uns des modèles choisis à la section précédente, pour ensuite évaluer le revenu net de ces modèles.\nIl faut encore une fois bien comprendre qu’en pratique, on ne pourrait pas faire cette comparaison, car on ne sait pas d’avance si les clients futurs vont acheter ou non. Mais dans cet exemple, les variables yachat et ymontant sont fournies pour ces 100 000 clients afin qu’on puisse voir ce qui se serait passé avec les différentes stratégies.\nLa stratégie de référence est celle qui consiste à envoyer le catalogue aux 100 000 clients sans les sélectionner. Le tableau qui suit montre des statistiques pour les variables ymontant et yachat pour les 100 000 clients à scorer. Le Tableau 5.11 résume la performance des différentes stratégies basées exclusivement sur le modèle logistique.\nEn résumé, la procédure numérique à réaliser est la suivante:\n\nChoisir les variables à essayer (interactions, etc.)\nChoisir l’algorithme ou la méthode de sélection\nObtenir un modèle final et calculer le point de coupure optimal selon notre matrice de coût.\nPour obtenir la performance finale, on obtient les prédictions pour les 100 000 clients de l’échantillon de validation et on classifie pour prédire la classe de yachat pour les données de validation à l’aide du point de coupure optimal choisi.\nOn calcule ensuite le revenu en soustrayant 10$ pour chaque envoi et en additionnant les montants d’achats des personnes qui ont reçu le catalogue.\n\nQuelques commentaires sur des raccourcis syntaxiques propres à R: dans une formule, spécifier ~. indique que l’on ajoute au modèle de régression toutes les variables explicatives de la base de données, moins la variable réponse. On peut aussi utiliser .^2 ou de manière équivalent .*. pour spécifier tous ces termes, ainsi que leurs interactions. Si on veut ajouter un terme quadratique pour une variable x, il faudra spécifier la transformation à l’intérieur de I(), par exemple I(x^2).\n\n# Diviser les bases de données \n# en échantillons d'apprentissage\n# et de validation\ndata(dbm, package = \"hecmulti\")\nvalid <- dbm[dbm$test == 1,] |>\n  dplyr::select(! c(ymontant, test))\nappr <- dbm[dbm$test == 0, ] |>\n  dplyr::select(! c(ymontant, test))\n# Formule du modèle avec toutes les interactions \n# d'ordre 2 (.^2) et les termes quadratiques I(x^2)\nformule <- formula(yachat ~ .^2 + \n                     I(x2^2) + I(x6^2) + \n                     I(x7^2) + I(x8^2) + \n                     I(x9^2) + I(x10^2))\n# Nouvelles bases de données avec toutes ces variables\n# On retire la première colonne (1, ordonnée à l'origine)\nappr_c <- data.frame(\n  cbind(model.matrix(formule, data = appr)[,-1]),\n  y = as.integer(appr$yachat))\nvalid_c <- data.frame(\n  cbind(model.matrix(formule, data = valid)[,-1]),\n  y = as.integer(valid$yachat))\nvalid_ymontant <- with(dbm, ymontant[test == 1L])\n\n# Ajustement des différents modèles\n\n# Modèle avec toutes les variables principales\nbase <- glm(yachat ~ ., \n            data = appr, \n            family = binomial)\n# Calcul du point de coupe optimal\n#  (par validation croisée)\nbase_coupe <- hecmulti::select_pcoupe(\n  modele = base, \n  c00 = 0, \n  c01 = 0, \n  c10 = -10, \n  c11 = 57)\n# Performance sur données de validation\nbase_pred <- \n  predict(object = base, \n          newdata = valid, \n          type = \"response\") > base_coupe$optim\nbase_perfo <- \n  -10*sum(base_pred) + \n  sum(valid_ymontant[base_pred], na.rm = TRUE)\n\n# Modèle avec toutes les variables + interactions\n# Ajustement\ncomplet <- glm(formula = formule, \n               data = appr, \n               family = binomial)\n# Sélection du point de coupure\ncomplet_coupe <- hecmulti::select_pcoupe(\n  modele = complet, c00 = 0, \n  c01 = 0, c10 = -10, c11 = 57)\n# Performance sur données de validation\ncomplet_pred <- \n  predict(object = complet, \n          newdata = valid, \n          type = \"response\") > complet_coupe$optim\n# Revenu\ncomplet_perfo <- \n  -10*sum(complet_pred) + \n  sum(valid_ymontant[complet_pred], na.rm = TRUE)\n\n# Sélection de modèle avec algorithme glouton\n# Recherche séquentielle (AIC)\nseqAIC <- step(object = complet, \n                direction = \"both\", # séquentielle\n                k = 2, # AIC \n                trace = 0) \nseqAIC_coupe <- \n  hecmulti::select_pcoupe(\n  modele = seqAIC, c00 = 0, \n  c01 = 0, c10 = -10, c11 = 57)\nseqAIC_pred <- \n  predict.glm(object = seqAIC, \n              newdata = valid, \n              type = \"response\") > \n  seqAIC_coupe$optim\nseqAIC_perfo <- \n  -10*sum(seqAIC_pred) + \n  sum(valid_ymontant[seqAIC_pred], \n      na.rm = TRUE)\n# Recherche séquentielle (BIC)\nseqBIC <- step(object = complet,\n                direction = \"both\", # séquentielle\n                k = log(nobs(complet)), #BIC\n                trace = 0)  \nseqBIC_coupe <- hecmulti::select_pcoupe(\n  modele = seqBIC, c00 = 0,\n  c01 = 0, c10 = -10, c11 = 57)\nseqBIC_pred <- \n  predict.glm(object = seqBIC, \n              newdata = valid, \n              type = \"response\") > \n  seqBIC_coupe$optim\nseqBIC_perfo <- \n  -10*sum(seqBIC_pred) + \n  sum(valid_ymontant[seqBIC_pred], \n      na.rm = TRUE)\n\n# Recherche exhaustive par algorithm génétique\n# avec moins de variables\nappr_r <- data.frame(\n  cbind(model.matrix(seqAIC)[,-1], \n        y = appr$yachat))\nvalid_r <- data.frame(\n  model.matrix(formula(seqAIC), \n               data = valid)[,-1])\nlibrary(glmulti)\nexgen <- glmulti::glmulti(\n  y = y ~ .,\n  #nombre de variables limitées\n  data = appr_r,  \n  level = 1,           # sans interaction\n  method = \"g\",        # recherche génétique\n  crit = \"bic\",            # critère (AIC, BIC, ...)\n  confsetsize = 1,         # meilleur modèle uniquement\n  plotty = FALSE, \n  report = FALSE,  # sans graphique ou rapport\n  fitfunction = \"glm\") \n\nTASK: Genetic algorithm in the candidate set.\nInitialization...\nAlgorithm started...\nImprovements in best and average IC have bebingo en below the specified goals.\nAlgorithm is declared to have converged.\nCompleted.\n\n# Redéfinir le modèle via \"glm\"\nexgen_modele <- \n  glm(exgen@objects[[1]]$formula,\n      data = appr_r,\n      family = binomial)\nexgen_coupe <- \n  hecmulti::select_pcoupe(\n    modele = exgen_modele, \n    c00 = 0, c01 = 0, c10 = -10, c11 = 57)\nexgen_pred <- \n  predict(exgen_modele, \n        newdata = valid_r, \n        type = \"response\") > exgen_coupe$optim\nexgen_perfo <-  \n  -10*sum(exgen_pred) + \n  sum(valid_ymontant[exgen_pred], \n      na.rm = TRUE)\n  \n# LASSO\n# Trouver le paramètre de pénalisation par\n# validation croisée (10 groupes)\ncvfit <- glmnet::cv.glmnet(\n  x = as.matrix(appr_c[, -ncol(appr_c)]), \n  y = appr_c$y, \n  family = \"binomial\", \n  type.measure = \"auc\") # aire sous courbe\n# Le critère par défaut est la déviance (-2ll)\n# Ajuster modèle avec pénalisation \nlasso <- glmnet::glmnet(\n  x = as.matrix(appr_c[,-ncol(appr_c)]), \n  y = appr_c$y, \n  family = \"binomial\", \n  lambda = cvfit$lambda.1se)\n# Calculer performance selon les points de coupure\nprobs_lasso <- \n  predict(lasso, \n          newx = as.matrix(appr_c[,-ncol(appr_c)]), \n          type = \"resp\")\nlasso_coupe <- with(\n  hecmulti::perfo_logistique(\n     prob = probs_lasso,\n     resp = appr_c$y),\n  coupe[which.max(VP*57 - FN*10)])\nlasso_pred <- c(predict(lasso, \n        newx = as.matrix(valid_c[,-ncol(valid_c)]), \n        type = \"resp\")) > lasso_coupe\nlasso_perfo <- -10*sum(lasso_pred) + \n  sum(valid_ymontant[lasso_pred], na.rm = TRUE)\n\nTable:\n\n\n\n\nTableau 5.11:  Résumé des caractéristiques des modèles logistiques avec (a) référence, soit l’envoi sans sélection à tous les clients; (b) 10 variables de base sans sélection; (c) toutes les variables, incluant les termes quadratiques et les interactions d’ordre 2; (d) sélection séquentielle avec AIC (e) sélection séquentielle avec BIC (f) recherche exhaustive avec variables de la procédure séquentielle AIC (sélection selon BIC) (g) LASSO avec pénalité optimale selon le critère de l’aire sous la courbe. Les points de coupure optimaux ont été déterminés par validation-croisée sur l’échantillon d’apprentissage (sauf LASSO), tandis que la performance du modèle (sensibilité et taux de bonne classification) ont été calculés à partir de l’échantillon test de 100 000 individus. \n \n  \n    modèle \n    no. variables \n    pt. coupure \n     sensibilité \n    taux bonne classif. \n    profit \n  \n \n\n  \n    (a) \n     \n     \n    1 \n    0.232 \n    601212 \n  \n  \n    (b) \n    14 \n    0.01 \n    0.2674 \n    0.366 \n    732102 \n  \n  \n    (c) \n    104 \n    0.01 \n    0.3605 \n    0.595 \n    918055 \n  \n  \n    (d) \n    28 \n    0.01 \n    0.3245 \n    0.52 \n    878598 \n  \n  \n    (e) \n    8 \n    0.01 \n    0.2924 \n    0.44 \n    804737 \n  \n  \n    (f) \n    10 \n    0.01 \n    0.3009 \n    0.463 \n    826323 \n  \n  \n    (g) \n    13 \n    0.01 \n    0.237 \n    0.254 \n    623345 \n  \n\n\n\n\n\n\nNous avons vu plus tôt, qu’avec les 10 variables de base, le meilleur point de coupure est de 0.11. En utilisant cette stratégie sur les 100 000 clients, le revenu net aurait été de 732102 dollars. C’est une énorme amélioration, de plus de 56%, par rapport à la stratégie de référence qui consiste à envoyer le catalogue à tout le monde (revenu net de 601 212$). Si on inclut tous les termes quadratiques et les termes les interactions d’ordre deux (104 variables en tout), le revenu net est inférieur avec une valeur de 918055$. Ici, le modèle est trop complexe et surajusté. Si on fait une sélection de variables (quasi méthodes sont présentées), suivie de la détermination du meilleur point de coupure, on fait alors toujours mieux qu’avec le modèle incluant les 10 variables de base seulement. L’approche la plus rentable parmi celles essayées aurait généré un profit de 918055 avec 104 variables explicatives: il s’agit d’un gain de 25.4% par rapport au modèle avec les 10 variables de base.\n\n\n5.3.6 Modèle Heckit\nNous venons tout juste d’étudier des stratégies qui consistent essentiellement, à estimer \\(\\Pr(\\texttt{yachat}=1)\\) et un point de coupure afin de décider à qui envoyer le catalogue en partant du postulat que tous les clients dépensent le même montant; le tout est basé uniquement sur la régression logistique. Le revenu moyen peut être estimé à partir de l’équation \\[\\begin{align*}\n\\mathsf{E}(\\texttt{ymontant}) = \\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1) \\Pr(\\texttt{yachat\n}=1),\n\\end{align*}\\] c’est-à-dire, la moyenne du montant dépensé est égale à la moyenne du montant dépensé étant donné qu’il y a eu achat, fois la probabilité qu’il ait eu achat. Une autre stratégie possible consiste donc à développer deux modèles : un pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) et un autre pour \\(\\Pr(\\texttt{yachat}=1)\\) et à les combiner afin d’obtenir des prévisions du montant dépensé.\n\n\nDescription du modèle Heckit\nLe paragraphe qui suit est plus technique et peut être omis. Il ne serait pas justifié d’ajuster séparément les deux modèles pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) et \\(\\Pr(\\texttt{yachat}=1)\\) et de calculer les prévisions en prenant le produit: \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\Pr(\\texttt{yachat}=1)\\). Cela provient du fait que le modèle pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) aurait été estimé seulement avec les clients qui ont acheté quelque chose et qu’ensuite on l‘appliquerait (au moment de calculer les prévisions) à la fois aux clients qui vont acheter et à ceux qui ne vont pas acheter. Il y a donc un biais de sélection dans l’échantillon qui a servi à ajuster le modèle au départ. Une manière de contourner ce problème est d’ajuster conjointement les deux modèles avec un modèle de Tobit de type 2. Ce dernier est basé sur l’hypothèse que les deux variables observées (\\(Y_1\\) et \\(Y_2\\)) proviennent de deux variables latentes non observées (\\(Y_1^{\\star}\\) et \\(Y_2^{\\star}\\)), où \\[\\begin{align*}\nY_1 = \\begin{cases}\n1 & \\text{ si } Y_1^{\\star} \\ge 0, \\\\\n0 & \\text{ si } Y_1^{\\star} < 0,\n\\end{cases}\n\\qquad \\qquad\nY_2 = \\begin{cases}\nY_2^{\\star} & \\text{ si } Y_1^{\\star} \\ge 0, \\\\\n0 & \\text{ si } Y_1^{\\star} < 0.\n\\end{cases}\n\\end{align*}\\] Dans notre exemple, \\(Y_1\\) correspond à \\(\\texttt{yachat}\\) et \\(Y_2\\) à \\(\\texttt{ymontant}\\). Ce qui lie les deux équations est le fait qu’on suppose que les variables sont binormales: les deux termes d’erreur sont de loi normale et sont corrélés, \\(\\boldsymbol{\\varepsilon} \\sim \\mathsf{No}_2(\\boldsymbol{0}_2, \\boldsymbol{\\Sigma})\\). Les variables dépendantes observées sont : \\[\\begin{align*}\nY_{1}^{\\star} &= \\beta_{01} + \\beta_{11} \\mathrm{X}_{11} + \\cdots + \\beta_{1p}\\mathrm{X}_{p1} + \\varepsilon_{1}\\\\\nY_{2}^{\\star} &= \\beta_{02} + \\beta_{12} \\mathrm{X}_{12} + \\cdots + \\beta_{1p}\\mathrm{X}_{q2} + \\varepsilon_{2}\n\\end{align*}\\] Notez que les variables explicatives ne sont pas nécessairement les mêmes dans les deux équations. En estimant conjointement les deux équations, on élimine le biais de sélection mentionné plus haut. Le choix des variables doit être fait avant avec les méthodes qu’on a vues. Le modèle Tobit ajuste un modèle probit et non logistique à la variable binaire (la fonction de liaison).\nNous avons déjà développé des modèles de régression linéaire pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) au chapitre précédent et nous venons de développer des modèles de régression logistique pour \\(\\Pr(\\texttt{yachat}=1)\\) dans ce chapitre. Nous avons donc tous les ingrédients pour implanter cette stratégie.\nNous allons cibler les clients dont la prévision du montant dépensé est plus grande que 10$ (le coût de l’envoi du catalogue).\nOn pourrait faire une sélection de variables pour chaque modèle: pour faire simple, nous allons sélectionner les variables de la procédure séquentielle et choisir les variables qui donnent le modèle avec le plus petit BIC pour la partie de régression linéaire et la régression logistique.\nPour obtenir les prévisions, nous allons estimer conjointement les modèles pour \\(\\mathsf{E}(\\texttt{ymontant} \\mid \\texttt{yachat}=1)\\) et pour \\(\\Pr(\\texttt{yachat}=1)\\) avec un modèle Tobit de type 2 (aussi appelé modèle Heckit), dont une brève description est donnée à la fin de la section.\nL’avantage de l’estimation simultanée est que l’on a pas à sélectionner le point de coupure, puisque l’on enverra le catalogue uniquement si le montant prédit pour \\(\\mathsf{E}(\\texttt{ymontant})\\) (non-conditionnel) est supérieur à 10$.\n\nlibrary(sampleSelection)\nformule_complet <- formula(ymontant ~ \n          (x1 + x2 + x3 + x4 + x5 + \n             x6 + x7 + x8 + x9 + x10)^2 + \n            I(x2^2) + I(x6^2) + I(x7^2) +\n            I(x8^2) + I(x9^2) + I(x10^2))\nselect_modlin <- \n  MASS::stepAIC(\n  object = lm(formule_complet,\n              data = dbm[dbm$test == 0,]),\n  scope = formula(ymontant ~ 1),\n  k = log(sum(dbm$test == 0)),\n  trace = FALSE)\nfachat <- formula(seqBIC)\nfmontant <- formula(select_modlin)\nheckit.ml <- sampleSelection::heckit(\n  selection = fachat,\n  outcome = fmontant, \n  method = \"ml\", \n  data = dbm[dbm$test == 0,])\nsortie_heckit <- summary(heckit.ml)\npred_achat <- \n  predict(heckit.ml, \n          part = \"selection\", \n          newdata = dbm[dbm$test == 1,], \n          type = \"response\") * \n  predict(object = heckit.ml,\n          part = \"outcome\", \n          newdata = dbm[dbm$test == 1,])\n#Remplacer valeurs manquantes par zéros\nvalid_ymontant[is.na(valid_ymontant)] <- 0\n# On envoie le catalogue seulement si le montant d'achat prédit est supérieur à 10$\n\n# Revenu total avec cette stratégie\nheckit_perfo <- \n  sum(valid_ymontant[which(pred_achat > 10)] - 10)\n\n\n\n\nLe modèle Heckit aurait produit un revenu net de 10007980$, un montant supérieur au revenu net de 918055$, qui était le meilleur trouvé à la sous-section précédente.\nPour conclure cet exemple, il s’avère donc que la régression logistique permet d’effectuer un bon ciblage des clients potentiels afin de maximiser les revenus. L’approche générale consistant à obtenir des prévisions pour \\(\\Pr(\\texttt{yachat}=1)\\) et ensuite trouver le meilleur point de coupure est très générale. D’autres types de modèles (arbre de classification, forêt aléatoire, réseau de neurones) pourraient être utilisés à la place de la régression logistique.\nNous reviendrons une dernière fois sur cet exemple dans le chapitre traitant des données manquantes. Nous verrons alors comment procéder si des valeurs manquantes sont présentes dans les variables explicatives.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nLa classification est une forme d’apprentissage supervisée.\nOn peut assigner l’observation à la classe la plus plausible, ou déterminer un point de coupure.\nSi on a un objectif particulier (fonction de gain), on peut optimiser les profits en assignant une importance différente à chaque scénario.\nOn peut catégoriser les observations dans une matrice de confusion et on peut calculer le taux de bonne classification comme mesure d’adéquation.\nDans le cas binaire, on s’intéresse généralement à\n\nla spécificité (proportion d’échecs correctement classifiés)\nla sensibilité (proportion de succès correctement classifiés)\nle taux de faux positifs ou faux négatifs\n\nL’aire sous la courbe de la fonction d’efficacité du récepteur (courbe ROC) et le lift donnent une mesure de la qualité des prédictions.\nL’erreur quadratique moyenne se réduit à calculer le taux de mauvaise classification, qu’on cherche à minimiser. On peut aussi utiliser la vraisemblance comme fonction objective.\nLes outils pour la sélection de variables couverts précédemment (critères d’information, LASSO, estimation de l’erreur par validation externe ou croisée) sont toujours applicables.\nLes modèles sont plus coûteux à estimer.\nIl y a moins d’information disponible avec une variable cible binaire, d’où une incertitude plus prononcée."
  },
  {
    "objectID": "05-reglogistique.html#modèles-pour-données-multinomiales",
    "href": "05-reglogistique.html#modèles-pour-données-multinomiales",
    "title": "5  Régression logistique",
    "section": "5.4 Modèles pour données multinomiales",
    "text": "5.4 Modèles pour données multinomiales\nSupposons que la variable \\(Y\\) que vous cherchez à modéliser est une variable catégorielle pouvant prendre trois valeurs ou plus. Voici quelques exemples :\n\nDestination de vacances l’année dernière (Québec, États-Unis, ailleurs).\nSi les élections avaient lieu aujourd’hui au Québec, pour quel parti voteriez-vous (PLQ, PQ, CAQ, QS).\nCombien de fois êtes-vous allé au cinéma l’année dernière: moins de cinq fois (\\(\\texttt{1}\\)), entre cinq et 10 fois (\\(\\texttt{2}\\)), ou plus de 10 fois (\\(\\texttt{3}\\)).\nQuelle importance accordez-vous au service après-vente? Un parmi « pas important » (\\(\\texttt{1}\\)), « peu important »(\\(\\texttt{2}\\)), « moyennement important » (\\(\\texttt{3}\\)), « assez important » (\\(\\texttt{4}\\)), « très important » (\\(\\texttt{5}\\)).\n\nDans les deux premiers exemples, la variable réponse \\(Y\\) est nominale (elle n’a pas d’ordre) alors qu’elle est ordinale dans les deux derniers. Pour une variable ordinale, le modèle logit multinomial peut être utilisé mais il existe d’autres possibilités comme le modèle logit cumulé. Nous couvrirons ces deux modèles.\n\n5.4.1 Régression logistique multinomiale\nEn régression logistique, \\(Y\\) est une variable binaire qui vaut soit 0, soit 1 et la probabilité de succès est \\[\\begin{align*}\n\\ln\\left(\\frac{p_i}{1-p_i}\\right) &= \\beta_0 + \\beta_1 \\mathrm{X}_{i1} + \\cdots + \\beta_p\\mathrm{X}_{ip},\\\\p_i &= \\Pr(Y_i=1 \\mid \\mathrm{X}_i) = \\textrm{expit}(\\eta_i).\n\\end{align*}\\] Dans ce modèle logistique, \\[\\begin{align*}\\ln\\left(\\frac{p_i}{1-p_i}\\right) = \\ln\\{\\Pr(Y_i=1 \\mid \\mathrm{X}_i)\\} -  \\ln\\{\\Pr(Y_i=0 \\mid \\mathrm{X}_i)\\}\n\\end{align*}\\] peut être vu comme étant le logit de la catégorie 1 en utilisant 0 comme catégorie de référence. Le modèle logistique multinomial procède de même en fixant une catégorie de référence et en modélisant le logit de chacune des autres catégories par rapport à la catégorie de référence. Avec \\(K\\) catégories (\\(k = 1, \\ldots, K\\)) et en choisissant la catégorie 1 comme référence, le modèle devient \\[\\begin{align*}\n\\ln\\left(\\frac{p_{ij}}{p_{i1}}\\right) = \\eta_{ij} = \\beta_{0j} + \\beta_{1j} \\mathrm{X}_{i1} + \\cdots + \\beta_{pj}\\mathrm{X}_{ip}, \\quad (j=2, \\ldots, K)\n\\end{align*}\\] où \\(p_{ik} = \\Pr(Y_i=k \\mid \\mathrm{X}_i)\\) \\((k=1, \\ldots, K)\\). Comme en régression logistique, on peut facilement exprimer ce modèle en termes des différentes probabilités, \\[\\begin{align*}\np_{i1} &= \\Pr(Y_i=1 \\mid \\mathrm{X}_i) = \\frac{1}{1+ \\sum_{j=2}^K\\exp(\\eta_{ij})}\\\\\np_{ik} &= \\Pr(Y_i=k \\mid \\mathrm{X}_i) = \\frac{\\exp(\\eta_{ik})}{1+ \\sum_{j=2}^K\\exp(\\eta_{ij})}, \\qquad k=2, \\ldots, K.\n\\end{align*}\\] On voit facilement que la somme des probabilités égale 1, c’est-à-dire \\(p_{i1} + \\cdots + p_{iK} = 1\\), ce qui fait que connaître la probabilité de \\(K-1\\) des catégories nous permet de déduire la dernière. En fait, le modèle logit multinomial ne fait que combiner plusieurs logit dans un seul modèle. L’interprétation des paramètres se fait comme en régression logistique sauf qu’il faut y aller équation par équation.\nL’exemple qui suit traite du taux de participation lors des élections américaines et des facteurs expliquant qu’un électeur ou une électrice se prévaut de son droit de vote, ainsi que la fréquence de participation. Les données sont tirées d’un sondage Ipsos réalisé pour le site de nouvelles FiveThirtyEight. Les données sont accompagnées de pondérations provenant du recensement permettant de corriger la représentativité du sondage et de refléter l’électorat américain dans sa globalité.\nLa base de données vote contient 5837 observations obtenues par voie de sondage. Nous allons modéliser l’intention de vote, catvote à l’aide d’une régression logistique multinomiale.\n\n\n\n\n\nFigure 5.9: Proportion des modalités des variables sociodémographiques des données de participation électorale.\n\n\n\n\nOn voit que les personnes plus fortunées, plus éduquées, plus âgées et celles qui s’associent à un parti politique principal (Républicains et Démocrates), votent davantage. L’écart selon l’âge est particulièrement édifiant, avec près de 50% des jeunes qui n’ont pas participé. Il faut garder en tête que le revenu, l’âge et le niveau d’éducation sont fortement associés et que les personnes plus jeunes ont eu moins d’occasions de voter (ce qui pourrait expliquer la plus grande propension pour les catégories de vote).\nUne étude plus attentive révèle que la distribution conditionnelle de ceux qui votent toujours est bimodale. La Figure 5.10 montre clairement que les très jeunes et les personnes âgées en font partie. Ainsi, le modèle est potentiellement mal spécifié car le vrai effet de l’âge n’est visiblement pas linéaire au niveau du log de la cote. Cela dit, les primovotant(e)s n’ont souvent eu qu’une seule occasion de voter, ce qui peut expliquer le comportement sur le graphique et l’absence de réponses pour occasionnellement. Pour éviter cet artefact, on considère les personnes de plus de 30 ans uniquement.\n\n\n\n\n\nFigure 5.10: Fréquence de vote selon l’âge.\n\n\n\n\nPour le modèle logit multinomial, nous allons prendre rarement/jamais comme catégorie de référence pour la variable réponse catvote. Notez qu’il est d’usage et préférable, pour réduire le risque de problèmes numériques et accélérer l’optimisation, de centrer et réduire les variables explicatives.\n\ndata(vote, package = \"hecmulti\")\n# Modèle multinomial\nmulti1 <- nnet::multinom(\n  catvote ~ scale(age, scale = FALSE), # centrer\n  data = vote, \n  subset = age > 30,\n  weights = poids,\n  Hess = TRUE,\n  trace = FALSE) \n# Tableau résumé de l'ajustement\nsummary(multi1)\n# Estimations des coefficients\ncoef(multi1)\n# Intervalles de confiance (Wald)\nconfint(multi1)\n# Critères d'information\nAIC(multi1)\nBIC(multi1)\n# Prédictions: probabilité de chaque scénario\npredict(multi1, type = \"probs\")\n# Prédictions: classe la plus susceptible\npredict(multi1, type = \"class\")\n\n\n\nTableau 5.12: Estimation des coefficients et intervalles de confiance à 95 pourcent pour le modèle multinomial logistique avec les données de vote.\n\n\n\n\n\n(a)  catégorie rarement/jamais vs toujours\n \n  \n      \n    coefficient \n    IC (2.5%) \n    IC (97.5%) \n  \n \n\n  \n    cst \n    0.783 \n    0.709 \n    0.858 \n  \n  \n    age \n    0.031 \n    0.025 \n    0.036 \n  \n\n\n\n\n\n\n\n\n\n(b) catégorie occasionnellement vs toujours\n \n  \n      \n    coefficient \n    IC (2.5%) \n    IC (97.5%) \n  \n \n\n  \n    cst \n    -0.128 \n    -0.224 \n    -0.032 \n  \n  \n    age \n    0.082 \n    0.075 \n    0.089 \n  \n\n\n\n\n\n\nComme il y a trois catégories pour la variable dépendante, il y a deux équations pour le modèle ajusté. En regardant les coefficients dans le Tableau 5.12, on obtient: \\[\\begin{align*}\n\\ln \\left\\{\\frac{\\Pr(\\texttt{catvote}_{1i}= \\texttt{occasionnellement}\\mid \\texttt{age}_i)}{\\Pr(\\texttt{catvote}_{1i} = \\texttt{rarement/jamais} \\mid \\texttt{age}_i)} \\right\\} &=\n0.783 +\n0.031 \\texttt{age}_i, \\\\\n\\ln \\left\\{\\frac{\\Pr(\\texttt{catvote}_{1i}= \\texttt{toujours}\\mid \\texttt{age}_i)}{\\Pr(\\texttt{catvote}_{1i}= \\texttt{rarement/jamais}  \\mid \\texttt{age}_i)} \\right\\} &=\n-0.128 + 0.082\\texttt{age}_i.\n\\end{align*}\\]\nPlus l’âge du répondant augmente, plus la probabilité que la personne votre toujours augmente. Ainsi, la cote moyenne pour toujours versus la référence rarement/jamais est multipliée par \\(1.031=\\exp(0.031)\\) pour chaque année de plus. Pour faire simple, on a employé une seule variable explicative, mais il est clair au vu de l’analyse exploratoire que d’autres variables sont utiles pour comprendre le comportement des électeurs et électrices. Qui est plus, la taille de la base de données nous permettrait de mesurer d’autres effets.\nOn peut comparer les modèles emboîtés à l’aide de tests de rapport de vraisemblance.\n\n# Ajuster modèle sous H0: les\n# prédictions correspondent à la\n# proportion empirique de chaque catégorie\nmulti0 <- nnet::multinom(catvote ~ 1,\n                         weights = poids,\n                         data = vote,\n                         subset = age > 30,\n                         trace = FALSE)\n# Test de rapport de vraisemblance\nanova(multi0, multi1)\n\n\n\n\n\nTableau 5.13:  Analyse de déviance: test du rapport de vraisemblance pour la variable explicative dans le modèle multinomial logistique. \n \n  \n    modèle \n    DL \n    déviance \n    DL \n    rapport vrais. \n    valeur-p \n  \n \n\n  \n    cst \n    9692 \n    9781.07 \n     \n     \n     \n  \n  \n    age \n    9690 \n    9077.63 \n    2 \n    703.44 \n    < 0.0001 \n  \n\n\n\n\n\n\nCette valeur est donnée dans la dernière colonne du tableau. De plus, cet effet est significatif car la valeur-\\(p\\) est inférieure à \\(10^{-4}\\).\nPour une comparaison directe entre les deux autres catégories, rarement/jamais et occasionnellement, il suffit de changer la catégorie de référence.\n\n\n5.4.2 Régression logistique cumulative à cotes proportionnelles\nSi les modalités de la réponse sont ordinales, la régression logistique multinomiale est toujours appropriée. Il peut néanmoins être préférable d’utiliser un modèle qui utilise l’ordre des modalités pour obtenir un modèle plus facile à interpréter et plus parcimonieux. Le modèle de régression logistique cumulative à cotes proportionnelles (McCullagh 1980) est une simplification du modèle multinomial sous l’hypothèse que les rapports de cotes sont les mêmes peut importe la catégorie.\nSupposons que les \\(K\\) modalités de la variable ordinale \\(Y\\) sont en ordre croissant et que l’on dispose de \\(p\\) variables explicatives \\(\\mathrm{X}_1, \\ldots, \\mathrm{X}_p\\) pour chaque observation. Soit \\(p_{ik}=\\Pr(Y_i=k \\mid \\mathbf{X}_{i})\\) (\\(k=1, \\ldots, K\\)) la probabilité que \\(Y_{i}\\) prenne la valeur \\(k\\).\nLe modèle logistique à cotes proportionnelles spécifie que pour \\(k=1, \\ldots, K-1\\), \\[\\begin{align*}\n\\frac{\\Pr(Y_i > k \\mid \\mathbf{X}_i)}{\\Pr(Y_i \\leq k \\mid \\mathbf{X}_i)} = \\frac{p_{i(k+1)} + \\cdots + p_{iK}}{p_{i1} + \\cdots + p_{ik}} = \\exp(\\mathbf{X}_i \\boldsymbol{\\beta} - \\zeta_k),\n\\end{align*}\\] en utilisant la paramétrisation de la fonction polr du paquet MASS. Le terme \\(-\\eta_{k}\\) correspond à l’ordonnée à l’origine spécifique à la catégorie \\(k\\) et \\(-\\infty=\\zeta_0 < \\zeta_1 < \\cdots < \\zeta_K =\\infty\\) aux points de coupe qui déterminent les probabilités de chaque catégorie. Puisque \\(\\Pr(Y_i > K \\mid \\mathbf{X}_i)=1\\), il y a \\(K-1\\) équations pour le rapport de cote. Si l’ordonnée à l’origine change d’une équation à l’autre, les paramètres quantifiant les effets des variables explicatives, \\(\\beta_1, \\ldots, \\beta_p\\) sont les mêmes pour chacune des cotes. Par conséquent, pour modéliser une variable ordinale \\(Y\\) ayant \\(K\\) valeurs possibles et avec \\(p\\) variables explicatives, le modèle cumulatif logistique utilise \\(p + K - 1\\) paramètres. Le modèle logit multinomial, qui peut également être utilisé pour les données ordinales, utilise plutôt \\((K-1) \\cdot(p+1)\\) paramètres. Le modèle logistique cumulatif à cotes proportionnelles est donc plus parcimonieux et, pour autant qu’il soit approprié, mènera à des estimations des paramètres plus précises qu’avec le modèle de régression logistique multinomiale. Les deux modèles sont identiques au modèle de régression logistique si la variable ordinale a uniquement deux modalités (variable binaire).\nLa cote pour \\(Y_i > k\\) mesure à quel point il est plus probable que \\(Y_i\\) prenne une valeur plus grande que \\(k\\) par rapport à une valeur plus petite ou égale à \\(k\\). Dans cet exemple, nous n’avons aucune transformation des variables explicatives, ni aucune interaction dans le modèle; l’interprétation des paramètres est donc simplifiée. Si le paramètre \\(\\beta_j\\) est positif, cela indique que plus \\(\\mathrm{X}_j\\) prend une valeur élevée, plus la variable \\(Y\\) a tendance à prendre aussi une valeur élevée. Inversement, si le paramètre \\(\\beta_j\\) est négatif, cela indique que plus \\(\\mathrm{X}_j\\) prend une valeur élevée, plus la variable \\(Y\\) a tendance à prendre une valeur basse. Plus précisément, pour chaque augmentation d’une unité de \\(\\mathrm{X}_j\\), la cote pour \\(\\Pr(Y_i > k \\mid \\mathbf{X}_i)\\) versus \\(\\Pr(Y_i \\leq k \\mid \\mathbf{X}_i)\\) est multipliée par \\(\\exp(\\beta_j)\\), peu importe la valeur de \\(Y\\). En terme de probabilité cumulée d’excéder \\(k\\), \\[\\begin{align*}\n\\Pr(Y_i > k \\mid \\mathbf{X}_i) &= \\textrm{expit}(-\\eta_k + \\beta_1 \\mathrm{X}_{i1} + \\cdots + \\beta_p \\mathrm{X}_{ip}), \\qquad k=1, \\ldots, K-1.\n\\end{align*}\\] En utilisant ces expressions, on peut obtenir la probabilité de chaque catégorie, \\[\\begin{align*}\n&\\Pr(Y_i = k \\mid \\mathbf{X}_{i}) =\\Pr(Y_i > k \\mid \\mathbf{X}_{i}) - \\Pr(Y_i > k-1 \\mid \\mathbf{X}_{i}).\n\\end{align*}\\]\nOn peut répéter le même modèle que précédemment pour les données de sondage, même s’il est peu susceptible que l’hypothèse de cotes proportionnelles soit valide. Dans R, la variable réponse doit être de classe ordered, une forme particulière de facteur dont les niveaux sont ordonnés en ordre croissant. On ajuste un modèle, cette fois avec sexe pour nous permettre de pratiquer l’interprétation d’une variable catégorielle.\n\n# Modèle de régression logistique \n# multinomiale ordinale à cote proportionnelle\nwith(vote, is.ordered(catvote))\nmulti2a <- MASS::polr(\n  catvote ~ sexe, \n  data = vote, \n  subset = age > 30,\n  weights = poids,\n  method = \"logistic\", \n  Hess = TRUE)\n\nmulti2b <- nnet::multinom(\n  catvote ~ sexe, \n  data = vote,\n  subset = age > 30, \n  weights = poids,\n  Hess = TRUE,\n  trace = FALSE)\n# Le modèle est paramétré en terme\n#  du rapport de cote, ascendant\nsummary(multi2a)\n# Test du rapport de vraisemblance pour \n# modèle à cote proportionnelle\n# deviance = -2*ll\npchisq(deviance(multi2a) - deviance(multi2b),\n       df = length(coef(multi2a)), \n       lower.tail = FALSE)\n# Intervalles de confiance pour beta_x\n#  - vraisemblance profilée\nconfint(multi2a)\n# Critères d'information\nAIC(multi2a)\nBIC(multi2a)\n\n# Tableau des coefficients \n# Négatif de l'ordonnée à l'origine:\nmulti2a$zeta\n# Uniquement pour variables explicatives\n# exp(beta) avec l'IC de vraisemblance profilée\nexp(c(coef(multi2a), confint(multi2a)))\n# On peut obtenir les intervalles de Wald \n# avec confint.default\n\n# Test d'adéquation \n# (rapport de vraisemblance, comparaison avec modèle saturé)\npchisq(q = deviance(multi2a),\n       df = df.residual(multi2a),\n       lower.tail = FALSE)\n# Petite valeur-p = modèle inadéquat\n\n\n\n\n\nTableau 5.14:  Tableau des estimations des coefficients du modèle pour réponses ordinales pour la régression logistique à cotes proportionnelles avec sexe. \n \n  \n    effet \n    coefficient \n    erreur-type \n  \n \n\n  \n    sexe [homme] \n    -0.166 \n    0.055 \n  \n  \n    cst [rarement/jamais|occasionnellement] \n    -1.297 \n    0.044 \n  \n  \n    cst [occasionnellement|toujours] \n    0.865 \n    0.041 \n  \n\n\n\n\n\n\nSi on écrit les équations pour la cote, on obtient \\[\\begin{align*}\n\\frac{\\Pr(Y = \\texttt{rarement} \\mid \\texttt{sexe})}{\\Pr(Y \\geq \\texttt{occasionnellement} \\mid \\texttt{sexe})} &= \\exp(-0.166\\texttt{sexe} + 1.297) \\\\\n\\frac{\\Pr(Y \\leq \\texttt{occasionnellement} \\mid \\texttt{sexe})}{\\Pr(Y = \\texttt{toujours} \\mid \\texttt{sexe})} &= \\exp(-0.166\\texttt{sexe} - 0.865).\n\\end{align*}\\]\nIci, l’effet estimé d’être un homme plutôt qu’une femme (sexe) est \\(-0.166\\) et ce paramètre est significativement différent de zéro (valeur-\\(p\\) de \\(0.003\\) obtenue en faisant un test de rapport de vraisemblance).\nAinsi, les hommes sont moins susceptibles de voter fréquemment que les femmes. Plus précisément, la cote d’être dans une catégorie plus élevée de catvote, par rapport à une catégorie plus basse, est multipliée par \\(\\exp(-0.166) = 0.847\\), ce qui correspond à une diminution de la cote des femmes \\(15\\)% (et donc la probabilité estimée que la personne vote plus fréquemment est plus faible).\nConsidérons pour illustrer le rôle des paramètres \\(\\zeta_1, \\ldots, \\zeta_{k-1}\\) pour la prédiction pour une femme (valeur de la référence). Soit \\(p_1\\), \\(p_2\\) et \\(p_3\\) les probabilités pour respectivement rarement/jamais, occasionnellement et toujours.  On peut calculer \\(\\mathrm{expit}(\\zeta_k)\\) (\\(k=0, \\ldots, K\\)) qui donne \\(0\\), \\(0.215\\), \\(0.704\\) et \\(1\\) et les différences donnent \\(\\widehat{p}_1 = 0.215\\), \\(\\widehat{p}_2 = 0.489\\) et \\(\\widehat{p}_3 = 0.296\\). Un rapide calcul numérique montre que c’est bien ce que retourne les prédictions dans le Tableau 5.15.\n\npredict(multi2a, \n        newdata = data.frame(sexe = factor(\"femme\")), \n        type = \"probs\")\n\n\n\n\n\nTableau 5.15:  Probabilités de chaque classe pour une femme avec le modèle à cotes proportionnelles qui inclut uniquement le sexe. \n \n  \n    rarement/jamais \n    occasionnellement \n    toujours \n  \n \n\n  \n    0.215 \n    0.489 \n    0.296 \n  \n\n\n\n\n\n\nAvant toute chose, il faut s’assurer que le modèle est approprié. Rappelez-vous que l’une des hypothèses de ce modèle est que les effets des variables explicatives sont les mêmes pour chaque équation.\n\n\\(\\mathscr{H}_0\\) : l’effet de chaque variable est le même pour les \\(K\\) logit du modèle multinomial logistique, soit \\(\\beta_{11} = \\cdots =\\beta_{1K}\\), \\(\\ldots\\), \\(\\beta_{p1} = \\cdots =\\beta_{pK}\\).\n\nUne très petite valeur-\\(p\\) (rejet de \\(\\mathscr{H}_0\\)) pour ce test serait une indication que le modèle de régression multinomiale ordinale n’est pas approprié et que le modèle multinomial logistique serait préférable. Comme la valeur-\\(p\\) est négligeable, on rejette pas \\(\\mathscr{H}_0\\) et l’hypothèse de cote proportionnelle ne tient pas la route.\n\n\n\n\n\nFigure 5.11: Probabilités prédites pour chaque modalité selon le modèle de régression multinomiale logistique et le modèle de régression ordinales à cotes proportionnelles selon l’âge.\n\n\n\n\nOn a précédemment utilisé un test du rapport de vraisemblance pour valider l’hypothèse des cotes proportionnelles: il n’y avait aucune indication que la simplification n’était pas adéquate. Ce n’est pas le cas pour le modèle qui ne contient que la variable age ou plusieurs variables explicatives: la Figure 5.11 montre les différences de probabilités ajustées pour les deux modèles qui incluent uniquement l’âge comme variable explicative. Règle générale, on n’ajustera jamais un modèle avec une seule des variables: un test du rapport de vraisemblance indique que toutes les variables explicatives sont utiles pour expliquer le comportement.\nOn peut également vérifier si le modèle est adéquat pour décrire les données en comparant le modèle pour les données ordinales avec un modèle saturé (qui contient autant de paramètres que d’observations/niveaux): la valeur-\\(p\\), infime, indique que le modèle plus complexe, soit le modèle saturé, est préférable. Cela nous indique que le modèle a un piètre pouvoir explicatif.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nLa régression multinomiale logistique pour une variable catégorielle à \\(K\\) niveaux est une extension directe de la régression logistique pour données binaires: il y a \\(K-1\\) équations de cote en termes des variables explicatives (puisque la somme des probabilités vaut 1), donc le nombre de paramètres croît rapidement.\nLe modèle est multiplicatif: la cote de catégorie \\(k\\) vs référence est multipliée par \\(\\exp(\\beta_{jk})\\) pour chaque augmentation de \\(\\mathrm{X}_j\\) d’une unité.\nLes coefficients manquants de la sortie du tableau peuvent être déduits par des manipulations algébriques.\nLe modèle cumulatif à cote proportionnelle est une simplification du modèle multinomial pour des données ordinales.\nOn suppose que l’effet des variables est le même pour la cote de la survie de chaque modalité\nLe modèle à cotes proportionnelles a moins de paramètres, mais le postulat de cotes proportionnelles doit être vérifié (via un test de rapport de vraisemblance ou un test du score).\n\n\n\n\n\n\n\nMcCullagh, Peter. 1980. “Regression Models for Ordinal Data.” Journal of the Royal Statistical Society: Series B (Methodological) 42 (2): 109–27. https://doi.org/10.1111/j.2517-6161.1980.tb01109.x."
  },
  {
    "objectID": "06-survie.html#introduction",
    "href": "06-survie.html#introduction",
    "title": "6  Analyse de survie",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nCette section traite de temps avant qu’un événement survienne. Le traitement de ce type de données, dites données de survie en référence au domaine médical, est particulier parce que l’information disponible est incomplète. Plusieurs mécanismes peuvent impacter la survie: généralement les données sont sujettes à troncature et à censure.\nPour déterminer les mécanismes de survie en présence, il peut être utile de représenter le processus de collecte de données à l’aide d’un diagramme de Lexis: ce dernier présente la trajectoire observée à l’aide d’une droite de pente un. L’axe des abscisses (\\(x\\)) donne le temps (au calendrier) et l’axe des ordonnées (\\(y\\)) la durée observée.\n\n\n\n\n\nFigure 6.1: Diagramme de Lexis pour données tronquées à gauche (\\(A\\) et \\(C\\)) et censurées à droite (\\(C\\) et \\(D\\)).\n\n\n\n\nLa Figure 6.1 montre des courbes fictives. On trace une droite de pente 1 représentant la durée en fonction du temps (date au calendrier) etl a fenêtre définit la période de collecte de donnée. La censure est indiquée par des cercles, les événements par des croix.\nOn parle de censure lorsque le temps réel de l’événement n’est pas observé (information partielle).\n\nCensure à droite: l’événement n’est pas encore survenu au temps \\(t\\): on sait que \\(T > t\\).\nCensure à gauche: l’événement survient avant le temps \\(t\\), donc la vraie valeur est inférieure à la valeur observée (\\(T < t\\))\nCensure par intervalle: l’événement est survenu dans la plage \\([t_1, t_2]\\) (données arrondies)\n\nLa censure peut être aléatoire (par exemple, une personne qui fait partie d’un protocole de rercherche déménage dans un autre pays parce que sa conjointe a trouvé un emploi là-bas et cette raison n’a aucun lien avec son état de santé). La censure administrative, un mécanisme déterministe qui survient lorsque les données sont collectées sur une période fixe de temps, ne fournit pas non plus d’information sur l’événement en question. On supposera dans ces notes que nous sommes dans un de ces deux cas de figure.\nSi la censure est informative, les outils présentés ne sont pas adéquats! Par exemple, si un patient d’une étude clinique est déchargé d’un protocole médical car il est trop mal en point, cela nous renseigne sur son état de santé et sur sa survie.\nLa troncature, rarement discutée, est liée au processus de collecte et détermine quelles observations sont incluses dans l’étude. La plage des valeurs possibles est tronquée. Les types de troncature sont:\n\ntroncature à gauche: le temps minimum est supérieur à \\(t_0\\)\ntroncature à droite: le temps maximum est inférieur à \\(t_1\\)\ntroncature par intervalle: le temps de l’événement doit survenir entre \\(t_0\\) et \\(t_1\\).\n\nLa troncature à gauche est la plus fréquente: elle survient par exemple si on étudie le temps d’abonnement, mais qu’on n’a accès qu’aux dossiers de clients et de clientes qui sont actifs dans le système. Ainsi, une personne qui se serait désabonnée une journée avant qu’on télécharge les données ne se trouvera pas dans la base de données. Si on ne tient pas compte de cet état de fait et des fantômes, nos estimations seront biaisées.\nLa troncature par intervalle survient quand on considère uniquement les personnes pour lesquelles l’événement d’intérêt est survenu. Si l’on s’intéresse à la durée de la relation d’emploi et seules les personnes à l’emploi qui ont pris leur retraite entre 2009 et 2021 sont considérées pour l’étude, on sera en présence de troncature par intervalle comme représenté dans la Figure 6.2.\n\n\n\n\n\nFigure 6.2: Diagramme de Lexis illustrant la troncature par intervalle.\n\n\n\n\nOn considère comme exemple une étude sur le chômage dû à la crise du coronavirus. On s’intéresse à tous ceux qui étaient en recherche active d’emploi entre mars et juin; seuls ceux qui étaient au chômage durant cette période seront considérés. Certaines personnes seront déjà au chômage en avril et donc leur durée de chômage en mars est déjà longue (troncature à gauche). Lors de notre suivi, d’autres personnes mentionneront avoir trouvé un emploi lors de notre appel, mais ne pourront nous renseigner sur la date exacte de leur embauche: cette dernière précèdera notre prise de contact, mais nous est inconnue (censure à gauche). D’autres personnes seront toujours au chômage en juin à la fin de l’étude et on ignorera le nombre réel de mois passés au chômage (censure à droite). Enfin, certaines personnes cesseront de chercher activement un emploi et donc quitteront l’étude (censure à droite). Tous ces méchanismes (complexes) peuvent être dictés par certaines covariables (employabilité, découragement) et être aléatoires ou pas. Pour estimer le taux de chômage, il faudra prendre en compte les méchanismes de survie dans notre modèle. On se concentrera sur le cas simple des données censurées à droite de façon aléatoire.\nAvec la survie, les statistiques descriptives usuelles sont trompeuses. La figure de cet article tiré de The Conversation montre l’impact drastique de la censure en représentant l’âge moyen au décès en fonction du genre musical, ordonné par ordre d’apparition de ce dernier. Puisque seules les personnes décédées sont incluses, les artistes de hip-hop décédés l’ont été forcément en bas-âge. Si on ne prend pas en compte la censure, on obtiendrait une conclusion erronnée.\n\n6.1.1 Exemple du temps d’abonnement\nUne entreprise oeuvrant dans le secteur des télécommunications s’intéresse aux facteurs influençant le temps qu’un client reste abonné à son service de téléphone cellulaire. Des données provenant d’un échantillon de clients se trouvent dans le fichier survie1, qui contient les variables suivantes:\n\n\\(\\texttt{temps}\\): temps (en semaines) que le client est resté abonné au service de téléphone cellulaire. Il s’agit du vrai temps si le client n’est plus abonné et d’un temps censuré à droite si le client est toujours abonné.\n\\(\\texttt{censure}\\): variable binaire qui indique si la variable \\(\\texttt{t}\\) est censurée (\\(0\\) si le client est toujours abonné) ou non (\\(1\\), la variable \\(\\texttt{t}\\) est la durée finale de l’abonnement).\n\\(\\texttt{age}\\): âge du client au début de l’abonnement.\n\\(\\texttt{sexe}\\): sexe du client, soit femme (\\(1\\)), soit homme (\\(0\\)).\n\\(\\texttt{service}\\): nombre de services en plus du cellulaire auquel le client est abonné parmi internet, téléphone fixe, télévision (câble ou antenne parabolique).\n\\(\\texttt{region}\\): région où habite le client en ce moment (valeurs entre 1 et 5).\n\n\n\n6.1.2 Contexte\nOn s’intéresse au temps avant qu’un événement survienne. On observe chaque sujet jusqu’à ce que l’une des deux choses suivantes se produise : l’événement survient avant la fin de la période d’observation ou bien l’étude se termine et l’événement n’est toujours pas survenu. Dans l’exemple, l’événement correspond au fait d’interrompre son abonnement. On dispose donc d’une variable « temps », que l’on nomme \\(T\\), pour chaque individu qui est soit censurée soit non censurée. Si l’individu a expérimenté l’événement avant la fin de la période d’observation, la valeur de \\(T\\) n’est pas censurée. Si l’événement n’est toujours pas survenu à la fin de la période d’observation, la valeur de \\(T\\) est censurée. Pour chaque individu, on dispose également d’un ensemble de variables explicatives \\(X_1, \\ldots, X_p\\). Pour l’instant, supposons que les valeurs de ces variables sont fixes dans le temps mais on reviendra plus loin au cas où leurs valeurs peuvent varier dans le temps. Bien que le terme analyse de survie semble implicitement référer à la santé, de nombreux autres exemples sont envisageables\n\ntemps qu’un client demeure abonné à un service offert par notre compagnie.\ntemps de survie d’un individu après avoir été diagnostiqué avec un certain type de cancer.\nancienneté d’un travailleur au service d’une compagnie.\ndurée de vie d’une franchise.\ntemps avant la faillite d’une entreprise (ou d’un particulier).\ntemps avant le prochain achat d’un client.\ntemps durant lequel un(e) employé(e) est au chômage.\n\nSi aucune observation n’est censurée, c’est-à-dire, si on a observé le « vrai » temps pour chaque sujet, on pourrait alors simplement modéliser \\(T\\) en incluant des covariables dans les paramètres de vraisemblance d’une loi positive (par exemple, avec une régression log-linéaire). En revanche, si des observations sont censurées dans l’échantillon, leur omission biaiserait l’analyse.\nCe chapitre se veut une introduction à l’analyse de données de survie. Comme le développement de la théorie de l’analyse de survie est assez complexe (plus encore que celle de la régression linéaire ou logistique), on s’intéressera ici uniquement aux principes de base afin d’être en mesure d’appliquer les méthodes et de bien interpréter les résultats. Plusieurs extensions sont également possibles. Un survol de ces dernières sera effectué dans des sections plus loin.\nIl existe deux grandes approches pour analyser des données de survies :\n\nnonparamétrique ou semi-paramétrique: estimateur de Kaplan–Meier, modèle de Cox (à risques proportionnels).\nparamétrique : modèle paramétrique avec loi continue (Weibull, log-normal, log-logistique, gamma).\n\nNous discuterons seulement de la première approche dans ce chapitre. Le tableau suivant fait une analogie entre ce que nous ferons dans ce chapitre et des méthodes que vous connaissez.\n\n\n\n\n\n\n\n\n\nréponse \\(Y\\)\nrésumé descriptif\ncomparaison de deux groupes\nmodèle général\n\n\n\n\ncontinue\nmoyenne\ntest-\\(t\\) pour deux échantillons\nrégression linéaire\n\n\nbinaire\nproportion\ntest d’indépendance du khi-deux\nrégression logistique\n\n\ntemps de survie (censure à droite)\nfonction de survie temps de survie médian\ntest log-rang test de Wilcoxon généralisé (Gehan)\nmodèle de Cox\n\n\n\nLa structure de données de base que l’on doit avoir pour travailler est la suivante:\n\nune variable temps, \\(T\\).\nune variable binaire \\(C\\) (censure).\nd’autres variables explicatives \\(X_1, \\ldots, X_p\\), une par colonne."
  },
  {
    "objectID": "06-survie.html#fonctions-de-survie-et-de-risque",
    "href": "06-survie.html#fonctions-de-survie-et-de-risque",
    "title": "6  Analyse de survie",
    "section": "6.2 Fonctions de survie et de risque",
    "text": "6.2 Fonctions de survie et de risque\nUn des éléments de base d’une analyse de survie (survival analysis) est la fonction (ou courbe) de survie. Soit \\(F(t)=\\Pr(T \\leq t)\\) la fonction de répartition du temps de survie \\(t\\) et \\(f(t) = \\mathrm{d} / \\mathrm{d} t F(t)\\). La fonction de survie est \\[\\begin{align*}\nS(t)= \\Pr(T > t) = 1-F(t)\n\\end{align*}\\] et donne la probabilité que le temps de survie soit supérieur à \\(t\\). On verra plus loin comment estimer cette fonction avec un échantillon et comment tester l’égalité de deux (ou plusieurs) fonctions de survie.\nLa fonction de risque (en anglais, hazard) est \\[\\begin{align*}\nh(t) =  \\frac{f(t)}{S(t)}\n\\end{align*}\\] où \\(f(t)\\) est la fonction de densité (pour \\(T\\) continu) ou de masse pour \\(T\\) discret. Dans le cas discret où le temps peut seulement prendre les valeurs \\(0, 1, 2, \\ldots\\), la fonction de risque est donc simplement la probabilité que l’événement survienne au temps \\(t\\), étant donné qu’il n’était pas survenu avant, \\[\\begin{align*}\n\\Pr(T=t \\mid T > t) = \\Pr(T=t) / \\Pr(T >t) = f(t)/S(t);\n\\end{align*}\\] c’est une probabilité conditionnelle. Dans le cas général, la fonction de risque est nécessairement positive mais peut prendre des valeurs supérieures à un. On ne peut donc pas, à strictement parler, la voir comme une probabilité et c’est pourquoi on parle plutôt de risque. En fait, cette fonction mesure le risque instantané que l’événement survienne au temps \\(t\\), étant donné qu’il n’était pas survenu avant.\nCette fonction est importante car il s’agit de celle que nous allons modéliser avec le modèle de régression de Cox. Si, en régression logistique, on modélise le logarithme des cotes, on modélise plutôt la fonction de risque en analyse de survie. Les fonctions de survie et de risque sont intimement reliées et \\[\\begin{align*}\nh(t) = - \\frac{\\mathrm{d} \\ln\\{S(t)\\}}{\\mathrm{d} t}, \\qquad \\qquad S(t) = \\exp \\left\\{ -\\int_0^t h(u) \\mathrm{d} u\\right\\}.\n\\end{align*}\\] Ainsi, si on connaît la fonction de survie, on peut retrouver la fonction de risque et vice-versa. Par conséquent, un modèle pour la fonction de survie spécifie une fonction de risque (et vice-versa)."
  },
  {
    "objectID": "06-survie.html#estimation-dune-courbe-de-survie-et-de-risque",
    "href": "06-survie.html#estimation-dune-courbe-de-survie-et-de-risque",
    "title": "6  Analyse de survie",
    "section": "6.3 Estimation d’une courbe de survie et de risque",
    "text": "6.3 Estimation d’une courbe de survie et de risque\nL’estimateur nonparamétrique le plus couramment utilisé pour l’estimation de la fonction de survie en présense de censure à droite est l’estimateur de Kaplan–Meier. De plus, cette méthode est nonparamétrique en ce sens qu’on ne suppose aucun modèle et qu’on suppose uniquement que la censure est non-informative.\nSi l’échantillon ne contient aucune observation censurée (on a des temps exacts pour tous les sujets), l’estimateur de Kaplan–Meier de la fonction de survie à un temps \\(t\\) donné est alors simplement la proportion des observations dans l’échantillon qui possède un temps de survie supérieur à \\(t\\). Par convention, on considère qu’une observation censurée à droite faisait partie de l’ensemble d’observations à risque au temps de censure observé.\nOn considère l’exemple des temps d’abonnement pour illustrer le concept. L’estimation de la fonction de survie selon la méthode Kaplan–Meier est obtenue grâce aux commandes suivantes:\n\nlibrary(survival)\ndata(survie1, package = \"hecmulti\")\n# Estimateur de Kaplan-Meier\n# La réponse \"temps\"est le temps de survie \n# et l'indicateur de censure \"censure\" est\n# \"0\" pour censuré à droite, \"1\" pour événement\nkapm <- \n  survfit(Surv(temps, censure) ~ 1, \n          conf.type = \"log\", \n          data = survie1)\nsummary(kapm)\nquantile(kapm)\nplot(kapm, \n     ylab = \"fonction de survie\", \n     xlab = \"temps\") \n\nLa fonction résumé (summary) renvoit l’estimation de la fonction de survie pour chaque temps d’échec (événement): l’estimateur est indéfini à ces valeurs. Le Tableau 6.1 offre une sortie (tronquée) du résumé, modifié pour mieux illustrer les changements. L’estimation de la probabilité que le temps d’abonnement soit supérieur à 30 semaines est \\(\\widehat{S}(30)=0.986\\).\n\n\n\n\nTableau 6.1:  Estimation de la fonction de survie (Kaplan–Meier) pour les données de survie d’abonnement. \n \n  \n    temps \n    nb à risque \n    nb échecs \n    nb cumul. \n    survie \n    erreur-type \n  \n \n\n  \n    2 \n    500 \n    1 \n    1 \n    0.998 \n    0.002 \n  \n  \n    11 \n    499 \n    1 \n    2 \n    0.996 \n    0.003 \n  \n  \n    14 \n    498 \n    1 \n    3 \n    0.994 \n    0.003 \n  \n  \n    18 \n    497 \n    1 \n    4 \n    0.992 \n    0.004 \n  \n  \n    27 \n    496 \n    1 \n    5 \n    0.990 \n    0.004 \n  \n  \n    29 \n    495 \n    1 \n    6 \n    0.988 \n    0.005 \n  \n  \n    30 \n    494 \n    1 \n    7 \n    0.986 \n    0.005 \n  \n  \n    34 \n    493 \n    4 \n    11 \n    0.978 \n    0.007 \n  \n  \n     \n     \n     \n     \n     \n     \n  \n  \n    189 \n    13 \n    1 \n    331 \n    0.204 \n    0.028 \n  \n  \n    202 \n    6 \n    1 \n    332 \n    0.170 \n    0.039 \n  \n  \n    216 \n    2 \n    2 \n    334 \n    0.000 \n     \n  \n\n\n\n\n\n\n\nggsurv <- survminer::ggsurvplot(kapm, palette = 1)\nggsurv$plot + # objet de class 'ggplot'\n  theme(legend.position = \"none\") + \n  labs(x = \"temps\", \n       subtitle = \"Fonction de survie\",\n       y = \"\")\n\n\n\n\nFigure 6.3: Estimation de Kaplan–Meier de la fonction de survie pour les données d’abonnement avec intervalles de confiance ponctuels à 95%.\n\n\n\n\nOn peut également utiliser la fonction quantile pour obtenir une estimation des quartiles et un intervalle de confiance On utilise généralement le temps de survie médian (au lieu de la moyenne) dans ce type d’étude. Ici, l’estimé du temps de survie médian est de 114 semaines: on estime que la moitié des clients vont avoir une durée d’abonnement supérieure à 114 semaines. De même, la moitié des clients vont avoir une durée d’abonnement inférieure à 114 semaines. Un intervalle de confiance de niveau 95% pour ce temps médian est [\\(110; 119\\)].\nUn estimé de la moyenne et de l’écart-type est donné, mais ce dernier est biaisé (trop bas) puisque les données censurées ne donnent qu’une borne inférieure pour la vraie valeur. Avec un modèle paramétrique pour la survie (par ex., une loi exponentielle), les paramètres estimés du modèle dicteraient ces deux valeurs. Le modèle de Kaplan–Meier estime la survie, mais si la plus grande observation est censurée, la courbe n’atteindra pas zéro.\nLe graphique de la fonction de survie permet de lire le temps de survie pour une probabilité donnée. Les bandes donnent un intervalle de confiance ponctuel de niveau 95% pour chaque temps donné.\nUne information pertinente de la sortie est le nombre de données censurées: parmi les 500 observations, il y a 334 clients qui ont terminé leur abonnement et 166 qui sont censurées (le client est toujours abonné et le temps est donc une borne inférieure de la durée d’abonnement). Les données censurées contiennent moins d’information que les temps observés de défaillance puisqu’on sait uniquement la borne inférieure de la plage possible des valeurs pour le vrai temps de défaillance.\nLes statistiques descriptives usuelles sont biaisées en raison de la censure. Avec l’estimateur de Kaplan–Meier, on peut aisément obtenir les quantiles. Si la courbe de survie estimée descend à zéro, il est également possible d’estimer la moyenne en calculant l’aire sous la courbe de survie.1 Si la courbe ne descend pas à zéro, on obtient une borne inférieure (sous-estimation de la moyenne), appelée moyenne restreinte.\n\nprint(kapm, print.rmean = TRUE)\n\nPar exemple, la moyenne estimée via Kaplan–Meier est 125 semaines (rmean), à comparer avec la moyenne empirique de temps, ici de 107.788 semaines, qui est biaisée.\n\n6.3.1 Calcul de l’estimateur de Kaplan–Meier\nPour comprendre l’estimateur de Kaplan–Meier, il est utile de s’intéresser à sa construction. Deux éléments sont essentiels: on parle d’échec ou d’événement au temps \\(t_i\\) si l’événement est observé (\\(T_i=t_i\\)) au temps \\(t_i\\). Le nombre de personnes à risque au temps \\(t_i\\) est le total des observations dont le temps mesuré excède \\(t_i\\) (censure et événements postérieurs à \\(t_i\\))\nPour la construction, on procède comme suit:\n\nOrdonner les temps (uniques) où il y a des échecs (temps où censure = 1), disons \\(t_{(1)} \\leq \\cdots \\leq t_{(m)}\\)\nÀ chaque temps \\(t_{(i)}\\) \\((i=1, \\ldots m)\\), on calcule le nombre de personnes à risque, \\(r_i\\), et le nombre d’échecs, \\(d_i\\).\nLe risque empirique est \\(\\widehat{h}_i = r_i/d_i\\), la proportion d’échecs parmi les personnes à risque.\n\nL’estimateur de Kaplan–Meier définit une fonction escalier\n\nEntre \\(t=0\\) et \\(t=t_{(1)}\\), la survie est de 1.\nEntre \\(t=t_{(1)}\\) et \\(t=t_{(2)}\\), la survie est \\(1-\\widehat{h}_1\\).\nEntre \\(t=t_{(2)}\\) et \\(t=t_{(3)}\\), la survie est \\((1-\\widehat{h}_1) \\times (1-\\widehat{h}_2)\\), etc.\n\nPour un temps \\(t\\) donné, on multiplie tous les termes (\\(1-\\widehat{h}_i\\)) des temps d’échecs passés, \\[\\begin{align*}\n\\widehat{S}(t) &= \\prod_{i: t_{(i)} < t} \\left( 1- \\widehat{h}_i\\right) \\\\ &= \\left( 1- \\frac{d_1}{r_1}\\right) \\times \\cdots \\times \\left( 1- \\frac{d_{i(t)}}{r_{i(t)}}\\right).\n\\end{align*}\\] où \\(i(t) =\\max(j \\in \\{1, \\ldots, m\\}: t \\geq t_{j})\\), soit le plus grand indice parmi \\(1, \\ldots, m\\) tel que \\(t \\geq t_{i(t)}\\). Par convention, si \\(t < t_{(1)}\\), on fixe \\(\\widehat{S}(t)=1\\). La fonction de survie n’est pas définie aux temps de défaillance observée, mais la convention veut qu’elle soit continue à gauche.\nAinsi, l’estimation de la survie estimée ne change qu’aux valeurs de \\(t_{(i)}\\) (\\(i=1, \\ldots, m\\)). Les contre-marches de l’escalier ainsi défini interviennent uniquement aux temps observés d’échecs. Si à un moment donné toutes les personnes à risque expérimentent l’événement, la courbe descend à zéro. Si on a uniquement de la censure à droite, la courbe de survie n’atteindra jamais zéro si la plus grande observation est censurée à droite. Avec la troncation à gauche, la même logique s’applique mais les personnes ne sont à risque qu’à partir du temps minimum observé."
  },
  {
    "objectID": "06-survie.html#comparaison-de-deux-courbes-de-survie",
    "href": "06-survie.html#comparaison-de-deux-courbes-de-survie",
    "title": "6  Analyse de survie",
    "section": "6.4 Comparaison de deux courbes de survie",
    "text": "6.4 Comparaison de deux courbes de survie\nSupposons que les individus ont été divisés en deux groupes et que \\(S_1(t)\\) et \\(S_2(t)\\) dénotent respectivement la fonction de survie du premier groupe et du deuxième groupe. On est souvent intéressé à tester l’égalité des fonctions de survie, c’est-à-dire, les hypothèses \\(\\mathscr{H}_0: S_1(t) = S_2(t)\\) pour tout \\(t\\) et \\(\\mathscr{H}_1: S_1(t) \\neq S_2(t)\\) pour au moins une valeur de \\(t\\).\nPar exemple, dans une étude sur le temps de survie après avoir été diagnostiqué avec un certain type de cancer, on pourrait vouloir comparer le temps de survie des individus ayant reçu le traitement standard (groupe 1) au temps de survie des individus ayant reçu un nouveau traitement (groupe 2).\nLes deux tests utilisés habituellement sont le test du log-rang (log-rank test) et le test de Wilcoxon généralisé (ou test de Gehan).\nTestons l’hypothèse que la courbe de survie des clients masculins est la même que celle des clients féminins dans l’exemple des données d’abonnement:\n\nstrat_sexe <- survfit(Surv(temps, censure) ~ sexe, data = survie1)\nlograng <- survdiff(Surv(temps, censure) ~ sexe, data = survie1)\n\nSi on utilise les méthodes (print, summary, plot, quantile) pour la sortie de survfit, on obtiendra les résultats pour chaque strate (ou niveaux de la variable catégorielle). Par exemple, il y a 309 hommes et 191 femmes et l’estimation du temps de survie médian est de 110 semaines pour les hommes et de 123 semaines pour les femmes.\nLa fonction survdiff avec la formule retourne le résultat du test asymptotique du log-rang pour l’hypothèse d’égalité des fonctions de survie. La statistique du khi-deux vaut 16.43 et la valeur-\\(p\\) du test est inférieure à \\(10^{-4}\\): on rejette donc \\(\\mathscr{H}_0\\) pour conclure qu’il y a donc une différence significative entre les deux courbes de survie.\nLes courbes de survie selon le sexe sont représentées dans la Figure 6.4. On voit que la courbe des femmes est systématiquement au-dessus de celle des hommes. Les femmes ont donc tendance à rester abonnées plus longtemps que les hommes, et cette différence est significative.\n\nplot(survfit(Surv(temps, censure) ~ sexe, \n             data = survie1), \n     conf.int = FALSE,\n     col = c(\"red\", \"blue\"), \n     xlab = \"temps\", \n     ylab = \"fonction de survie\")\n\n\n\n\n\n\nFigure 6.4: Courbes de survie pour les durées d’abonnement selon le sexe de l’individu.\n\n\n\n\nIl est également possible de tester l’égalité des courbes de survie avec plus de deux groupes. Par exemple, s’il y a \\(k\\) groupes, l’hypothèse nulle est alors \\(\\mathscr{H}_0: S_1(t)=S_2(t)=\\cdots = S_k(t)\\) pour tout \\(t\\), versus l’alternative qu’au moins deux des fonctions ont une valeur différente pour au moins une valeur de \\(t\\); la loi nulle asymptotique du test est alors \\(\\chi^2_{k-1}\\).\nL’estimateur de Kaplan–Meier ne permet pas l’inclusion de variables explicatives à proprement parler: si on peut veut les différences au niveau de la survie selon les modalités d’une variable explicative catégorielle, on divise pour ce faire l’échantillon en sous-groupes et on utilise l’estimateur de Kaplan–Meier pour chacune des modalités en gardant en tête que cela réduit la taille de l’échantillon disponible et que l’estimation résultante est possiblement trop incertaine pour être utile.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nL’analyse de survie est l’étude de temps d’attente (variable positive) avant que survienne un événement.\nL’étude des temps de défaillance nécessite l’utilisation d’outils statistiques spécifiques en raison des mécanismes de censure et de troncation.\nLa fonction de survie \\(S(t)\\) encode la probabilité que le temps de défaillance excède le temps \\(t\\).\nLa fonction de risque encode la probabilité de mourir au temps \\(t\\) sachant qu’on a survécu jusque là.\nLa connaissance de la fonction de survie permet d’obtenir la fonction de risque et vice-versa.\nLe mécanisme d’information partielle le plus courant en analyse de survie est la censure à droite (on ne connaît qu’une borne inférieure pour le temps de défaillance, l’événement n’étant toujours pas survenu au temps donné).\nSi on traitait les temps de censure comme des temps de défaillance observée, on sous-estimerait la durée de survie.\nL’estimateur de Kaplan–Meier est l’estimateur du maximum de vraisemblance nonparamétrique si on a de la censure à droite aléatoire ou non-informative. Il ne fait aucun postulat sur la distribution de la survie.\nPour que l’estimation soit de qualité, il faut un nombre conséquent d’observations (disons 1000). La quantité d’observations censurées impacte la précision de l’estimation.\nL’estimateur est déficient si le plus grand temps observé est censuré à droite (l’estimation de la fonction de survie ne décroît pas à zéro).\nLe test du log-rang permet de valider si deux fonctions de survie sont égales (en tout temps).\nOn peut estimer la fonction de survie indépendamment pour chaque modalité d’une variable explicative catégorielle en stratifiant: cela réduit la taille de l’échantillon pour chaque strate.\nLe modèle de Kaplan–Meier ne permet pas d’estimer l’impact de variables explicatives sur la survie."
  },
  {
    "objectID": "06-survie.html#modèle-à-risques-proportionnels-de-cox",
    "href": "06-survie.html#modèle-à-risques-proportionnels-de-cox",
    "title": "6  Analyse de survie",
    "section": "6.5 Modèle à risques proportionnels de Cox",
    "text": "6.5 Modèle à risques proportionnels de Cox\nLe modèle à risques proportionnels de Cox (proportional hazard model) est l’un des modèles les plus utilisés pour l’analyse des données de survie.\n\n6.5.1 Description du modèle de Cox\nSoit \\(h(t; \\boldsymbol{x})\\) la valeur de la fonction de risque au temps pour un individu dont les valeurs des variables explicatives sont \\(X_1=x_1, \\ldots, X_p=x_p\\). Le modèle à risques proportionnels est \\[\\begin{align*}\nh(t; \\boldsymbol{x}) = h_0(t)\\exp(\\beta_1x_1 + \\cdots + \\beta_p x_p)\n\\end{align*}\\] où \\(h_0(t)\\) est la fonction de risque de base; il n’est pas nécessaire de spécifier cette dernière, d’où la nature semiparamétrique du modèle de Cox. Le postulat de risques proportionnels implique que le terme de droite \\(\\exp(\\mathbf{X}\\boldsymbol{\\beta})\\) ne dépend pas du temps, et plus particulièrement \\(\\beta_1, \\ldots, \\beta_p\\) ne dépend pas du temps. Nous verrons subséquemment une extension qui permet de prendre en compte les variables explicatives dont la valeur change dans le temps en scindant ces observations.\nLorsque toutes les variables explicatives prennent la valeur zéro, \\(\\boldsymbol{X}=\\boldsymbol{0}\\), on recouvre \\(h(t; \\boldsymbol{0})= h_0(t)\\). Par conséquent, la fonction \\(h_0(t)\\) peut être interprétée comme la fonction de risque lorsque toutes les variables explicatives valent zéro. Toutefois, tout comme la valeur de l’ordonnée à l’origine dans un modèle de régression linéaire, cette interprétation n’est pas nécessairement valide si la situation où toutes les variables explicatives valent zéro n’est pas possible ou si elle ne survient pas dans notre échantillon.\nLa deuxième partie du modèle, \\(\\exp(\\beta_1x_1 + \\cdots + \\beta_p x_p)\\), vient modéliser l’effet d’un changement des valeurs des variables explicatives sur la fonction de risque de base. Tout comme dans le cas de la régression logistique (l’effet des variables sur la cote), c’est un effet multiplicatif, d’où le terme risques proportionnels.\nPour l’interprétation des paramètres, il sera plus simple de penser en termes de rapport de risque (hazard ratio), qui est défini comme étant le rapport des fonctions de risque pour deux ensembles de valeurs des variables explicatives. Pour simplifier l’illustration, supposons que nous avons seulement une variable explicative \\(X\\) et que \\(h(t; x) = h_0(t)\\exp(\\beta x)\\). Le rapport de risque lorsque \\(X=x_1\\) par rapport à \\(X=x_0\\) est \\[\\begin{align*}\n\\frac{h(t; x_1)}{h(t; x_0)} = \\exp\\{\\beta(x_1-x_0)\\}.\n\\end{align*}\\] Par conséquent, l’impact d’une augmentation de \\(X\\) d’une unité (quand \\(x_1-x_0=1\\)) est \\(\\exp(\\beta)\\). Ainsi, pour chaque augmentation d’une unité pour \\(X\\), le risque que l’événement survienne est multiplié par \\(\\exp(\\beta)\\).\nLe terme risques proportionnels fait référence à la situation où le rapport de risque dépend seulement de la différence \\(x_1-x_0\\) et non pas du temps lui-même. Le rapport de risque est constant par rapport au temps \\(t\\). Cela implique que l’effet d’une variable est stable dans le temps. Nous verrons plus loin comment faire en sorte que l’effet d’une variable puisse varier dans le temps.\nDébutons avec un exemple simple en utilisant les données d’abonnement: on ajuste un modèle de Cox en utilisant seulement la variable binaire sexe.\n\ncox1 <- coxph(Surv(temps, censure) ~ sexe, \n              data = survie1,\n              ties = \"exact\")\n# Coefficients, tests et IC 95% de Wald\nsummary(cox1)\n# Test du rapport de vraisemblance\ncar::Anova(cox1, type = 3)\n\nLa sortie inclut notamment des tests de significativité globale basés sur la vraisemblance comparant le modèle sans variable explicative avec celui ajusté (rapport de vraisemblance, score et Wald). Ces trois statistiques ciblent la même hypothèse, aussi on peut ne s’attarder qu’au test du rapport de vraisemblance, qui est plus fiable et généralement plus puissant. Le tableau des coefficients donne les estimations \\(\\widehat{\\beta}\\). Le rapport de risque pour sexe=1 versus sexe=0 est \\(\\exp(\\widehat{\\beta})\\), tandis qu<on obtiendrait le rapport de risque pour sexe=0 versus sexe=1, soit \\(\\exp(-\\widehat{\\beta})\\). Les statistiques de test et la valeur-\\(p\\) associée sont basées sur la statistique de Wald, soit \\(\\widehat{\\beta}/\\mathsf{se}(\\widehat{\\beta})\\), tandis que les intervalles de confiance à 95% pour le rapport de risque sont des intervalles de Wald de la forme \\(\\exp\\{\\widehat{\\beta} \\pm 1.96 \\cdot \\mathsf{se}(\\widehat{\\beta})\\}\\) pour le rapport de risque. Un rapport de risque de 1 signifie que le risque n’est pas affecté par la variable explicative. Ici, le test correspondant à l’hypothèse nulle \\(\\beta=0\\) ne mène pas au rejet de l’hypothèse qui veut que la variable n’impacte pas le risque. On pourrait obtenir les intervalle\n\n\n\n\nTableau 6.2:  Rapport de risques et intervalles de confiance de Wald à niveau 95%. \n \n  \n    terme \n    exp(coef) \n    borne inf. \n    borne sup. \n  \n \n\n  \n    sexe \n    0.63 \n    0.5 \n    0.79 \n  \n\n\n\n\n\n\nIl y a une seule variable explicative, le sexe de l’individu. L’estimation du paramètre de l’effet de sexe est -0.47. Ce paramètre est significativement différent de \\(0\\) (valeur-\\(p\\) du test du rapport de vraisemblance inférieure à \\(10^{-10}\\)). Pour l’interprétation, on utilise la colonne exp(coef) qui contient la valeur \\(\\exp(\\hat{\\beta}_{\\texttt{sexe}}) = \\exp(-0.466) = 0.628\\). Ainsi, le rapport du risque d’une femme par rapport à un homme est \\[\\begin{align*}\n\\frac{\\hat{h}(t; \\texttt{sexe}=1)}{\\hat{h}(t; \\texttt{sexe}=0)}= 0.628.\n\\end{align*}\\] Par conséquent, le risque qu’une femme interrompe son abonnement est 0.628 fois celui d’un homme. Une femme est donc moins à risque de quitter qu’un homme. Nous avions déjà vu cela à la section précédente lorsque nous avions comparé les courbes de survie des hommes et des femmes. Il est important de se rappeler qu’avec ce modèle, l’effet d’une variable est le même dans le temps (peu importe la valeur de \\(t\\)). Donc, une femme est moins à risque de quitter qu’un homme à tout moment, d’après ce modèle. Inversement, le ratio du risque d’un homme par rapport à une femme est \\(\\exp(-\\hat{\\beta}_{\\texttt{sexe}}) = 1/0.628=1.59\\). Ainsi, à tout moment, un homme a un risque d’interrompre son abonnement qui est 59% plus élevé que celui d’une femme.\nComme il y a un seul paramètre ici, les tests basés sur la vraisemblance pour \\(\\mathscr{H}_0: \\boldsymbol{\\beta}=\\boldsymbol{0}\\) reviennent à tester l’effet de la variable sexe. Le test de Wald est le même que celui du tableau des coefficients. Dans le cas particulier où il y a une seule variable explicative catégorielle comme ici, le test du score est équivalent au test du log-rang que nous avons vu à la section précédente à une petite différence près lorsqu’il y a des doublons (ex aequo) dans les temps de survie.\nOn pourrait également utiliser une variable explicative continue plutôt qu’une variable binaire; le principe est le même.\n\ncox2 <- coxph(Surv(temps, censure) ~ age, \n              data = survie1,\n              ties = \"exact\")\nsummary(cox2)\n\nLe rapport de risque pour âge est 0.959 et donc le risque diminue de 4.1% chaque fois que l’âge augmente d’un an — le risque d’interrompre l’abonnement diminue lorsque l’âge augmente et cet effet est significatif (valeur-\\(p\\) du test de significativité globale inférieures à \\(10^{-4}\\)).\nGénéralement, on considérera le modèle de Cox avec toutes les variables explicatives simultanément. La variable \\(\\texttt{region}\\) est nominale tandis que la variable \\(\\texttt{service}\\) est ordinale (avec quatre modalités). Nous allons les incorporer, comme d’habitude, en utilisant des variables indicatrices avec \\(\\texttt{region=1}\\) et \\(\\texttt{service=0}\\) (le client n’est abonné à aucun autre service) comme catégories de référence.\n\nwith(survie1, table(service))\ncox3 <- coxph(Surv(temps, censure) ~ \n                age + sexe + region + service, \n              data = survie1, \n              ties = \"exact\")\nsummary(cox3)\n# Effets de type 3 \n# (modèle avec toutes les variables sauf une)\ncar::Anova(cox3, \n           type = 3)\n\n\n\n\n\nTableau 6.3:  Rapport de risque et intervalles de confiance à niveau 95% de Wald pour le modèle de Cox de base avec toutes les variables explicatives. \n \n  \n    terme \n    exp(coef) \n    borne inf. \n    borne sup. \n  \n \n\n  \n    age \n    0.95 \n    0.94 \n    0.96 \n  \n  \n    sexe \n    0.51 \n    0.40 \n    0.65 \n  \n  \n    region2 \n    0.67 \n    0.47 \n    0.98 \n  \n  \n    region3 \n    1.03 \n    0.73 \n    1.46 \n  \n  \n    region4 \n    0.80 \n    0.57 \n    1.13 \n  \n  \n    region5 \n    0.97 \n    0.68 \n    1.37 \n  \n  \n    service1 \n    0.35 \n    0.27 \n    0.45 \n  \n  \n    service2 \n    0.17 \n    0.12 \n    0.25 \n  \n  \n    service3 \n    0.12 \n    0.07 \n    0.19 \n  \n\n\n\n\n\n\n\n\n\n\nTableau 6.4:  Tests du rapport de vraisemblance pour les effets de type III pour le modèle de Cox avec toutes les variables explicatives. \n \n  \n    terme \n    statistique \n    ddl \n    valeur-p \n  \n \n\n  \n    age \n    68.07 \n    1 \n    <1e-04 \n  \n  \n    sexe \n    32.82 \n    1 \n    <1e-04 \n  \n  \n    region \n    7.67 \n    4 \n    0.1 \n  \n  \n    service \n    159.31 \n    3 \n    <1e-04 \n  \n\n\n\n\n\n\nLes effets des variables sont maintenant des effets marginaux. Ainsi, lorsque les autres variables demeurent fixes, le risque de quitter d’une femme est 0.511 fois plus petit que celui d’un homme. L’effet marginal (une fois que les autres variables sont incluses) de la variable sexe est significatif (valeur-\\(p\\) inférieure à \\(10^{-4}\\)).\nToutes autres choses étant égales, chaque augmentation de l’âge d’un an fait diminuer le risque d’interrompre l’abonnement. Plus précisément, le risque est multiplié par 0.95 lorsque l’âge augmente d’un an et cet effet est significatif.\nPour la variable service, l’interprétation se fait par rapport à la catégorie de référence, qui est la catégorie \\(\\texttt{0}\\) (abonné à aucun autre service). Ainsi, si le client est abonné à un autre service, son risque de quitter est 0.353 fois celui d’un client qui n’est pas abonné à un autre service (toutes autres choses étant égales). Si le client est abonné à deux autres services, son risque de quitter est encore plus petit comparativement à un client qui n’est pas abonné à un autre service (rapport de risque de 0.174) Finalement, si le client est abonné à trois autres services, son risque de quiter est encore plus petit (rapport de risque de 0.115). Les paramètres de ces trois variables sont tous significatifs. Ainsi, les clients qui sont abonnés à un, deux ou trois services ont un risque de quitter qui est significativement plus faible que celui d’un client qui n’est pas abonné à un autre service. Le tableau d’analyse de déviance (effets de type 3) donne les statistiques du rapport de vraisemblance pour tester globalement la significativité d’une variable explicative modélisée avec plusieurs indicatrices. Pour la variable \\(\\texttt{service}\\), le test présenté teste l’hypothèse nulle \\(\\mathscr{H}_0: \\beta_{\\texttt{service}_1}=\\beta_{\\texttt{service}_2}=\\beta_{\\texttt{service}_3}=0\\) contre l’alternative qu’un moins un de ces trois paramètres est non-nul. Le test est largement significatif (statistique du rapport de vraisemblance valant 159.31 avec une valeur-\\(p\\) inférieure à \\(10^{-4}\\)). L’effet de la variable \\(\\texttt{service}\\) est donc globalement significatif. Afin de comparer les autres modalités entre elles, par exemple afin de voir si le risque de quitter est différent entre un client qui a deux services et un autre qui a trois services, il suffit de changer la catégorie de référence à la commande class et de réajuster le modèle.\nFinalement, la variable \\(\\texttt{region}\\) n’est pas globalement significative (statistique du rapport de vraisemblance de 7.67 avec une valeur-\\(p\\) de 0.1 basée sur la loi asymptotique \\(\\chi^2_4\\)).\n\n\n6.5.2 Estimation de la fonction de survie pour des valeurs particulières des variables explicatives\nIl est possible d’obtenir l’estimation de la fonction de survie pour des valeurs particulières des variables explicatives. Pour ce faire, il faut avoir un autre fichier de données qui contient les valeurs des variables explicatives pour lesquelles on veut une estimation de la fonction de survie. Si on ajuste le modèle avec aucune variable explicative, on retrouvera alors l’estimation de Kaplan–Meier de la fonction de survie.\nSupposons qu’on ajuste le modèle avec les variables sexe et âge seulement dans l’exemple du temps d’abonnement, et que l’on désire la fonction de survie pour les hommes de 25 et 60 ans et pour les femmes de 25 et 60 ans. Le fichier survie2 contient les données qui seront utilisées à cette fin.\n\n\n\n\n\nFigure 6.5: Courbes de survies du modèle de Cox pour les quatre profils client.\n\n\n\n\n\n\n\nLes quatre fonctions de la Figure 6.5 correspondent aux profils pour lesquels nous désirons une estimation de la courbe de survie. La courbe 1 est pour les hommes de 25 ans, la courbe 2 pour les femmes de 25 ans, la courbe 3 pour les hommes de 60 ans et la courbe 4 pour les femmes de 60 ans. On voit donc que, parmi ces quatre profils, les hommes de 25 ans sont le plus à risque de quitter tandis que les femmes de 60 ans sont le moins à risque de quitter.\n\n\n6.5.3 Variables explicatives dont la valeur change dans le temps\nIl est clair que certaines caractéristiques d’un individu évoluent dans le temps (time-varying covariates). Si le sexe d’un individu est stable dans le temps, son revenu, son statut matrimonial, l’endroit où il habite, sont par contre des caractéristiques qui peuvent changer dans le temps. Il peut alors être intéressant d’en tenir compte dans l’analyse. Rappelez-vous que le modèle à risques proportionnels est \\[\\begin{align*}\nh(t; \\boldsymbol{x}) = h_0(t) \\exp(\\beta_1x_1 + \\cdots + \\beta_px_p).\n\\end{align*}\\]\nSupposons que la variable \\(X_1\\) change au fil du temps et que les autres demeurent fixes. On peut alors réécrire le modèle \\[\\begin{align*}\nh(t; \\boldsymbol{x}) = h_0(t) \\exp\\{\\beta_1x_1(t) + \\cdots + \\beta_px_p\\},\n\\end{align*}\\] où \\(x_1(t)\\) indique que la valeur de \\(X_1\\) dépend du temps \\(t\\).\nSupposons que la variable \\(\\texttt{service}\\), qui représente le nombre d’autres services souscrits, est la seule que nous voulons modéliser comme une variable qui varie dans le temps. Pour l’âge, nous prenons simplement l’âge au début de l’abonnement, idem pour la région.\nLe plus difficile est de créer correctement le fichier de données pour effectuer ce genre d’analyse. Pour chaque personne, on peut identifier plusieurs moments où l’une ou l’autres des valeurs des variables explicatives change. Supposons qu’on a observé un événement au temps \\(t_1\\), et que la valeur de la covariable change à \\(t_0\\), où \\(0 < t_0 < t_1\\). On peut envisager cette observation donne deux contributions: une pour la trajectoire sur l’intervalle \\((0, t_0]\\) (censure à droite) et l’autre, après la modification, sur l’intervalle \\((t_0, t_1]\\) (troncature à gauche). Puisque la valeur est observée passée le premier intervalle, on a besoin de l’information sur toute la fenêtre (les deux bornes) et le type d’événement pour la première fenêtre sera de la censure à droite.\n\n\n\n\n\nFigure 6.6: Diagramme de Lexis avec trajectoires. Pour ajuster le modèle, on peut casser la contribution d’une observation en segments: considérons un seul changement survenant au temps \\(t_c\\). Pour le premier segment, on enregistre \\(t_c\\) comme valeur maximale (censure à droite), tandis que pour la deuxième portion, l’observation est tronquée à gauche à partir de \\(t_c\\).\n\n\n\n\nLa base de données survie3 contient le format adéquat: une colonne evenement qui indique la censure (tout intervalle intermédiaire pour un individu est traité comme de la censure à droite) et les bornes de la fenêtre, debut et fin. Dans cet exemple, il y a eu au plus un changement dans la variable \\(\\texttt{service}\\), comme présenté dans le fichier survie3. Les variables explicatives \\(\\texttt{age}\\), \\(\\texttt{sexe}\\) et \\(\\texttt{region}\\) sont comme précédemment.\n\n\n\n\nTableau 6.5:  Aperçu des cinq premières observations de la base de données survie3. \n \n  \n    id \n    debut \n    fin \n    evenement \n    age \n    sexe \n    region \n    service \n  \n \n\n  \n    1 \n    0 \n    130 \n    0 \n    48 \n    1 \n    3 \n    2 \n  \n  \n    1 \n    130 \n    178 \n    0 \n    48 \n    1 \n    3 \n    1 \n  \n  \n    2 \n    0 \n    159 \n    0 \n    31 \n    1 \n    3 \n    2 \n  \n  \n    3 \n    0 \n    110 \n    1 \n    36 \n    1 \n    4 \n    0 \n  \n  \n    4 \n    0 \n    109 \n    1 \n    30 \n    0 \n    2 \n    0 \n  \n  \n    5 \n    0 \n    78 \n    0 \n    22 \n    0 \n    5 \n    1 \n  \n  \n    5 \n    78 \n    108 \n    1 \n    22 \n    0 \n    5 \n    0 \n  \n\n\n\n\n\n\nOn regarde plus en détail le profil des cinq premiers clients présenté dans le Tableau 6.5, dont seuls deux ont changé le nombre d’abonnements; les individus 3–5 se sont désabonnés du service cellulaire à un moment donné. Le premier client était abonné à deux autres services au début de son abonnement au téléphone cellulaire mais, après 130 semaines d’abonnement, a effectué un changement à son forfait pour ne conserver qu’un autre service en plus du cellulaire. Pour le deuxième client, comme \\(\\verb+temps_ch+\\) est manquante, il est toujours abonné à deux autres services et ce, jusqu’à la fin de l’étude.\n\ndata(survie3, package = \"hecmulti\")\ncox4 <- coxph(Surv(time = debut, \n                   time2 = fin, \n                   event = evenement) ~ \n                age + sexe + region + service, \n              data = survie3)\n\nL’interprétation se fait comme précédemment; on a omis l’option ties = \"exact\" parce que cette option est trop gourmande en calcul. Puisque c’est la valeur d’une variable qui varie dans le temps et non pas son effet, on a l’interprétation usuelle. Par exemple, le risque de quitter pour un client qui a un autre service est 0.601 fois celui d’un client qui n’a aucun autre service (référence). Le fait d’avoir deux ou trois services diminue encore plus le risque de quitter (rapports de risque de 0.265 et 0.209, respectivement).\n\n\n6.5.4 Postulat de risques proportionnels\nLe modèle de Cox fait l’hypothèse que la fonction de risque de base est identique peut importe la valeur des variables catégorielles. La Figure 6.7 illustre à quoi ce postulat correspond dans un cas simple où il y a uniquement une variable binaire explicative.\n\n\n\n\n\nFigure 6.7: Courbes de risques proportionnelles (panneau supérieur) et non proportionnelles (panneau inférieur) avec rapport de risque et fonctions de survie correspondantes.\n\n\n\n\nSi ce n’est pas le cas, alors les résultats du modèle ne sont pas nécessairement fiables.Comme pour un modèle de régression, il est possible de créer des résidus du modèles et de faire des graphiques diagnostics pour potentiellement infirmer le postulat de risques proportionnels (Grambsch and Therneau 1994). Si l’hypothèse tient la route, alors il ne devrait pas y avoir de tendance temporelle dans les résidus.\nLa commande cox.zph permet de tester le postulat de risques proportionnels à l’aide d’un test du score pour voir si la pente \\(\\beta(t)\\) associée à une covariable est nulle en fonction du temps \\(t\\); si la valeur-\\(p\\) est grande, cela indique une absence de preuve. La fonction plot permet également d’obtenir un graphique des résidus en fonction du temps.\n\n\n\n\n\nFigure 6.8: Estimations des coefficients en fonction du temps basés sur les moindres carrés pondérés (diagnostic graphique de Grampsch et Therneau).\n\n\n\n\n\n\n\n\nTableau 6.6:  Postulat de risques proportionnels: test du score de Grampsch et Therneau (1994) pour les coefficients constants dans le temps et valeur-p asymptotique basée sur la loi nulle khi-deux. \n \n  \n    effet \n    score \n    ddl \n    valeur-p \n  \n \n\n  \n    age \n    4.22 \n    1 \n    0.040 \n  \n  \n    sexe \n    1.11 \n    1 \n    0.291 \n  \n  \n    region \n    3.81 \n    4 \n    0.432 \n  \n  \n    service \n    10.97 \n    3 \n    0.012 \n  \n  \n    global \n    21.23 \n    9 \n    0.012 \n  \n\n\n\n\n\n\nDans la Figure 6.8, on voit que le coefficient pour service augmente au fil du temps pour tous les groupes. On pourrait capturer cette interaction ou stratifier pour calculer le risque selon le nombre de services, au risque d’avoir trop peu d’observations pour estimer de manière fiable le risque de base. C’est explicable par le fait que les personnes avec plus de services tendent à avoir une survie plus longue.\n\ndiag_risqueprop <- cox.zph(cox3)\nprint(diag_risqueprop)\nplot(diag_risqueprop)\n\nSi le postulat n’est pas validé, on peut interpréter l’effet comme un rapport de risque moyen pondéré sur la période de suivi, mais ce dernier change selon le moment (Stensrud and Hernán 2020). Cela implique également que les erreurs-types associées aux estimations sont trompeuses. La section suivante permettra de généraliser le modèle de Cox et traiter ce cas de figure.\n\n\n6.5.5 Stratification\nUne manière de modéliser la non-proportionnalité pour une variable catégorielle est par la stratification. Supposons que nous avons une variable explicative catégorielle \\(Z=1, \\ldots, K\\) pour lequel le postulat de risque proportionnels n’est pas valide. On s’intéresse à l’effet des variables \\(\\mathbf{X}\\). Typiquement, on ne s’intéresse pas directement à l’effet de la variable \\(Z\\). Le modèle de Cox avec stratification (pour la variable \\(Z\\)) est \\[\\begin{align*}\nh(t; \\mathbf{x}, z=k) = h_k(t) \\exp(\\boldsymbol{\\beta} \\mathbf{x}),\n\\end{align*}\\] où \\(h_k(t)\\) est la fonction de risque de base quand \\(Z=k\\) (\\(k=1, \\ldots, K\\)). L’effet des autres variables explicatives \\(\\mathbf{X}\\) est supposé être le même peut importe la valeur de \\(Z\\), mais la fonction de risque de base peut différer. Le rapport de risque pour \\(Z=k\\) versus \\(Z=j\\) est \\(h_k(t)/h_j(t)\\); cette quantité dépend du temps \\(t\\), mais pas des autres caractéristiques mesurées par \\(\\mathbf{X}\\). L’effet de la variable est donc variable dans le temps (et non pas constant). Ce modèle permet donc de modéliser la non-proportionnalité pour la variable \\(Z\\). Si on stratifie par rapport à une variable, il ne faut pas l’inclure dans le modèle en plus car elle est déjà modélisée via la stratification. Notez que les paramètres \\(\\boldsymbol{\\beta}\\) seront estimés à l’aide des données de toutes les strates, mais les fonctions de risque \\(h_k(t)\\) seront obtenues à l’aide des sous-échantillons correspondant aux valeurs de \\(Z\\).\nL’avantage de la stratification est que cette méthode permet de modéliser n’importe quel changement dans l’effet d’une variable dans le temps sans devoir spécifier un type de changement particulier, comme lorsqu’on doit choisir la forme de l’interaction.Il est important de comprendre qu’on ne pourra pas estimer l’effet de la variable de stratification comme d’ordinaire en étudiant le coefficient \\(\\beta\\) associé. On perd la possibilité de tester l’effet de la variable de stratification et on réduit la taille de l’échantillon pour l’estimation de la fonction de risque de base. On devrait principalement utiliser la stratification seulement avec des variables pour lesquelles nous n’avons pas besoin d’estimer l’effet (variables secondaires ou de contrôles).\n\n# Stratification par service\ncox7 <- coxph(Surv(temps, 1-censure) ~ \n                age + sexe + strata(service), \n              data = survie1)\n# Décompte par région\nwith(survie1, table(service)\n# Coefficients\nsummary(cox7)\n\n\n\n\n\nTableau 6.7:  Rapport de risques et intervalles de confiances à niveau 95% pour le modèle de Cox stratifié par service. \n \n  \n    terme \n    exp(coef) \n    borne inf. \n    borne sup. \n  \n \n\n  \n    age \n    0.96 \n    0.94 \n    0.97 \n  \n  \n    sexe \n    0.61 \n    0.44 \n    0.85 \n  \n\n\n\n\n\n\nOn voit à la lecture de la sortie dans le Tableau 6.7 qu’il n’y a plus de paramètres pour la variable service. Les paramètres des autres variables s’interprètent comme d’habitude. On peut néanmoins résumer l’information pour service en calculant une statistique descriptive, par exemple les différences de survie à des temps donnés.\nLa Figure 6.9 illustre l’effet de la stratification sur l’estimation du risque de base et des courbes de survie. On voit que la résolution des courbes est moindre, puisque chaque fonction est estimée à partir d’un sous-ensemble des données.\n\n\n\n\n\nFigure 6.9: Courbes de survie estimées par nombre de service pour le modèle de Cox avec stratification pour un homme de 40 ans.\n\n\n\n\n\n\n6.5.6 Modèle non-proportionnel\nPour simplifier l’exposition, supposons que nous avons une seule variable explicative \\(X\\). L’équation du modèle à risques proportionnels est \\(h(t; x) = h_0(t)\\exp(\\beta x)\\) et suppose que la fonction de risque de base \\(h_0(t)\\) est indépendante de la variable explicative \\(X\\). Une manière de modéliser la non-proportionnalité est d’inclure un terme d’interaction entre la variable et le temps. Il existe plusieurs façons de le faire. Par exemple, on pourrait inclure une nouvelle variable qui est le produit entre le temps et la variable \\(X\\). Le modèle est alors \\[\\begin{align*}\nh(t; x) = h_0(t) \\exp(\\beta_1x + \\beta_2xt).\n\\end{align*}\\] Pour ce modèle, le rapport de risque, pour une augmentation d’une unité de \\(X\\) est \\(\\exp(\\beta_1+ \\beta_2t)\\) et dépend du temps \\(t\\): c’est un modèle avec risques non proportionnels. On retombe sur le modèle à risques proportionnels lorsque \\(\\beta_2=0\\).\nSupposons que l’effet du nombre de service augmente avec le temps. On peut inclure à la fois service, qui capture l’effet au temps zéro, et le surenchérissement ou la diminution à mesure que le temps d’abonnement progresse.\n\n# Créer variables binaires par service\nsurvie1_modif <- survie1 |>\n  dplyr::mutate(service1 = service == 1,\n                service2 = service == 2,\n                service3 = service == 3)\ncox_np <- survival::coxph(\n    Surv(temps, censure) ~ \n     age + sexe + service + \n      tt(service1) + tt(service2) + tt(service3), \n     data = survie1_modif, \n     tt = function(x, t, ...){t * x})\n\nLe bloc code précédent illustre comment créer le terme non proportionnel: on spécifie avec l’option tt() la variable qui change dans le temps et on spécifie par la suite la nature de l’interaction temporelle en définissant une fonction tt.\nLes variables catégorielles n’étant pas supportées en l’état, elles doivent préalablement être transformées en variable indicatrices binaires. Une fois les nouvelles variables dans la base de données, on procède à la spécification du terme de risque non proportionnel.\n\n\n\n\nTableau 6.8:  Rapport de risque et intervalles de confiance à niveau 95% pour le modèle à risques non proportionnels (interaction linéaire entre temps et service). \n \n  \n    terme \n    exp(coef) \n    test de Wald \n    valeur-p \n  \n \n\n  \n    age \n    0.953 \n    -7.368 \n    <0.001 \n  \n  \n    sexe \n    0.543 \n    -5.241 \n    <0.001 \n  \n  \n    service1 \n    0.144 \n    -4.293 \n    <0.001 \n  \n  \n    service2 \n    0.072 \n    -3.762 \n    <0.001 \n  \n  \n    service3 \n    0.010 \n    -4.139 \n    <0.001 \n  \n  \n    tt(service1) \n    1.010 \n    2.173 \n    0.0298 \n  \n  \n    tt(service2) \n    1.010 \n    1.584 \n    0.1132 \n  \n  \n    tt(service3) \n    1.023 \n    2.476 \n    0.0133 \n  \n\n\n\n\n\n\nLe Tableau 6.8 contient les résultats. Les coefficients pour l’interaction avec \\(t\\) sont petits, mais c’est parce que la variable temps n’est pas standardisée et qu’elle s’étend de 0 à 200 semaines: de petites variations sont possiblement importantes quand le temps augmente. Les estimations des coefficients pour l’interaction sont tous positifs, ce qui suppose que le risque augmente avec le temps. Si on considère comme source plausible d’effet du nombre de services un quelconque rabais, il semble que cet effet protecteur s’amenuise. Deux des termes d’interaction sont significatifs à niveau 5% (statistiques de Wald \\(Z\\) de 2.173, 1.584 et 2.476 et valeurs-\\(p\\) correspondantes de 0.03, 0.113 et 0.013).\nOn peut aussi utiliser la structure de modèle à risques non-proportionnels pour capturer l’effet des changements qui interviennent au sein de variables explicatives dans le temps. Par exemple, l’âge de la personne (en années) augmente à mesure que le temps d’abonnement (en semaines) passe, d’où \\(\\texttt{age}(t) = \\texttt{age} + t/52\\).\n\n# interaction entre service et temps\n# objet avec 'tt' varie dynamiquement\ncox6 <- coxph(\n    Surv(temps, censure) ~ \n     tt(age) + sexe + service, \n     data = survie1, \n     tt = function(x, t, ...){x + t/52})\nsummary(cox6)\n\nIci, on inclut uniquement la variable transformée tt(age) et son effet est toujours fortement significatif pour expliquer le désabonnement potentiel: les personnes plus âgées sont moins susceptibles de résilier leur abonnement.\n\ncox_np <- survival::coxph(\n    Surv(temps, censure) ~ \n     tt(age) + sexe + service, \n     data = survie1, \n     tt = function(x, t, ...){x + t/52})\nsummary(cox_np)\n\nOn spécifie avec l’option tt() dans la formule la variable qui change dans le temps et par la suite la nature de l’interaction temporelle avec l’argument tt.\n\n\n\n\nTableau 6.9:  Rapport de risque et intervalles de confiance à niveau 95% pour le modèle à risques non proportionnels (interaction linéaire entre temps et âge). \n \n  \n    terme \n    exp(coef) \n    borne inf. \n    borne sup. \n  \n \n\n  \n    age \n    0.91 \n    0.87 \n    0.95 \n  \n  \n    tt(age) \n    1.00 \n    1.00 \n    1.00 \n  \n  \n    sexe \n    0.52 \n    0.41 \n    0.65 \n  \n  \n    region2 \n    0.70 \n    0.48 \n    1.02 \n  \n  \n    region3 \n    1.03 \n    0.73 \n    1.46 \n  \n  \n    region4 \n    0.80 \n    0.56 \n    1.12 \n  \n  \n    region5 \n    0.97 \n    0.69 \n    1.37 \n  \n  \n    service1 \n    0.36 \n    0.28 \n    0.46 \n  \n  \n    service2 \n    0.18 \n    0.12 \n    0.26 \n  \n  \n    service3 \n    0.12 \n    0.07 \n    0.20 \n  \n\n\n\n\n\n\nL’interaction employée ici n’est pas la seule fonction du temps qu’on pourrait spécifier: on pourrait par exemple inclure une fonction de type escalier \\(\\mathrm{I}(T< t_0)\\) qui indique que l’effet de la variable disparaît après le temps \\(t_0\\), si par exemple un rabais disparaît après une certaine durée d’abonnement, avec rabais * I(t<t0) pour une valeur numérique t0 fixée."
  },
  {
    "objectID": "06-survie.html#modèle-à-risques-compétitifs",
    "href": "06-survie.html#modèle-à-risques-compétitifs",
    "title": "6  Analyse de survie",
    "section": "6.6 Modèle à risques compétitifs",
    "text": "6.6 Modèle à risques compétitifs\nParfois, la raison pour laquelle un individu quitte l’état étudié peut avoir un intérêt en soi. Par exemple si on s’intéresse au temps qu’un employé demeure au service de la compagnie, la distinction entre le fait qu’il ait démissionné ou bien qu’il ait été renvoyé peut avoir un impact sur l’effet des variables explicatives. Comme autre exemple, si on s’intéresse au temps de survie d’un individu après qu’il ait été diagnostiqué avec un certain type de cancer, il pourrait être important de distinguer selon la cause exacte de la mort.\nDe manière générale, supposons qu’il y a \\(K\\) manières possibles que l’événement survienne.\n\n\n\n\n\nFigure 6.10: Schéma illustrant la transition entre état de base et autres événements compétitifs.\n\n\n\n\nDans notre exemple d’abonnement cellulaire, supposons que nous avons trois causes possibles pour la perte d’un client: soit il a interrompu son abonnement pour aller chez le compétiteur A, soit pour aller chez le compétiteur B, soit il n’a plus de cellulaire du tout. On considère un modèle avec transition d’un état de base (abonné) vers un état absorbant (désabonnement, soit chez compétiteur \\(A\\), compétiteur \\(B\\) ou abandon du cellulaire).\nOn a deux avenues pour l’estimation de ce type de modèle: soit on ajuste un modèle de Kaplan–Meier, soit un modèle de Cox.\nOn peut alors spécifier \\(K\\) fonctions de risques (une pour chaque manière) et obtenir le modèle de Cox à risques compétitifs (competing risks), \\[\\begin{align*}\nh_1(t; \\boldsymbol{x})&= h_{01}(t) \\exp(\\beta_{11}x_1 + \\cdots + \\beta_{p1} x_p)\\\\\n&\\vdots\\\\\nh_K(t; \\boldsymbol{x})&= h_{0K}(t) \\exp(\\beta_{1K}x_1 + \\cdots + \\beta_{pK} x_p)\\\\\n\\end{align*}\\] Notez que les coefficients sont différents d’une équation à l’autre. En estimant ce modèle, on obtient donc une estimation de l’effet des variables selon la raison du départ de l’état. De plus, on peut aussi inclure des variables dont la valeur change dans le temps, comme vu précédemment.\nCe qui simplifie énormément la situation est qu’il est prouvé qu’on peut estimer les paramètres de chaque équation séparément sans perte de précision. Par conséquent, en pratique, il suffira d’ajuster \\(K\\) modèles de Cox séparément.\nLes données pour cet exemple se trouvent dans le fichier survie4. La seule nouveauté par rapport au fichier original est la variable \\(\\texttt{censure}\\) qui est maintenant codée ainsi\n\n1, si le temps est censuré (l’individu est toujours abonné à notre service)\n2, si l’individu a quitté pour aller chez le compétiteur A\n3, si l’individu a quitté pour aller chez le compétiteur B\n4, si l’individu a quitté parce qu’il n’a plus besoin de cellulaire.\n\nOn peut calculer la fréquence de chaque modalité avec table. Ainsi, il y a donc 166 clients toujours abonnés, 170 qui nous ont quitté pour aller chez A, 121 pour aller chez B, et 43 qui n’ont plus de cellulaires.\nPour ajuster le modèle lorsque la cause du départ est le compétiteur A, le code définit censure == 2.\n\n# Rappel pour `event`:\n#   1 pour observation, \n#   0 pour censure à droite\n# On utilise la convention TRUE = 1, FALSE = 0\ndata(survie4, package = \"hecmulti\")\ncox5 <- coxph(Surv(time = temps, \n                   event = censure == 2,\n                   type = \"right\") ~ \n                age + sexe + region + service, \n              data = survie4,\n              ties = \"exact\")\nsummary(cox5)\n\n\n\n\n\nTableau 6.10:  Rapport de risque et intervalles de confiance à niveau 95% pour le modèle à risques compétitifs (probabilité de quitter pour compétiteur A). \n \n  \n    terme \n    exp(coef) \n    borne inf. \n    borne sup. \n  \n \n\n  \n    age \n    0.95 \n    0.94 \n    0.97 \n  \n  \n    sexe \n    0.44 \n    0.32 \n    0.62 \n  \n  \n    region2 \n    0.54 \n    0.32 \n    0.92 \n  \n  \n    region3 \n    0.82 \n    0.50 \n    1.35 \n  \n  \n    region4 \n    0.72 \n    0.45 \n    1.16 \n  \n  \n    region5 \n    1.05 \n    0.66 \n    1.66 \n  \n  \n    service1 \n    0.38 \n    0.27 \n    0.54 \n  \n  \n    service2 \n    0.18 \n    0.11 \n    0.30 \n  \n  \n    service3 \n    0.11 \n    0.05 \n    0.22 \n  \n\n\n\n\n\n\nNotez qu’on précise que les valeurs 1, 3 et 4 sont des observations censurées. Ici, l’événement d’intérêt est que le client est parti chez le compétiteur A. S’il est toujours abonné (\\(\\texttt{censure=1}\\)), s’il est parti chez le compétiteur B (\\(\\texttt{censure=3}\\)) ou s’il nous a quitté car il n’a plus de cellulaire (\\(\\texttt{censure=4}\\)), alors l’événement « quitter pour aller chez A » n’est pas survenu. C’est pourquoi on doit traiter ces situations comme des censures.\nAinsi, on voit que l’événement est survenu 170 fois et qu’il y a 330 censures. L’interprétation des paramètres se fait comme précédemment. Sauf qu’il faut préciser qu’il s’agit du risque de quitter pour aller chez le compétiteur A. Par exemple, le risque de quitter pour aller chez le compétiteur A d’une femme est 0.444 fois le risque de quitter pour aller chez le compétiteur A d’un homme. Ainsi, les femmes sont moins à risque de quitter pour aller chez le compétiteur A que les hommes.\nDans R, on peut aussi ajuster simultanément tous les modèles en spécifiant que l’événement d’intérêt est un facteur: c’est alors la catégorie de référence qui fait foi de l’état de départ. Il faut également une variable qui identifie l’observation — dans le cas qu’on considère, c’est simplement une colonne avec des valeurs de \\(1\\) à \\(n\\).\n\nrc_cox <- coxph(\n  Surv(time = temps, \n       event = factor(censure)) ~ sexe + age + service,\n  data = survie4 |>\n    dplyr::mutate(id = 1:nrow(survie4)),\n  id = id)\n\nOn obtient l’ensemble des coefficients dans le tableau résumé, un pour chaque événement compétitif autre que la référence.\nSi on voulait ajuster le modèle de Kaplan–Meier, pour une transition d’un état de base (abonné) vers un état dit absorbant (désabonnement, soit chez compétiteur A ou B ou abandon du cellulaire), il faut obligatoirement ajuster toutes les courbes simultanément. On ajustera le modèle multi-état en spécifiant un facteur pour l’événement, où encore une fois la catégorie de référence est abonnement (censure=1).\n\nrc_km <- survfit(Surv(time = temps, \n             event = factor(censure)) ~ 1,\n             data = survie4)\n\nLes représentations graphiques pour le modèle à risque compétitif sont légèrement différente. Si on dichotomise l’événement d’intérêt (survie ou échec), il y a deux options possibles et la probabilité d’échec est complémentaire à la survie. Avec plus d’un choix, on obtiendra un graphique avec une estimation de la probabilité pour chaque modalité de l’événement: la Figure 6.11 montre ceci pour la sortie du modèle de Cox et le modèle de Kaplan–Meier en ajoutant la courbe correspondant à la probabilité de demeurer abonné.\n\n\n\n\n\nFigure 6.11: Probabilité d’événement sans variable explicative (Kaplan-Meier, gauche) et avec âge, service et sexe (modèle de Cox, droite).\n\n\n\n\nLa vignette du paquet survival offre des détails sur l’implémentation des méthodes qui traitent de l’inclusion de variables explicatives dont la valeur change dans le temps et sur les risques compétitifs.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nLe modèle de Cox suppose qu’on peut diviser le risque en deux parties: le risque de base \\(h_0(t)\\) commun à tou(te)s (composante nonparamétrique) et l’effet multiplicatif \\(\\exp(\\mathbf{X}\\boldsymbol{\\beta})\\) (composante paramétrique)\nPuisque la fonction de risque de base est commune à toutes les observations, moins d’incertitude sur l’estimation de la survie.\nL’impact sur la survie de changement dans les variables explicatives n’est pas multiplicatif.\nLe modèle de Cox suppose que le rapport de cote ne dépend pas du temps (postulat de risques proportionnels).\nOn peut vérifier ce postulat et généraliser le modèle au besoin.\nPlutôt que d’inclure une variable catégorielle, on peut utiliser la stratification pour estimer le risque de base séparément sur chaque sous-groupe.\nle modèle à risques non-proportionnels permet d’inclure une interaction entre le temps et une avec variable ou un coefficient.\nSi les variables explicatives changent au fil du temps, on peut décomposer la contribution de l’observation en plusieurs segments.\nIl y a un lien possible avec le modèle à risque proportionnels si l’effet est le même pour tous (comme l’âge).\nLe modèle multi-état (modèle à risques compétitifs) permet d’estimer la probabilité de chaque transition: la survie pour l’événement de base reste le même (désabonnement), mais on décompose la probabilité de la censure selon les différents événements compétitifs.\n\n\n\n\n\n\n\nGrambsch, Patricia M., and Terry M. Therneau. 1994. “Proportional hazards tests and diagnostics based on weighted residuals.” Biometrika 81 (3): 515–26. https://doi.org/10.1093/biomet/81.3.515.\n\n\nStensrud, Mats J., and Miguel A. Hernán. 2020. “Why Test for Proportional Hazards?” JAMA 323 (14): 1401–2. https://doi.org/10.1001/jama.2020.1267."
  },
  {
    "objectID": "03-regroupements.html#introduction",
    "href": "03-regroupements.html#introduction",
    "title": "7  Analyse de regroupements",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nSi la publicité ciblée personnalisée a pris de l’essort ces derniers années en commercialisation, la segmentation de consommateurs reste une partie prenante essentielle de toute campagne de publicité ou de développement de produits.\nL’analyse de regroupement est une technique d’analyse descriptive qui sert à combiner des sujets en groupes de telle sorte que les individus d’un même groupe soient le plus semblables possible et que les groupes soient le plus différent possible les uns des autres, avec des valeurs aberrantes clairement identifiées. Cette similarité est définie selon des caractéristiques provenant de variables explicatives. Le résultat de l’analyse de regroupement sera une étiquette associée à chaque observation l’assignant à un regroupement ou l’identifiant comme aberrance, nous permettant ainsi de caractériser par le biais de statistiques descriptives les différents segments obtenus.\nIl y a une certaine analogie avec l’analyse factorielle. En analyse factorielle, on cherche à déterminer s’il y a des groupes de variables corrélées entre elles et à les regrouper pour réduire le nombre de variables. En analyse de regroupements, on cherche plutôt à créer des groupes d’observations similaires. Les deux méthodes servent pour l’analyse exploratoire ou descriptive.\nPour créer les regroupements, on utilisera \\(p\\) variables explicatives \\(X_1, \\ldots, X_p\\) pour chacune des \\(n\\) observations, où \\(X_{ij}\\) dénotera la valeur de la \\(j\\)e variable explicative pour le \\(i\\)e sujet.\n\n\n\n\n\n\nÉtapes d’une analyse de regroupements\n\n\n\n\nChoisir les variables pertinentes à l’analyse. Cette étape peut nécessiter de créer, transformer de nouvelles variables ou d’aggréger les données.\nDécider quel méthode sera utilisée pour la segmentation.\nChoisir les hyperparamètres de l’algorithme (nombre de regroupements, rayon, etc.) et la mesure de dissemblance.\nValider la qualité de la segmentation (interprétabilité, taille des groupes, homogénéité des regroupements).\nAvec les étiquettes, calculer un prototype de groupe.\nInterpréter les regroupements obtenus à partir des prototypes"
  },
  {
    "objectID": "03-regroupements.html#données",
    "href": "03-regroupements.html#données",
    "title": "7  Analyse de regroupements",
    "section": "7.2 Données",
    "text": "7.2 Données\nVoici en vrac quelques exemples de bases de données sur lesquelles on pourrait effectuer une analyse de regroupements.\nLes programmes de fidélisation font partie de la stratégie de commercialisation de plusieurs grandes chaînes (pharmacies, épiceries): en échange de rabais et d’offres promotionnelles, la clientèle fournit des informations sociodémographique (nom, adresse, date de naissance, etc.) et utilise un identifiant numérique, une carte ou une application pour inscrire chaque achat: ce faisant, le système peut traquer les habitudes de consommation.1 Créer des regroupements permet de mieux cerner les besoins et habitudes de segments de consommateurs et ainsi d’adapter l’offre promotionnelle. Les algorithmes utilisés pour l’analyse de regroupements peuvent également servir à la résolution d’entité, qui consiste à fusionner les profils de bases de données sans identifiant unique client.\nUn autre exemple d’application de l’analyse de regroupements est la segmentation de la clientèle de transport en commun. Dans la région métropolitaine de Montréal, l’Agence régionale de transport métropolitain recueille des informations sur les passages et transactions par le biais des cartes à puce Opus (achat de passes mensuelles ou de billets unitaires, lieu de l’achat, etc.) ainsi que les passages (heure, type de véhicule, emplacement approximatif pour les services d’autobus ou station de métro). En créant des regroupements, une agence de transport peut ainsi ajuster son offre et proposer des abonnements ou des produits qui reflètent les besoins de sa clientèle. Un exemple extrême de traquage de compagnie de transport est Nederlandse Spoorwegen (NS): toute personne qui veut voyager en train sur les chemins de fers néerlandais doit acheter une carte à puce et la charger, en plus de composter son billet au départ et à l’arrivée de son voyage. Cette approche, qui peut sembler intrusive, permet néanmoins de mesurer précisément la demande sur les lignes en fonction du moment de la journée et de l’associer à chaque client.\nSouvent, les bases de données marketing sont souvent de nature longitudinale: chaque ligne correspond à une transaction, mais plusieurs d’entre elles peuvent être le fait d’une même personne/compte. Une fois l’analyse exploratoire des données complétée, on procédera à l’aggrégation des observations par compte client, puisque la segmentation doit être effectuée à cette échelle. C’est également à ce stade qu’on pourra créer de nouvelles variables explicatives à partir de l’information présente dans la base de données: par exemple, on pourrait considérer la fréquence moyenne d’achat, le montant moyen par transaction, le mode du moment de la journée, la variabilité de cette fréquentation, le pourcentage des ventes provenant d’articles en solde, la variabilité du montant du panier, etc. Cette liste, non exhaustive, illustre l’étape cruciale de l’extraction de l’information utilisée dans l’analyse statistique: il faut être conscients que la qualité de la segmentation dépend du choix de variables employées.\nIl y a une pléthore d’exemples d’analyse de regroupements. Par exemple, les articles suivants de science politique utilisent les résultats d’élections passées ou de sondages pour établir typologie des électeurs français suite à la présidentielle, une segmentation de quartiers de Los Angeles et de New York selon leur vote ou le profils des électeurs albertains. Ce travail de maîtrise se penche de son côté sur le positionnement de joueurs lors de match de la NBA."
  },
  {
    "objectID": "03-regroupements.html#choix-des-variables",
    "href": "03-regroupements.html#choix-des-variables",
    "title": "7  Analyse de regroupements",
    "section": "7.3 Choix des variables",
    "text": "7.3 Choix des variables\nL’analyste est libre de choisir quelles variables seront incluses dans le modèle. Le choix des variables est important: en général on veut créer des groupes d’individus qui sont homogènes par rapport à certains aspects de leur comportement ou de leur situation. On ne doit alors inclure que les variables pertinentes à cet aspect. Inclure de nombreuses variables pour lesquelles il y a une forte similitude entre individus contribue à diluer les différences.\nPar exemple, si le but de l’analyse est de segmenter nos clients selon leurs habitudes de consommation (genre de boutiques fréquenté, fréquence, etc.), on n’inclura pas des variables démographiques qui feraient ressortir les différences de genre, d’âge, de revenu, etc. En fait, souvent l’analyse de regroupements servira justement à créer des groupes qui seront comparés par rapport à d’autres variables qui n’ont pas été utilisées pour créer les groupes.\nLa compréhension de la base de données est cruciale pour comprendre le comportement. Si on essaie de faire une segmentation du comportement d’utilisateurs et utilisatrices de transports en commun à partir d’informations auxiliaires comme le temps de passage, le nombre de correspondance et la fréquence d’utilisation, il peut être utile de créer de nouvelles variables (par exemple, une variable indicatrice qui indique si la personne voyage durant les heures de traffic entre 7h30 et 9h et 16h à 18h, le nombre hebdomadaire moyen de jours ouvrables pendant lesquels elle se déplace, etc). L’inclusion des ces variables auxiliaires peut augmenter la qualité de la segmentation.\nPour voir si certaines variables sont inutiles, il peut être utile de comparer les représentants des groupes (par exemple, le barycentre ou une observation lambda du groupe) pour voir si les moyennes ou caractéristiques diffèrent. Si ce n’est pas le cas, on pourrait envisager de recommencer la procédure en enlevant cette variable.\nSi on a un nombre important de variables explicatives à disposition, il est parfois utile de réduire préalablement la dimension (par exemple, en effectuant une analyse en composantes principales) et à ne retenir que les premières composantes pour faciliter la tâche. Cette approche n’est pas la panacée: quelquefois, cette réduction de la dimension masque les différences entre groupes et mène à une segmentation inférieure à l’utilisation des variables originales.\nMalheureusement, il n’est pas évident de prime abord de déterminer quelles variables inclure dans la base de données pas plus qu’il n’est facile de juger de la qualité d’une segmentation ou du nombre de regroupements à effectuer. Les choix individuels auront un impact certain sur les regroupements obtenus: on recommande d’essayer plusieurs alternatives et de vérifier graphiquement ou à l’aide de critères d’ajustement si les regroupements obtenus sont homogènes et compacts.\nSi certaines variables définissent naturellement des groupes, par exemple l’âge des personnes, et fait qu’ils et elles ont des caractéristiques intrinsèquement différentes, il peut être utile de faire une segmentation indépendamment pour chacun de ces sous-groupes.\nDans ce chapitre, nous utiliserons des données simulées inspirées de campagnes de financement d’organismes de charité. Ces dernières font souvent du démarchage publicitaire auprès de donateurs ou envoient par publipostage des demandes de dons à toutes les adresses postales. Ces efforts ont un coût important: nous essaierons de créer des catégories de donateurs afin de mieux cibler les donateurs et donatrices et le moment adéquat pour ce démarchage. Plusieurs grandes compagnies sont associées à ces organismes et les parrainent: notre base de données contiendra le profil de toutes ces personnes, qu’elles fassent un don ou pas.\nLa base de données dons contient 19353 observations pour 16 variables: le Tableau 7.1 fournit les statistiques descriptives. Elle a été crée en regroupement les identifiants: de nombreuses variables explicatives sont dérivées des données brutes, notamment le temps entre dons, les statistiques descriptives (montant moyen, minimum maximum) pour les dons monétaires. Ces choix de variables sont loins d’être anodins et peuvent influencer la segmentation décrite dans ce chapitre. Une rapide exploration des données révèle que près de 0% des employé(e)s n’ont pas donné à l’organisme. Une poignée de dons sont très élevés, mais la plupart des montants tourne autour de 5$, 10$, 20$, etc.\n\n\n\n\nTableau 7.1:  Statistiques descriptives des variables du jeu de données dons. \n \n  \n    variable \n    moyenne \n    écart-type \n    min \n    max \n    manquant \n  \n \n\n  \n    ndons \n    5.13 \n    5.21 \n    1 \n    27.0 \n    0 \n  \n  \n    recence \n    86.41 \n    81.80 \n    2 \n    299.0 \n    0 \n  \n  \n    anciennete \n    168.72 \n    95.77 \n    2 \n    302.0 \n    0 \n  \n  \n    vdons \n    117.66 \n    431.08 \n    1 \n    19260.0 \n    0 \n  \n  \n    vdonsmax \n    30.72 \n    88.13 \n    1 \n    2460.0 \n    0 \n  \n  \n    vdonsmin \n    10.98 \n    28.79 \n    1 \n    1570.0 \n    0 \n  \n  \n    npromesse \n    1.72 \n    2.37 \n    0 \n    15.0 \n    0 \n  \n  \n    vpromesse \n    61.32 \n    282.07 \n    0 \n    12680.0 \n    0 \n  \n  \n    nradiations \n    0.52 \n    0.89 \n    0 \n    10.0 \n    0 \n  \n  \n    vradiations \n    28.09 \n    141.47 \n    0 \n    11815.0 \n    7142 \n  \n  \n    ddons \n    2.13 \n    1.94 \n    0 \n    23.9 \n    5736 \n  \n  \n    ddonsmax \n    3.85 \n    3.24 \n    0 \n    23.9 \n    5736 \n  \n  \n    ddonsmin \n    1.32 \n    1.83 \n    0 \n    23.9 \n    5736 \n  \n  \n    nrefus \n    2.18 \n    2.26 \n    0 \n    11.0 \n    0 \n  \n  \n    nrefusconsec \n    1.56 \n    2.06 \n    0 \n    11.0 \n    0 \n  \n  \n    nindecis \n    0.47 \n    0.93 \n    0 \n    8.0 \n    0 \n  \n\n\n\n\n\n\nLa grande proportion de données manquantes pose un problème immédiat pour la segmentation, puisque la plupart des procédures ne permettent pas de traiter ces dernières et éliminent d’office les observations correspondantes de la base des données. Ici, plusieurs valeurs manquantes (NA) peuvent être logiquement remplacées par des valeurs numériques: par exemple, la valeur cumulative des dons (vdons) d’une personne qui n’a jamais donné est nulle.2 En revanche, le temps d’attente entre deux dons pour une personne qui a fait un don ou moins n’est pas bien défini.\nSi on essaie de créer manuellement des groupes, il apparaît logique de séparer en trois segments initiaux la base de données: les personnes qui n’ont jamais donné à l’organisme de charité mais dont les caractéristiques sont connues, les personnes qui ont fait un seul don et celles qui ont fait des dons multiples. Un algorithme ferait de toute façon vraisemblablement ressortir cette information, mais nous empêcherait d’exploiter pleinement l’ensemble des variables explicatives et de ses dérivées. On pourra effectuer la segmentation séparément sur chaque groupe avec en intrant des variables explicatives différentes.\nLes intrants de l’analyse de regroupement (soit le choix des variables) est laissé à la discrétion de l’analyste. Dans notre exemple, on pourrait aisément créer de nouvelles variables pour faire ressortir des informations jugées pertinentes. Est-ce qu’on s’intéresse au montant moyen des dons, soit vdons/ndons? Est-ce que la valeur des radiations nous intéresse, ou bien devrait-on plutôt considérer le pourcentage de la valeur promise réalisée?\nOn considère ci-dessous l’ensemble des personnes qui ont fait plusieurs dons. On modifie certaines variables explicatives pour réduire la corrélation entre variables et obtenir des variables plus évocatrices: le montant moyen de dons, le nombre de refus relatif à l’ancienneté du donateur ou de la donatrice et finalement la valeur de la promesse moyenne, si applicable (zéro sinon). Plusieurs variables (délais minimum et maximum entre dons, valeurs minimum, radiations, etc.) sont également abandonnées pour simplifier l’exposition et pour éviter qu’elles ne ressortent indûment. On voit également que plusieurs valeurs de radiations sont manquantes: cette variables est éliminée d’office.\n\ndonsmult <- dons |>\n  filter(ndons > 1L) |>\n  mutate(mtdons = vdons/ndons,\n         snrefus = nrefus/anciennete*mean(anciennete),\n         mpromesse = case_when(\n           npromesse > 0 ~ vpromesse/npromesse,\n           TRUE ~ 0)) |>\n  select(!c(\n    vradiations, # valeurs manquantes\n    nindecis, vdons, ddonsmax,\n    ddonsmin, vdonsmin, npromesse,\n    vpromesse, nrefus, nradiations)) |>\n  relocate(mtdons)\n\nLe champ des applications de l’analyse de regroupements est parfois surprenant. Par exemple, cet article de FiveThirtyEight propose une segmentation des électeurs démocrates new-yorkais ou des quartiers de Los Angeles. Un autre exemple incongru est la compression d’images: la Figure 7.1 montre une image du bâtiment Decelles (coin supérieur gauche) et la reconstruction avec trois, quatre et 10 couleurs obtenues en appliquant l’algorithme des \\(K\\)-moyennes sur la matrice formée par les valeurs des canaux (rouge, vert, bleu) de l’image.\n\n\n\n\n\nFigure 7.1: Compression d’image avec l’algorithme des \\(K\\)-moyennes: image originale (en haut à gauche), compression avec trois (en haut à droite), quatre (en bas à gauche) et 10 (en bas à droite) couleurs."
  },
  {
    "objectID": "03-regroupements.html#mesures-de-dissemblance",
    "href": "03-regroupements.html#mesures-de-dissemblance",
    "title": "7  Analyse de regroupements",
    "section": "7.4 Mesures de dissemblance",
    "text": "7.4 Mesures de dissemblance\nComment mesurer si deux observations appartiennent à un même regroupement et sont similaires? Idéalement, on aimerait avoir une situation comme dans la Figure 7.2 où les regroupements sont clairement visibles. On aimerait que la similarité entre observations d’un même groupe, ou intra-groupe, soit élevée et que la similarité entre groupe soit faible. Les regroupements devraient être éloignés les uns des autres, tandis que les observations au sein de ces regroupements devraient être proches. Dans la plupart des cas, il y aura des observations isolées qui n’appartiennent pas nécessairement logiquement à l’un ou l’autre des groupes: on appelle parfois ces observations aberrances.\n\n\n\n\n\nFigure 7.2: Données simulées avec deux regroupements hypothétiques.\n\n\n\n\n\n7.4.1 Mesures de dissemblance\nLes algorithmes de segmentation comparent les observations entre elles: souvent, la matrice de données est réduite à une mesure de distance entre observations (soit les lignes de la base de données). Une mesure de dissemblance sert à quantifier la proximité de deux objets à partir de leurs coordoonnées. Elle mesure la distance entre deux vecteurs lignes d’observations \\(\\mathbf{X}_i\\) et \\(\\mathbf{X}_j\\) en se basant sur les \\(p\\) variables explicatives. Plus la dissemblance est petite, plus les sujets \\(\\mathbf{X}_i\\) et \\(\\mathbf{X}_j\\) sont similaires. La plupart des mesures de dissemblances \\(d\\) ont les propriétés mathématiques suivantes:\n\n\\(d(\\mathbf{X}_i, \\mathbf{X}_j) \\geq 0\\) (positivité), avec égalité (distance nulle) si et seulement si \\(\\mathbf{X}_i=\\mathbf{X}_j\\) (mêmes caractéristiques pour toutes les variables explicatives);\n\\(d(\\mathbf{X}_i, \\mathbf{X}_j)=d(\\mathbf{X}_j, \\mathbf{X}_i)\\) (symmétrie);\n\nToute mesure de distance3 est une mesure de dissemblance. La mesure de dissemblance la plus utilisée en pratique est la distance euclidienne entre sujets, soit \\[\\begin{align*}\nd(\\mathbf{X}_i, \\mathbf{X}_j; l_2) = \\left\\{(X_{i1}-X_{j1})^2 + \\cdots + (X_{ip}-X_{jp})^2\\right\\}^{1/2}.\n\\end{align*}\\] C’est tout simplement la longueurdu segment qui relie deux points dans l’espace \\(p\\) dimensionnel.\nPlus généralement, la distance de Minkowski ou distance \\(l_q\\) entre les vecteurs ligne \\(\\mathbf{X}_i\\) et \\(\\mathbf{X}_j\\) est \\[\\begin{align*}\nd(\\mathbf{X}_i, \\mathbf{X}_j; l_q) = \\left( \\sum_{k=1}^p |X_{ik}-X_{jk}|^q \\right)^{1/q},\\qquad q > 0;\n\\end{align*}\\] la distance Euclidienne correspondant à \\(q=2\\), et la distance de Manhattan à \\(q=1\\).4 Finalement, si \\(q=\\infty\\), la distance se réduit à \\(\\max_{k=1}^p |X_{ik}-X_{jk}|\\), soit le maximum des différences entre coordonnées des vecteurs d’observations.\nIl existe un très grand nombre d’autres mesures de dissemblance pour variables quantitatives, ordinales, nominales et binaires. Si les variables sont toutes binaires, la mesure d’appariement simple (simple matching), qui mesure la proportion des variables pour lesquelles les deux sujets ont des valeurs différentes, est une mesure de dissemblance adéquate.\nDans le cas de jeux de données avec des variables mixes, une option populaire est la distance de Gower (Gower 1971). Cette dernière compare deux individus selon leurs caractéristiques et est construite à partir de similarité, avec \\(\\mathbf{D} = (\\mathbf{I}_n-\\mathbf{S})^{1/2}\\) comme matrice de dissimilarité des \\(n\\) observations. La similarité entre deux individus est définie comme \\[\\begin{align*}\nS_{ij} = \\frac{\\sum_{k=1}^p s_{ijk} \\delta_{ijk}}{\\sum_{k=1}^p \\delta_{ijk}}\n\\end{align*}\\] où \\(\\delta_{ijk}\\) est un poids qui vaut zéro si la variable \\(\\mathrm{X}_k\\) est manquante pour l’un ou l’autre des individus.\nOn distingue trois type de variables dans la distance de Gowers:\n\nles variables binaires asymmétrique de type absence/présence donnent une valeur de \\(\\delta=1, s=1\\) si les deux sont présentes \\(X_{ik}=X_{jk}=1\\), \\(\\delta_{ijk}=1\\) et \\(s_{ijk}=0\\) si \\(X_{ik} \\neq X_{jk}\\) et \\(\\delta_{ijk}=0\\) si \\(X_{ik}=X_{jk}=0\\).\n\\(s_{ijk}=1\\) les variables qualitatives ont la même modalité et \\(s_{ijk}=0\\) sinon\n\\(s_{ijk} = 1-|X_{ik}-X_{jk}|/R_k\\) pour une variable continue, où \\(R_k\\) est l’étendue de la variable \\(R_k=\\max_{i} X_{ik} - \\min_i X_{ik}\\) dans l’échantillon.\n\nLa dissemblance résultante pour les types mixtes vaut zéro quand toutes les variables sont similaires/égales et un si elles sont complètement différentes/maximalement distantes.\nOn peut traiter les variables ordinales soit comme des variables continues, soit comme des variables nominales avec la mesure d’appariement simple; ce faisant, on n’utilise pas l’ordre entre les modalités.\n\n\n7.4.2 Dissemblance et valeurs manquantes\nDans plusieurs cas, on se trouvera en présence de valeurs manquantes dans le jeu de données. Cela peut arriver pour plusieurs raisons valables (aucune candidature ne représente un partir dans une circonscription donnée pour un parti lors d’une élection, l’information est manquante, une femme ne peut avoir de cancer de la prostate, etc.) Il faut bien penser à vérifier si l’algorithme de votre choix peut gérer ces valeurs manquantes. Sinon, ces dernières devront être imputées préalablement à l’analyse de regroupements ou vous devrez faire sans les variables explicatives correspondantes.\nLes définitions des distances révèlent que chaque variable explicative a le même poids. En revanche, plus une variable a une grande variance, plus elle aura de l’influence sur le calcul de la distance, ce qui peut être bon ou mauvais selon la structure des groupes. Règle générale, il est préférable d’éviter qu’une variable domine dans la segmentation. La standardisation des variables et les transformations préalables effectuées sur les variables (log, arcsin, etc.) impacteront le résultat.\nOn peut standardiser au préalable les variables avant de faire l’analyse. Par défaut, les variables continues seront centrées et réduites, ou standardisées, afin d’avoir une moyenne de zéro et une variance de un (scale). On peut ensuite faire les analyses comme précédemment. Si on a des valeurs aberrantes, cela peut impacter le calcul des moyennes et variances; d’autres estimateurs de localisation et d’échelles plus robustes, par exemple la médiane et la déviation absolue par rapport à la médiane (mad) peuvent alors être plus adéquats pour diminuer l’impact des valeurs aberrantes même si le coût de calcul associé est plus conséquent. Notez qu’il est illogique de standardiser les variables binaires et catégorielles.\n\n# Standardisation usuelle \n# (soustraire moyenne, diviser par écart-type)\ndonsmult_std <- scale(donsmult)\n# Standardisation robuste\ndonsmult_std_rob <- apply(\n  donsmult, \n  MARGIN = 2, \n  FUN = function(x){(x - median(x))/mad(x)})\n# apply permet d'appliquer une fonction\n# par ligne, colonne ou cellule\n# MARGIN = 2 indique colonne \n# (on centre chaque colonne tour à tour)\n# Déviation absolue par rapport à la médiane\n# mad = moyenne de |obs - mediane|"
  },
  {
    "objectID": "03-regroupements.html#algorithmes-pour-la-segmentation",
    "href": "03-regroupements.html#algorithmes-pour-la-segmentation",
    "title": "7  Analyse de regroupements",
    "section": "7.5 Algorithmes pour la segmentation",
    "text": "7.5 Algorithmes pour la segmentation\nL’analyse de regroupements est une branche de l’apprentissage non-supervisé: contrairement à la classification, il n’existe pas de vraies étiquettes sur lesquelles se baser pour déterminer la qualité d’une segmentation. Des critères graphiques et des mesures d’homogénéité peuvent néanmoins déterminer à quel points les segments créés sont distincts les uns des autres.\nL’analyse de regroupements cherche à créer une division de \\(n\\) observations de \\(p\\) variables en \\(k\\) regroupements. Il existe un grand nombre d’algorithmes qui permettent de partitionner les données en regroupements à partir d’un jeu de données ou d’une matrice de dissemblance. Les sections suivantes survoleront différents algorithmes en s’attardant à l’heuristique de l’implémentation, aux différentes étapes de la procédure, aux hyperparamètres qui influencent le résultat (par ex., le nombre de groupes, la distance minimale entre regroupements, la forme des regroupements, les éléments représentatifs) qui détermine la sortie ainsi que les forces et faiblesses des algorithmes. À l’ère des mégadonnées, la complexité d’un algorithme de regroupements, une mesure du nombre d’opérations nécessaires pour effectuer le calcul, impactera le choix possible: l’algorithme de regroupements hiérarchiques (agglomératif ou divisif), de même que l’algorithme de partition autour des médoïdes (PAM) sont à proscrire dans ces scénarios. Outre l’algorithme, il y a des coûts associés au calcul de la matrice de dissemblance entre chacune des paires des \\(n\\) observations: cette opération nécessite \\(\\mathrm{O}(n^2p)\\) flops pour le calcul et \\(\\mathrm{O}(n^2)\\) entrées de stockage.5 Dans le cas de matrice creuses avec beaucoup de zéros, le coût de stockage et le coût pour réaliser des opérations matricielle (décomposition en valeurs propres et vecteurs propres) peut être réduit à l’aide d’algorithmes dédiés.\nLes méthodes de regroupement peuvent être regroupées grossièrement dans les catégories suivantes:\n\nméthodes basées sur les centroïdes et les médoïdes (\\(k\\)-moyennes, \\(k\\)-médoides PAM, CLARA)\nmélanges de modèles (mélanges Gaussiens, etc.)\nméthodes basées sur la connectivité (regroupements hiérarchiques, AGNES et DIANA)\nméthodes basées sur la densité (DBScan)\n\nDans certaines méthodes paramétriques (catégories 1 à 3), le nombre de groupes est fixé apriori et est un hyperparamètre du modèle. Les méthodes nonparamétriques déterminent plutôt ce nombre automatiquement, mais spécifient un paramètre qui contrôle le degré de lissage.\nNous survolerons uniquement les caractéristiques des principales méthodes.\n\n7.5.1 \\(K\\)-moyennes\nL’algorithme des \\(K\\)-moyennes est un des plus couramment employé en raison de son faible coût. L’idée est la suivante: on assigne chaque observation à un de \\(K\\) regroupements et on calcule la distance entre cette dernière et un prototype \\(\\boldsymbol{\\mu}_k\\) pour le regroupement \\(k\\). La fonction objective que l’on cherche à minimiser est \\[\n\\min_{\\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K}\\min_{\\stackrel{r_{ik} \\in \\{0, 1\\}}{r_{i1} + \\cdots + r_{iK}=1}}\\underset{\\text{distance entre obs. $i$ et le prototype le plus près}}{\\sum_{i=1}^n \\sum_{k=1}^K r_{ik}d(\\mathbf{X}_i,  \\boldsymbol{\\mu}_{k})}\n\\tag{7.1}\\] où \\(r_{ik}=1\\) si l’observation \\(\\mathbf{X}_i\\) (soit la \\(i\\)e ligne de la base de données) est assignée au groupe \\(k\\). Si on utilise la distance Euclidienne carrée, alors la fonction objective correspond à la somme du carré des erreurs au sein de chaque regroupement et on cherche à minimiser l’erreur quadratique moyenne. Les coordonnées optimales \\(\\widehat{\\boldsymbol{\\mu}}_k\\) pour le prototype si on connaît les étiquettes de groupes sont celles du barycentre des \\(n_k\\) observations du groupe \\(k\\), soit \\[\\begin{align*}\n\\widehat{\\boldsymbol{\\mu}}_k = \\frac{\\sum_{i} r_{ik} \\mathbf{X}_i}{n_k}, \\quad k = 1, \\ldots, K;\n\\end{align*}\\] d’où l’appelation \\(K\\)-moyennes. Si on utilise plutôt la distance de Manhattan (\\(l_1\\)), alors la solution est la médiane coordonnée par coordonnées des observations du groupe. Il n’est pas possible de déterminer l’allocation optimale de \\(n\\) observations en \\(K\\) groupes (problème NP complet), mais il est en revanche possible de trouver rapidement une solution approximative au problème.\nPour ce faire, on sélectionne préalablement un nombre \\(K\\) de regroupements et les coordonnées de départ pour les prototypes. L’algorithme itère entre deux étapes:\n\nAssignation (étape E): calculer la distance entre chaque observation et les prototypes; assigner chaque observation au prototype le plus près.\nMise à jour (étape M): estimer les coordoonnées des nouveaux prototypes; si on utilise la distance Euclidienne, cela revient à calculer le barycentre (la moyenne variable par variable) des observations assignées aux regroupements.\n\nEn pratique, l’algorithme convergera rapidement vers une solution locale. Cette dernière est simplement une assignation pour laquelle, d’une itération à l’autre, aucune observation ne change de groupe.\nL’algorithme des \\(K\\)-moyennes présenté offre une forme de partitionnement dite rigide: chaque observation est assignée à un seul regroupement. Si cette appartenance unique peut être logique pour les points à proximité du barycentre, ceux situés à l’intersection des frontières qui définissent les différents regroupements pourraient parfois légitimement faire partie d’un ou l’autre de ces derniers. On pourrait plutôt assigner un poids représentant la probabilité d’être dans un des \\(K\\) regroupements, appelé responsabilité et dénotée \\(r_{ik}\\). Avec une assignation rigide, \\(r_{ik}=1\\) si l’observation \\(i\\) est dans le regroupement \\(k\\) et \\(r_{ik}=0\\) sinon.\n\nLa Figure 7.3 montre une animation avec un jeu de données fictif et \\(K=3\\) regroupements.\n\n\n\n\n\nFigure 7.3: Animation de l’algorithme des \\(K\\)-moyennes avec \\(K=3\\) regroupements.\n\n\n\n\n\nQuelquefois, on peut vouloir prédire les étiquettes de groupes de nouvelles observations. Sans réentraîner l’algorithme, on pourrait ainsi assigner de nouvelles observations au barycentre le plus près.\nVoici quelques forces et faiblesses de la méthode des \\(K\\)-moyennes\n\nL’algorithme des \\(K\\)-moyennes a une complexité linéaire dans la dimension et dans le nombre de variables, soit \\(\\mathsf{O}(np)\\). Ce faible coût de calcul est un avantage avec des mégadonnées (\\(n\\) grand) et en haute dimension \\(p\\) grand).\nL’algorithme converge rapidement vers une solution et on a des garanties que la solution est un maximum local, puisque l’algorithme minimise les répartitions et les prototypes tour à tour.\nLes \\(K\\)-moyennes créent des regroupements globulaires d’apparence sphérique si on utilise la distance Euclidienne: cela revient à faire une séparation linéaire de l’espace (voir Figure 7.6).\nChaque observation est assignée à un seul des \\(K\\) regroupements (assignation rigide).\nComme toutes les observations font partie des \\(K\\) groupes, les valeurs aberrantes ne sont pas traitées à part. Or, la présence de valeurs aberrantes impacte le barycentre des observations du groupe. Comme ce dernier donne le prototype du groupe, l’algorithme manque de robustesse.\nL’algorithme est sensible aux valeurs initiales des prototypes et retourne des solutions différentes selon ces dernières.\n\n\n\n\n\n\nPerformance de l’algorithme des \\(K\\)-moyennes en fonction de différents scénarios: (haut, à gauche) nombre incorrect de classe et données normales de même variance, bien séparées, (haut, à droite) données avec excès de zéro, un cas où les \\(K\\)-moyennes ignorent la topologie des regroupements, et ne segmente pas adéquatement les regroupements connectés (bas, à gauche) données elliptiques de même variance, mais fortement corrélées. Comme le critère minimise la distance intra-groupe sans pondération, les points regroupés appartiennent à différentes classes et (bas, à droite) données sphériques de variances différentes. L’algorithme des \\(K\\)-moyennes réussit une bonne segmentation si les groupements sont compacts et bien séparés.\n\n\n\n\n\nChoix des hyperparamètres\nL’algorithme des \\(K\\)-moyennes comporte plusieurs paramètres, dit hyperparamètres, qui sont fixés par l’utilisateurs préalablement à la segmentation. Ces derniers incluent\n\nles valeurs initiales des prototypes\nle nombre de groupes \\(K\\)\nle choix de la mesure de distance.\n\n\n\nValeurs initiales des prototypes\nComme mentionné précédemment, les regroupements obtenus peuvent varier fortement en fonction des valeurs de départ: la Figure 7.4 montre trois regroupements visibles avec une segmentation qui fusionne deux groupes apparents (gauche), et une solution plus sensée à droite. Une segmentation sera supérieure à une autre si elle a une plus petite valeur de la fonction objective de l’Équation 7.1: les points seront moins dispersés autour de leurs prototypes.\n\n\n\n\n\nFigure 7.4: Résultat d’une analyse de regroupement avec \\(K=3\\) groupes avec une mauvaise initialisation principale (gauche) et une bonne initialisation (droite).\n\n\n\n\nLa solution la plus simple est de choisir aléatoirement des coordonnées initiales pour les prototypes et de répéter la segmentation plusieurs fois, en choisissant à la fin celle qui a la plus petite valeur du critère objectif.\nOn peut également choisir des valeurs suffisamment éloignées: l’algorithme des \\(K\\)-moyennes\\({}^{++}\\) est une variante algorithmique qui propose de choisir des barycentres éloignés les uns des autres (ce qui réduit typiquement le nombre d’itérations). Cette méthode d’initialisation sélectionne une observation au hasard et on l’assigne comme premier prototype, disons \\(\\boldsymbol{\\mu}_1\\). Par la suite, on procède avec \\(k=2, \\ldots, K\\) aux étapes suivantes:\n\ncalcul de la distance carrée minimale entre l’observation \\(\\mathbf{X}_i\\) et les prototypes précédemment choisis, \\[\\begin{align*}\np_i = \\min \\{d(\\mathbf{X}_i, \\boldsymbol{\\mu}_1; l_2)^2, \\ldots, d(\\mathbf{X}_i, \\boldsymbol{\\mu}_{k-1}; l_2)^2)\\}\n\\end{align*}\\]\nChoisir la valeur initiale du \\(k^{\\text{e}}\\) prototype au hasard parmi les observations avec une probabilité de \\(p_i/\\sum_{j} p_j\\) pour l’observation \\(\\mathbf{X}_i\\).\n\nÀ la fin, on obtiendra \\(K\\) valeurs initiales qui serviront à l’initialisation. Ce faisant, on peut espérer ne pas avoir à faire plusieurs allocations aléatoires, puisque les valeurs de départ choisies sont raisonnablement éloignées les unes des autres.\n\n\nNombre de regroupements\nL’autre paramètre crucial des \\(K\\)-moyennes est le nombre de regroupements, \\(K\\). Il est difficile de savoir combien de regroupements sélectionner apriori, puisque la visualisation en haute dimension est difficile et on est souvent loin de la situation présentée dans la Figure 7.4. On pourrait envisager de rouler l’algorithme avec plusieurs valeurs de \\(K\\) et de comparer les résultats, mais sur quelle base?\nLa fonction objective de l’Équation 7.1 avec la distance Euclidienne représente la somme du carré des distances (SCD) entres les observations d’un groupe et leur barycentre, soit la variabilité totale des observations des \\(K\\) différents groupes autour de leur barycentre, \\[\\begin{align*}\n\\mathsf{SCD}_K = \\mathsf{SCD}_{1,K} + \\cdots + \\mathsf{SCD}_{K,K}\n\\end{align*}\\] où la somme du carré des distances des observations du groupe \\(k\\) (pour lesquelles \\(r_{.k}=1\\)) \\[\\begin{align*}\n\\mathsf{SCD}_{k,K} &= \\underset{\\mbox{distance $l_2$ entre obs. du groupe $k$ et barycentre $k$}}{\\sum_{i=1}^n r_{ik}\\|\\mathbf{X}_i -  \\boldsymbol{\\mu}_{k}\\|_2}.\n\\end{align*}\\] La somme des carrés totales correspond à la somme du carré des distances au barycentre avec un seul regroupement, \\(\\mathsf{SCT} = \\mathsf{SCD}_{1}\\).\nLa valeur optimale de la somme du carré des distances mesure va mécaniquement diminuer à mesure que le nombre de regroupements augmente parce que le modèle aura plus d’opportunités pour réduire la variabilité intra-groupe, donc \\(\\mathsf{SCD}_1 > \\mathsf{SCD}_2 \\cdots\\). En pratique, cela peut ne pas être le cas si le minimum local est sous-optimal. Si la réduction de la somme du carré des distances est négligeable, on pourrait penser que la valeur ajoutée d’un groupe supplémentaire (qui implique plus de paramètres à estimer et plus de segments à interpréter) est faible.\nOn peut calculer un coefficient de détermination, qui mesure pourcentage de variance expliquée, soit \\(R^2_K = 1-\\mathsf{SCD}_K/\\mathsf{SCT}\\). De la même manière, on s’attend à une diminution du critère et on pourrait calculer le \\(R^2\\) semi-partiel \\((\\mathsf{SCD}_{k} - \\mathsf{SCD}_{k-1})/\\mathsf{SCT}\\) pour \\(k \\geq 2\\).6\nOn pourrait aussi tracer un diagramme de la somme du carré des distances en fonction de \\(K\\) en ajoutant une pénalité à notre fonction objective. En effet, avec la distance Euclidienne carrée, il y a une analogie à faire avec un modèle de régression et on peut légitimement utiliser un critère d’information pour guider notre choix de \\(K\\): le nombre de paramètres est \\(Kp\\), soit les valeurs des \\(p\\) coordonnées des \\(K\\) barycentres. On utilisera donc un critère d’information de type BIC.\nIl est possible que ces critères donne beaucoup plus de regroupements que ce que l’analyste est prêt(e) à envisager. Il faut garder en tête que, davantage qu’un critère mathématique, l’interprétabilité des regroupements est notre principale critère. Les critères d’information peuvent retourner trop ou pas assez de groupe: à titre d’exemple, le panneau de gauche de la Figure 7.5 montre la somme du carré des distances pour la Figure 7.4; on voit un coude à \\(K=2\\), mais il y avait visiblement trois regroupements, dont deux rapprochés.\n\n\n\n\n\nFigure 7.5: Valeur de la fonction objective (somme du carré des distances) en fonction du nombre de regroupements \\(K\\).\n\n\n\n\n\n\nMesure de distance\nToutes les distances \\(l_q\\) peuvent être utilisées, mais le choix de la distance Euclidienne carrée est particulièrement commode et populaire7 entraîne une partition linéaire de l’espace, comme l’illustre la Figure 7.6. La solution du problème d’optimisation est explicite, ce qui accélère les calculs (les prototypes correspondent aux barycentres). Sauf indication contraire, on supposera dans ce qui suit que la distance entre un point et un prototype est calculée avec la distance Euclidienne au carré.\n\n\n\n\n\nFigure 7.6: Partitions de Voronoï pour les barycentres (cercles) obtenus dans la solution des \\(K\\)-moyennes. La ligne de démarcation qui sépare les groupes est linéaire.\n\n\n\n\n\n\n7.5.1.1 Application en R\nDans R, la fonction kmeans dans le paquet de base stat permet de faire l’analyse de regroupement. Elle ne prend pas en charge les valeurs manquantes. La fonction a plusieurs arguments, dont les coordonnées initiales des prototypes (center; cet argument peut également être un entier qui dicte le nombre de groupes), le nombre maximum d’itération de l’algorithme EM (iter.max) et le nombre de fois qu’on redémarre l’algorithme avec des valeurs aléatoires (nstart).\nOn va estimer le modèle en faisant varier le nombre de regroupements avec pour chaque valeur de \\(K\\) 10 ensembles de valeurs de départ aléatoires.\n\nset.seed(60602)\nkmoy <- list()\nngmax <- 10L\nfor(i in seq_len(ngmax)){\n kmoy[[i]] <- kmeans(donsmult_std,\n                     centers = i,\n                     nstart = 10)\n}\n\nIl suffit ensuite de choisir le nombre de regroupements voulus. Rappelez-vous que le résultat des k-moyennes est aléatoire (parce que les valeurs initiales des prototypes le sont) et les étiquettes peuvent être permutées d’une fois à l’autre même si les regroupements sont les identiques.\nÀ des fins d’illustration, regardons la solution avec \\(K=5\\) regroupements. On pourrait également utiliser l’algorithme \\(K\\)-moyennes\\({}^{++}\\) avec kcca du paquet flexclust. Le code ci-dessous montre le résultat avec la distance de Manhattan (\\(K\\)-médianes)\n\nset.seed(60602)\nkmed5 <- flexclust::kcca(\n  x = donsmult_std,\n  k = 5,\n  family = flexclust::kccaFamily(\"kmedians\"),\n  control = list(initcent = \"kmeanspp\"))\n# Vérifier répartition\nkmed5@clusinfo\n# Coordonnées des K-médianes (standardisées)\nt(t(kmed5@centers)*dm_std + dm_moy)\n# Étiquettes\nkmed5@cluster\n\nIl est toujours utile de regarder la taille des regroupements pour voir si on ne se trouve pas avec des regroupements fortements débalancés.\n\nkmoy5 <- kmoy[[5]]\n# Regarder la répartition\nkmoy5$size\n## [1]  993   64 3812 4496 4252\n\nOn peut étudier les coordonnées des prototypes (par exemple, avec kmoy5$centers), mais ici les données standardisées ne sont pas directement interprétables. On procède plutôt au calcul des statistiques descriptives des profils rapportées dans le Tableau 7.2.\n\ndonsmult |>\n  group_by(groupe = kmoy5$cluster) |>\n  summarise_all(mean)\n\n\n\n\n\nTableau 7.2:  Moyenne des variables explicatives par segment (segmentation avec K-moyennes et cinq regroupements). \n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n  \n \n\n  \n    décompte \n    993 \n    64 \n    3812 \n    4496 \n    4252 \n  \n  \n    mtdons \n    13.92 \n    445.49 \n    24.98 \n    15.32 \n    12.11 \n  \n  \n    ndons \n    2.98 \n    11.38 \n    13.71 \n    4.00 \n    4.63 \n  \n  \n    recence \n    64.56 \n    67.14 \n    27.34 \n    29.00 \n    172.06 \n  \n  \n    anciennete \n    219.46 \n    255.45 \n    252.77 \n    83.59 \n    247.85 \n  \n  \n    vdonsmax \n    22.39 \n    1069.30 \n    61.19 \n    22.53 \n    19.23 \n  \n  \n    ddons \n    7.49 \n    1.92 \n    1.65 \n    1.60 \n    1.87 \n  \n  \n    nrefusconsec \n    1.82 \n    0.52 \n    0.47 \n    0.62 \n    3.15 \n  \n  \n    snrefus \n    3.17 \n    0.88 \n    1.05 \n    4.23 \n    2.71 \n  \n  \n    mpromesse \n    15.32 \n    620.32 \n    45.13 \n    17.70 \n    7.67 \n  \n\n\n\n\n\n\nLes regroupements obtenus sont interprétables:\n\nGroupe 1: Petits donateurs, faible nombre de dons. N’ont pas donné depuis longtemps. Refus fréquents et délai entre dons élevés\nGroupe 2: Grands donateurs fidèles: plus petit groupe. Ces personnes ont fait plusieurs dons, leur valeur maximale est élevée. N’ont pas donné récemment.\nGroupe 3: Petits donateurs récidivistes. Dons plus élevés que la moyenne mais beaucoup de dons de faible valeur et peu fréquents.\nGroupe 4: Petits nouveaux. Moins d’ancienneté, dons fréquents et refus fréquents relativement à l’ancienneté.\nGroupe 5: Petits donateurs inactifs. Plutôt anciens, plusieurs refus.\n\nOn peut représenter graphiquement les regroupements obtenus sur les premières composantes principales avec les deux mesures de dissemblance.\n\n\n\n\n\nFigure 7.7: Nuage de points des deux premières composantes principales des observations de dons multiples avec les étiquettes des regroupements obtenus selon la méthode des \\(K\\)-moyennes et \\(K\\)-médianes avec \\(K=5\\) regroupements.\n\n\n\n\nAvec les \\(K\\)-médianes, les personnes qui ont fait des dons plus élevés sont fusionnés avec d’autres personnes qui ont fait des dons moins élevés et les groupes sont plus de taille comparable. Selon l’objectif des regroupements, cela peut être avantageux, mais cibler les donateurs les plus généreux semble plus logique dans le contexte.\nOn peut étudier l’impact de l’augmentation du nombre de groupes à l’aide de différents critères. Le premier est la somme des carrés des distances intra-groupes.\n\nscd <- sapply(kmoy, function(x){x$tot.withinss})\n# Graphiques\nhomogene <- homogeneite(scd)\nbic_kmoy <- sapply(kmoy, BIC)\n\n\n\n\nFigure 7.8: Graphiques de l’homogénéité (\\(R\\) carré et \\(R\\) carré semi-partiel).\n\n\n\n\nOn peut aussi observer directement la diminution de la somme du carré des erreurs en incluant une pénalité. Ici, tous les critères pointent vers un nombre de regroupements plus élevé que 10, mais ce peut être trop.\n\n\n\n\n\nFigure 7.9: Coefficient BIC pour les \\(K\\)-moyennes en fonction du nombre de regroupements.\n\n\n\n\n\n\n\n7.5.2 \\(K\\)-médoides\nL’algorithme des \\(K\\)-moyennes spécifie que le barycentre des regroupements est le prototype. On pourrait également choisir pour ce dernier une des observations du groupe. Cette approche dite des médoïdes est plus coûteuse en calcul, mais permet d’avoir une observation réellement observée et est un peu moins sensible aux extrêmes et aux aberrances, bien que ce fait soit disputé.\nL’algorithme de partition autour des médoïdes (PAM) procède comme suit:\n\nInitialisation: sélectionner \\(K\\) des \\(n\\) observations comme médoïdes initiaux.\nAssigner chaque observation au médoïde le plus près.\nCalculer la dissimilarité totale entre chaque médoïde et les observations de son groupe.\nPour chaque médoïde \\((k=1, \\ldots, K\\)): considérer tous les \\(n-K\\) observations à tour de rôle et permuter le médoïde avec l’observation. Calculer la distance totale et sélectionner l’observation qui diminue le plus la distance totale.\nRépéter les étapes 2 à 4 jusqu’à ce que les médoïdes ne changent plus.\n\nPuisque qu’on considère chaque observation comme candidat à devenir un médoïde à chaque étape, le coût de calcul est prohibitif.\nL’algorithme CLARA, décrit dans Kaufman and Rousseeuw (1990), réduit le coût de calcul et de stockage en divisant l’échantillon en \\(S\\) sous-échantillons de taille approximativement égale (par défaut 5) et en utilisant l’algorithme PAM sur chacun. Une fois les médoïdes obtenus, le reste de toutes les observations de l’échantillon sont assignées au regroupement du médoïde le plus près. La qualité de la segmentation est pour chacune des \\(S\\) segmentations est calculée en obtenant la distance moyenne entre les médoïdes et les observations; on retourne la solution qui a la plus petite distance moyenne.\nLa qualité des regroupements est obtenue en utilisant la moyenne des distances entre les regroupements et leurs médoïdes. On peut également tracer un graphique des silhouettes: pour chaque observation, on calcule la moyenne des dissimilarités entre l’observation \\(\\mathrm{X}_i\\) et celles de chaque regroupement, disons \\(a_i\\). On calcule de la même manière la distance moyenne entre \\(\\mathrm{X}_i\\) et chaque autre regroupement et on retient le minimum de ces distances, \\(b_i\\).\nLa valeur de la silhouette est simplement \\(s_i=(b_i-a_i)/\\max\\{a_i, b_i\\}\\). Il est possible que la silhouette \\(s_i\\) soit négative: cela indique généralement des observations mal regroupées. De bons regroupements seront obtenus si la silhouette est élevée: on s’attend, si les groupes sont très éloignées les uns des autres, à avoir des profils plus uniformes et une silhouette moyenne plus élevée.\n\n\n\n\n\nFigure 7.10: Profil des silhouettes pour deux regroupements d’un jeu de données: la segmentation de droite est supérieure parce que les regroupements sont plus homogènes et mieux équilibrés.\n\n\n\n\nOn estime avec nos données de dons multiples les regroupements. Étant donné la taille conséquente de la base de données, il est préférable d’utiliser l’algorithme CLARA (Clustering large applications).\n\nkmedoide <- list()\nset.seed(60602)\nfor(k in seq_len(ngmax)){\n  # Algorithme quadratique en sampsize\nkmedoide[[k]] <- cluster::clara(x = donsmult_std,\n               k = k,\n               sampsize = 500,\n               metric = \"euclidean\", # distance,\n               #cluster.only = TRUE, # ne conserver que étiquettes\n               rngR = TRUE, # germe aléatoire depuis R\n               pamLike = TRUE, # même algorithme que PAM\n               samples = 10) #nombre de répétitions\n}\n\nComme les \\(K\\)-moyennes, on fera plusieurs essais pour trouver de bonnes valeurs de départ. On peut tracer le profil des silhouettes (Figure 7.11)\n\nplot(factoextra::fviz_silhouette(kmedoide[[4]]),\n     print.summary = FALSE)\n##   cluster size ave.sil.width\n## 1       1  146          0.29\n## 2       2  190          0.25\n## 3       3   90          0.33\n## 4       4   74          0.26\n\n\n\n\nFigure 7.11: Silhouettes pour les données de dons multiples avec l’algorithme CLARA pour \\(K=4\\) regroupements.\n\n\n\n\nPuisque les prototypes (médoïdes) sont des observations, on peut simplement extraire leur identifiant. La sortie inclut plusieurs éléments dont la taille des regroupements, la valeur du critère PAM, etc.\n\nmedoides_orig <- donsmult[kmedoide[[4]]$i.med,]\nmedoides_orig\n# Taille des regroupements\nkmedoide[[4]]$clusinfo\n\nVoici quelques avantages et inconvénients des \\(K\\)-médoides.\n\nles prototypes sont des observations de l’échantillon.\nla fonction objective est moins impactée par les extrêmes.\nle coût de calcul est prohibitif avec des mégadonnées.\n\n\n\n7.5.3 Mélange de modèles\nL’algorithme des \\(K\\)-moyennes fait une allocation rigide: chaque observation est assignée à un seul regroupement, ignorant de ce fait l’incertitude rattachée à l’étiquetage des observations. Les frontières de la région, obtenue en calculant l’intersection des courbes sphériques de regroupement, sont linéaires.\nPeut-être plus problématique, la distance Euclidienne non pondérée impose des regroupements convexes et sphériques de taille semblable: la qualité des regroupements des \\(K\\) moyennes est donc mauvaise si les regroupements ne sont pas sphériques ou globulaires, ou sont de concentrations inégales.\nUne approche plus générale considère que \\(X_1, \\ldots, X_p\\) sont tirées d’un mélange à \\(K\\) composantes de lois spécifiées. Généralement, on choisit une loi normale multidimensionnelle pour le \\(k\\)e groupe \\(G\\), \\[\\begin{align*}\n\\boldsymbol{X} \\mid G=k \\sim \\mathsf{No}_p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n\\end{align*}\\] On suppose qu’on a \\(K\\) groupes, chacun caractérisé par une densité de dimension \\(p\\), soit \\(f_k(\\boldsymbol{X}_i;\\boldsymbol{\\theta}_k)\\) si \\(\\boldsymbol{X}_i\\) provient du groupe \\(k\\) pour \\(k=1, \\ldots, K\\).\nOn réécrit la vraisemblance en fonction de \\(\\pi_k\\), la probabilité qu’une observation \\(\\mathbf{X}_i\\) tombe dans le groupe \\(k\\), \\[\\begin{align*}\nL_i(\\boldsymbol{\\theta}_1, \\ldots, \\boldsymbol{\\theta}_K; \\pi_1, \\ldots, \\pi_K, \\mathbf{X}_i)= \\sum_{k=1}^K\\pi_k\nf_{k}(\\boldsymbol{X}_i,\n\\boldsymbol{\\theta}_{k}).\n\\end{align*}\\]\nSi on savait de quelle composante l’observation originait, on pourrait simplement obtenir les estimation du maximum de vraisemblance pour les paramètres de moyenne et de variance. Inversement, si on avait les valeurs des paramètres, on pourrait déterminer de quel composante l’observation est la plus susceptible de parvenir à l’aide des poids. Le modèle est estimé à l’aide de l’algorithme d’espérance-maximisation, qui itère entre l’estimation des probabilités, et celles des autres composantes. Le paramètres retournés correspondent à un maximum local, et on peut également obtenir un estimé de la variabilité de ces paramètres. Ainsi, le mélange de modèle nous donne accès à la fois à l’incertitude des paramètres et à la probabilité \\(\\pi_k\\) qu’une observation appartiennent au groupe \\(G_k\\).\nLa loi multinormale est caractérisée par une moyenne (qui peut servir de prototype) et par une matrice de covariance \\(\\boldsymbol{\\Sigma}_k\\). Si on paramétrise cette dernière, on peut obtenir plus de flexiblité selon que les variances soient différentes d’une variable à l’autre, ou que les variables soient corrélées. On peut également spécifier que certains éléments (structure de corrélation, variances) de \\(\\boldsymbol{\\Sigma}_k\\) sont communes à tous les regroupements. En laissant les paramètres varier, on peut capturer l’effet de regroupements de tailles et de densité différente au prix de plus de paramètres et d’un plus petit nombre d’observations pour estimer chacun d’entre eux.\nSi \\(p\\) est élevé, la structure de covariance non structurée possède trop de paramètres pour être utile. On limitera ce nombre en choisissant plutôt une paramétrisation plus parsimonieuse qui impose des contraintes sur la forme des ellipsoïdes, propres ou communes à tous les groupes.\nLa matrice de covariance dans mclust est paramétrisée en fonction de \\(\\lambda\\), qui contrôle le volume, une matrice diagonale \\(\\mathbf{A}\\) qui contrôle les variances de chaque observation et \\(\\mathbf{D}\\) une matrice orthogonale qui permet de créer de la corrélation entre observations. Un index \\(k\\) spécifie que cette composante varie d’un regroupement à l’autre.\nLes trois lettres de l’identifiant pour volume/forme/orientation déterminent si cette composante est égale (E), si elle varie d’un regroupement à l’autre (V) ou si elle est indéterminée (I). Par exemple, EII spécifie une matrice de covariance où chaque composante a variance \\(\\lambda\\) et où les composantes sont indépendantes. Voir mclust.options(\"emModelNames\") et la documentation dans le Tableau 3 de Scrucca et al. (2016).\n\n\n\n\n\nFigure 7.12: Forme des ellipsoïdes pour le mélange de modèle selon la forme de la structure de covariance. Image extraite de Scrucca et al. (2016) (Figure 2) partagée sous licence CC BY 4.0.\n\n\n\n\nVoici quelques avantages et inconvénients des mélanges de modèles Gaussiens\n\ncette approche est plus flexible que les \\(K\\)-moyennes.\nl’ajout d’une composante uniforme permet de gérer les aberrances (supporté par mclust).\nl’algorithme EM garantie la convergence à un minimum local (comme pour les \\(K\\)-moyennes)\non obtient une assignation probabiliste plutôt que rigide, également pour la classification\nle coût de calcul est plus élevé que les \\(K\\)-moyennes\nle nombre de paramètre des matrices de covariance augmente rapidement avec la dimension \\(p\\)\n\n\n7.5.3.1 Hyperparamètres\nPour le mélange de modèle, on doit fixer apriori le nombre de groupes \\(K\\), la forme des ellipsoïdes et les valeurs pour l’initialisation. Les mêmes considérations pratiques qu’avec les \\(K\\)-moyennes s’appliquent, bien qu’ici l’utilisation des critères d’information permette plus légitimement de choisir le nombre de regroupements.\nLa forme des ellipsoïdes est un compromis entre simplicité (d’estimation) et nombre de paramètres: un modèle plus flexible sera plus difficile à estimer et nécessitera plus de temps de calcul et un plus grand nombre d’échantillon. En petite dimension, il peut être utile d’effectuer une visualisation préalable pour déterminer quel type de modèle serait suffisant. Règle générale, il faut aussi considérer le nombre de paramètres à estimer (qui dépend de \\(p\\)) et le nombre d’observations par regroupement. Comme tous les modèles sont estimés avec la méthode du maximum de vraisemblance, on peut toujours ajuster tous les types de structures de covariance pour un nombre de regroupements \\(K\\) donné et retourner les critères d’information (BIC) pour sélectionner le meilleur mélange de modèles. La fonction mclustBIC du paquet mclust permet de calculer ces modèles et la méthode summary retourne les trois meilleurs modèles selon le critère d’information.\n\n\n7.5.3.2 Paquet mclust\nLa stratégie de base du paquet mclust (Scrucca et al. 2016) est d’ajuster des mélanges de modèles gaussiens avec plusieurs structures de covariance en faisant varier le nombre de regroupements. Le modèle sélectionné parmi tous les candidats est celui qui a la plus petite valeur du critère BIC: ce dernier dépend de la qualité de l’ajustement et la pénalité prend en compte le nombre de paramètres de covariance, en plus des moyennes. Il est possible d’ajouter une composantes pour le bruit, de manière à éviter que les valeurs aberrantes impactent négativement la segmentation.\nUne fois le modèle obtenu, plusieurs fonctionalités sont disponibles pour représenter graphiquement les ellipses des modèle pour chaque paire de variable, les nuages de points des paires de variables avec différents symboles et couleurs pour les regroupements, etc.\n\n## Mélanges de modèles gaussiens\nset.seed(60602)\nlibrary(mclust)\nmmg <- Mclust(data = donsmult_std,\n       G = 1:10,\n       # Ajouter composante uniforme\n       #  pour bruit (aberrances)\n       initialization = list(noise = TRUE))\n# Résumé de la segmentation\nsummary(mmg)\n\nOn peut obtenir les étiquettes (avec 0 pour le bruit) avec mmg$classification. Le graphique du critère d’information Bayésien (BIC) montre le négatif: on cherche donc la structure de covariance et le nombre qui maximise \\(-\\mathsf{BIC}\\).\n\nplot(mmg, what = \"BIC\")\n\n\n\n\nFigure 7.13: Valeur du négatif du critère d’information Bayésien pour les mélanges de modèles gaussiens selon le nombre de regroupements et la structure de covariance.\n\n\n\n\nAvec notre grande base de données, le modèle identifie neuf regroupements et un volume variable. On peut utiliser des techniques de réduction de la dimension pour obtenir une représentation graphique.\n\n# Matrice des nuage de points (paires de variables)\n# plot(mmg, what = \"classification\")\n# Réduction de la dimension\nreduc_dim_mmg <- mclust::MclustDR(mmg)\npar(mfrow = c(1,2)) # graphiques côte-à-côte\nplot(reduc_dim_mmg, what = \"contour\")\n## Error in parameters$variance$sigma[, , k]: indice hors limites\nplot(reduc_dim_mmg, what = \"scatterplot\")\n\n\n\n\nFigure 7.14: Projection des observations, colorées par regroupement (gauche) et structure des regroupements avec ellipsoides de confiance (droite).\n\n\n\n\n\n\n\n7.5.4 Regroupements hiérarchiques\nHistoriquement très utilisés dans les années 70, les méthodes de regroupement hiérarchique offrent une méthode déterministe de regroupement à partir d’une matrice de dissimilarité.\nL’algorithme pour la procédure agglomérative procède comme suit:\n\nInitialisation: chaque observation forme son propre groupe.\nles deux groupes les plus rapprochés sont fusionnés; la distance entre le nouveau groupe et les autres regroupements est recalculée.\non répète l’étape 2 jusqu’à obtenir un seul regroupement.\n\nLa procédure divisive procède de la même façon, mais en partant d’un seul ensemble et en subdivisant ce dernier jusqu’à ce qu’il y ait autant d’observations que de groupes. Cette dernière est préférable si on veut isoler de grands regroupements, mais est rarement employée.\nIl y a plusieurs façons de calculer la distance entre deux groupes d’observations. Selon notre définition, nous obtiendrons des regroupements différents. Les méthodes les plus populaires incluent\n\nliaison simple (plus proches voisins)\nliaison complète (voisins les plus éloignés)\nliaison moyenne: utilise la moyenne des distances entre toutes les paires de sujets (un pour chaque groupe) provenant des deux groupes.\nméthode de Ward: calcul de l’homogénéité globale\n\n\n\n\n\n\nFigure 7.15: Distances entre regroupements selon la liaison (simple, complète, barycentre, homogenéité de Ward).\n\n\n\n\nLa méthode de Ward n’est pas définie en terme de distance entre représentants de groupes, mais plutôt en terme de mesure d’homogénéité au sein des groupes. Supposons qu’à une étape du processus hiérarchique, nous avons \\(M\\) groupes et que nous voulons passer à \\(M-1\\). Pour chaque groupe \\(k\\), nous pouvons calculer la somme des carrés des distances par rapport à la moyenne du groupe, disons \\(\\mathsf{SCD}_k\\): plus cette distance est petite, plus le groupe est compact et homogène. On calcule ensuite l’homogénéité globale en faisant la somme de l’homogénéité de tous les groupes, soit \\(\\mathrm{H}^{(M)} = \\mathsf{SCD}_1 + \\cdots + \\mathsf{SCD}_M\\). La méthode de Ward va regrouper les deux groupes qui feront augmenter le moins possible l’homogénéité.\nEn général, les algorithmes de regroupement hiérarchiques stockent une matrice de dissemblance \\(n \\times n\\), et donc un coût de stockage quadratique et un coût de calcul \\(\\Omega(n^2)\\) avec \\(\\mathrm{O}(n^3)\\). Il faut réaliser que ce coût de calcul est prohibitif en haute dimension. Certains algorithmique efficaces pour la méthode de liaison simple permettent un temps de calcul quadratique sans calcul de toutes les distances, à coût \\(\\mathrm{O}(n)\\). Si la méthode de liaison simple est la moins coûteuse du lot, elle n’est pas aussi populaire car elle fonctionne bien si l’écart entre deux regroupements est suffisamment grand. S’il y a du bruit entre deux regroupements, la qualité des regroupements en sera affectée. La méthode de liaison complète est moins sensible au bruit et aux faibles écarts entre regroupements, mais a tendance à casser les regroupements globulaires. Puisque le critère d’homogénéité de Ward ressemble à celui des \\(K\\)-moyennes, la sortie aura tendance à bien regrouper les amas globulaires.\nGénéralement, le résultat de la procédure agglomérative avec la méthode de liaison simple inclura quelques valeurs isolées et un seul grand regroupement. Une alternative récente (Gagolewski, Bartoszuk, and Cena 2016), appelée Genie, modifie la fonction objective de la méthode de liaison simple en retenant son efficacité de calcul. Plutôt que de simplement trouver la paire de regroupements à distance minimale, cette fusion n’est appliquée que si une mesure d’inéquité est inférieur à un seuil spécifié par l’utilisateur. Si les regroupements sont fortement inéquitables, la fusion survient entre les regroupements dont un de la taille minimale courante. L’implémentation R (Gagolewski 2021) dans le paquet genieclust est nettement plus rapide que les autres alternatives et ne nécessite pas de calculer la matrice de dissimilarité.\nOn peut comparer les performances des regroupements hiérarchiques selon la méthode de groupement. La page web de scikit-learn developers montre la performance sur des exemples jouets très artificiels, qui montre que selon la structure des données, l’impact de la fonction de liaison. Ici, aucun approche hiérarchique ne performe mieux que les autres dans tous les exemples.\n\n\n\n\n\n\nFigure 7.16: Animation du regroupement hiérarchique (procédure agglomérative) avec la distance de Ward.\n\n\n\n\nLa Figure 7.16 montre les différentes étapes de l’algorithme avec les regroupements étapes par étape, jusqu’à ce qu’on obtienne deux groupes. À l’étape 1, les observations (14, 19) sont regroupées, puis (2, 15), (10, 17). Ce n’est qu’à l’étape 7 qu’on ajoute une observation à un regroupement de deux existants.\n\n\n7.5.4.1 Sélection des hyperparamètres\nOutre le choix de la fonction de liaison qui déterminera la distance entre les regroupements à chaque étape, on devra choisir le nombre de regroupements.\nOn peut représenter le modèle à l’aide d’un dendrogramme, un arbre dont les feuilles indiquent les regroupements à chaque étape jusqu’à la racine à la dernière étape. La distance entre chaque embranchement est déterminée par notre critère: cela nous permet de sélectionner un nombre de regroupements \\(K\\) après inspection du dendrogramme et d’extraire la solution en élaguer l’arbre à cette profondeur.\n\n\n\n\n\nFigure 7.17: Dendrogramme pour l’exemple de regroupement hiérarchique avec la méthode de Ward et 20 observations.\n\n\n\n\nLa hauteur du dendrogramme donne la valeur du critère associé à la mesure de regroupement: on peut sélectionne le nombre de regroupements \\(K\\) en sélectionnant une étape où la qualité de l’ajustement diminue drastiquement. Pour le critère de Ward qui utilise l’homogénéité, on peut créer le pourcentage de variance expliquée, \\(R^2\\) en calculant \\(R^2_{(M)} = 1-\\mathrm{H}_{(M)}/\\mathrm{H}_{(1)}\\), où \\(\\mathrm{H}_{(1)}\\) est simplement la somme du carré des distances distances par rapport à la moyenne lorsque toutes les observations sont dans un même groupe. Le R-carré semi-partiel, qui mesure la perte d’homogénéité d’une étape à l’autre, renormalisée par \\(\\mathrm{H}_{(1)}\\), permet également de mesurer la perte d’homogénéité (relative) en combinant ces deux groupes. On peut faire un graphique de ces deux critères en fonction du nombre de regroupements et chercher un point d’inflection (un coude) à partir duquel la perte d’homogénéité est moindre ou encore le \\(R^2\\) augmente plus lentement.\nLa fonction stat::hclust permet de faire des regroupements agglomératifs (agnes), mais fastcluster propose une version avec une empreinte mémoire inférieure. Le paquet cluster offre de son côté l’algorithme divisif (diana).\nVoici quelques particularités des méthodes de regroupement hiérarchique.\n\nla solution du regroupement hiérarchique est toujours la même (déterministe)\nl’assignation d’une observation à un regroupement est finale\nles aberrances ne sont pas traitées et sont souvent assignées dans des regroupements à part\nles méthodes d’arborescence sont faciles à expliquer\nle nombre de groupes n’a pas à être spécifié apriori (une seule estimation)\nle coût de calcul est prohibitif, avec une complexité quadratique de \\(\\mathrm{O}(n^2)\\) pour la méthode de liaison simple et autrement \\(\\mathrm{O}(n^3)\\) pour la plupart des autres fonctions de liaison.\n\n\n\n\n7.5.5 Méthodes basées sur la densité\nL’algorithme DBSCAN (density-based spatial clustering of applications with noise) est une méthode de partitionnement basée sur la densité des points. L’idée de base de l’algorithme est de tracer une boule de rayon \\(\\epsilon\\) autour de chaque observation et de voir si elle inclut d’autres observations. L’algorithme contient deux hyperparamètres: le rayon \\(\\epsilon\\) et \\(M\\), le nombre minimal de points pour former un regroupement. L’algorithme classe les observations en trois catégories: aberrance, point central et point frontière.\n\nUn point central est une observation qui possède \\(M-1\\) voisins à distance \\(\\epsilon\\).\nUn point périphérique est un point qui est distant de moins de \\(\\epsilon\\) d’un point central, sans en être un.\nUn point isolé est une observation qui n’est pas rattachée à aucun regroupement.\n\nL’algorithme répète les étapes suivantes jusqu’à ce que chaque observation ait été visitée.\n\nChoisir un point aléatoirement parmi ceux qui n’ont pas été visités.\nSi le point n’est pas étiqueté, calculer le nombre de points voisins qui se trouvent dans un rayon \\(\\epsilon\\): s’il y a moins de \\(M\\) observations, provisoirement étiqueter l’observation comme point isolé, sinon comme point central.\nSi l’observation est un point central avec \\(M-1\\) voisins ou plus, créer un regroupement.\nÉtiqueter chaque point à distance \\(\\epsilon\\) créé et l’ajouter au regroupement s’il a un point central comme voisin.\n\nCe site web offre une visualisation interactive des différentes étapes de l’algorithme et de comparer la performance de DBSCAN selon le type de regroupements.\n\nPuisque chaque point est visité à tour de rôle et comparé aux autres pour trouver les plus proches voisins, la complexité brute est \\(\\mathrm{O}(n^2)\\) mais une implémentation efficace permet de réduire ce coût à \\(\\mathrm{O}(n\\ln n)\\) avec un coût pour l’allocation de la mémoire linéaire de \\(\\mathrm{O}(n)\\).\nVoici quelques caractéristiques de DBSCAN:\n\nle traitement des aberrances est automatique et l’algorithme est robuste.\nle nombre de regroupements n’a pas à être spécifié apriori.\nla forme des regroupements est arbitraire, peut être non convexe et de taille différente.\nla complexité de l’algorithme est d’au mieux \\(\\mathrm{O}(n\\ln n)\\).\nles hyperparamètres ont une interprétation physique, mais leur choix n’est pas aisé\nDBSCAN ne permet pas de traiter le cas où la densité des regroupements change et risque de fusionner des regroupements s’il y a une série d’observations qui permet de relier deux regroupements.\ncomme la plupart des algorithmes, le voisinage des points devient épars quand \\(p\\) augmente en raison du fléau de la dimension.\n\n\n\n\n\n\nFigure 7.18: Illustration de la classification des points avec DBSCAN: toutes les observations sont assignées à un regroupement, moins une aberrance.\n\n\n\n\n\n\n\n\n7.5.5.1 Choix des hyperparamètres\nLes deux paramètres, \\(M\\) et \\(\\epsilon\\), sont positivement corrélés: si on augmente le nombre minimal de point \\(M\\) par regroupement, il faudra également augmenter le rayon \\(\\epsilon\\) pour éviter d’avoir un nombre trop élevé de points isolés étiquetés comme points isolés ou comme aberrances.\nPour spécifier le nombre minimal d’observations voisines \\(M\\) pour créer un point central, il faut aussi considérer la dimension \\(p\\) des variables explicatives: la recommandation est de requérir au moins \\(p+1\\) points dans le voisinage. Le choix du rayon peut être plus difficile à déterminer: . Une option est de fixer le nombre de plus proches voisins \\(M\\) et de considérer la distance entre chaque observation et ses plus proches voisins: au sein d’un regroupement, on s’attend à ce que cette distance soit petite. Cela permettra également de déterminer un seuil acceptable pour \\(\\epsilon\\) pour éviter que trop d’observations soient isolées.\nLa fonction kNNdistplot du paquet dbscan permet de tracer un graphique de la distance moyenne des \\(k\\) plus proches voisins pour chaque observation: en prenant \\(k=M-1\\), on peut calculer la distance entre le \\(k\\) plus proche voisin de chaque observation et ordonner ces distances. La recommandation est de choisir \\(\\epsilon\\) en prenant une distance où la plupart des observations ne sont pas voisines (critère du coude).\n\n\n\n\n\nFigure 7.19: Graphique des distances entre chaque observation et son troisième plus proche voisin (gauche), en fonction du pourcentage d’observations à moins de cette distance et regroupements obtenus avec DBSCAN avec \\(M=10\\) et \\(\\epsilon=1.1\\) (droite).\n\n\n\n\nUne variante de l’algorithme DBSCAN, intitulée OPTICS, est plus coûteuse mais permet de gérer le cas de regroupements de densités variables en évitant la spécification de \\(\\epsilon\\)."
  },
  {
    "objectID": "03-regroupements.html#conclusion",
    "href": "03-regroupements.html#conclusion",
    "title": "7  Analyse de regroupements",
    "section": "7.6 Conclusion",
    "text": "7.6 Conclusion\nLe résultat d’une analyse de regroupements est une étiquette pour chaque observation. Parfois, la méthode d’analyse de regroupement retourne également un prototype (le barycentre, une observation du groupe ou la médiane coordonnée par coordonnée) qui permet d’interpréter les regroupements.\nL’analyse de regroupement est une méthode d’apprentissage non-supervisé: l’objectif est de déduire la structure présente dans un ensemble de points sans étiquette préalable (contrairement à la classification). Ainsi, une fois cqu’on a obtenu les étiquettes, on peut comparer les regroupements entre eux avant d’effectuer le profilage. Est-ce que les regroupements sont homogènes et que les observations sont près de leur représentant de groupe? On pourrait calculer les silhouettes et voir si les groupes sont bien équilibrés, etc. S’il n’existe pas de solution, il existe des segmentations de moins bonne qualité (parce que difficilement interprétables, avec des regroupements qui contiennent une poignée d’observations). Si la segmentation n’est pas satisfaisante, on retourne à la planche à dessin et on modifie les variables, la méthode ou la calibration des hyperparamètres jusqu’à ce qu’on soit satisfaits du résultat.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nL’objectif d’une analyse de regroupement est de mettre en commun des observations de telle sorte que les observations d’un même groupe soient le plus semblables possible, et que les groupes soient le plus différent possible les uns des autres.\nChaque observation se voit assigner une étiquette de groupe.\nOn procède ensuite à une analyse descriptive, segment par segment, à l’aide de prototypes\nL’analyse de regroupement est une méthode d’apprentissage non-supervisée: il n’y a pas de véritable séparation.\nLa segmentation n’est utile que si elle a une valeurs ajoutée.\nPlusieurs choix de l’analyste (mesure de dissemblance, algorithme ou méthode de regroupement, choix des hyperparamètres) peuvent donner une segmentation différente. L’analyste a une grande marge de manoeuvre.\nChaque algorithme de segmentation a des avantages et inconvénients.\nL’algorithme des \\(K\\)-moyennes est le plus employé et son faible coût permet son utilisation avec des mégadonnées.\nAucun algorithme ne performe uniformément mieux, mais certains sont plus faciles à employer que d’autres.\n\navec des mégadonnées, la complexité est un facteur important à considérer pour le choix de la méthode.\nla plupart du temps, le choix des hyperparamètres nécessite un peu d’essai-erreur.\nla segmentation peut être médiocre parce que les hyperparamètres sont mal choisis.\n\nLe nombre de groupes peut être guidé par le contexte: les formules et indicateurs de qualité servent de balises.\n\n\n\n\n\n\n\nGagolewski, Marek. 2021. “genieclust: Fast and Robust Hierarchical Clustering.” SoftwareX 15 (July). https://doi.org/10.1016/j.softx.2021.100722.\n\n\nGagolewski, Marek, Maciej Bartoszuk, and Anna Cena. 2016. “Genie: A New, Fast, and Outlier-Resistant Hierarchical Clustering Algorithm.” Information Sciences 363: 8–23. https://doi.org/10.1016/j.ins.2016.05.003.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71. https://doi.org/10.2307/2528823.\n\n\nKaufman, Leonard, and Peter J. Rousseeuw. 1990. Finding Groups in Data: An Introduction to Cluster Analysis. Edited by Wiley. Hoboken, NY. https://doi.org/10.1002/9780470316801.\n\n\nScrucca, Luca, Michael Fop, T. Brendan Murphy, and Adrian E. Raftery. 2016. “mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.” The R Journal 8: 289–317. https://doi.org/10.32614/RJ-2016-021."
  },
  {
    "objectID": "07-donneesmanquantes.html",
    "href": "07-donneesmanquantes.html",
    "title": "8  Données manquantes",
    "section": "",
    "text": "Il arrive fréquemment d’avoir des valeurs manquantes dans notre échantillon. Ces valeurs peuvent être manquantes pour diverses raisons. Si on prélève nous-mêmes nos données, un répondant peut refuser de répondre à certaines questions. Si on acquiert nos données d’une source externe, les valeurs de certaines variables peuvent être manquantes directement dans le fichier obtenu. Si on ne prend pas en compte le méchanisme générant les valeurs manquantes, ces dernières peuvent également biaiser nos analyses. Le but de ce chapitre est de faire un bref survol de ce sujet."
  },
  {
    "objectID": "07-donneesmanquantes.html#principes-de-base",
    "href": "07-donneesmanquantes.html#principes-de-base",
    "title": "8  Données manquantes",
    "section": "8.1 Principes de base",
    "text": "8.1 Principes de base\nSoit \\(X\\) une variable pour laquelle des données sont manquantes. Voici la définition de trois processus de génération de données manquantes.\n\nLes données manquantes de \\(X\\) sont dites manquantes de façon complètement aléatoire (MCAR, de l’anglais missing completely at random) si la probabilité que la valeur de \\(X\\) soit manquante ne dépend ni de la valeur de \\(X\\) (qui n’est pas observée), ni des valeurs des autres variables.\n\nLe fait qu’une variable est manquante peut être relié au fait qu’une autre soit manquante. Des gens peuvent refuser systématiquement de répondre à deux questions dans un sondage. Dans ce cas, si la probabilité qu’une personne ne réponde pas ne dépend pas des valeurs de ces variables (et de toutes les autres), nous sommes encore dans le cas MCAR. Si par contre, la probabilité que les gens ne répondent pas à une question sur leur revenu augmente avec la valeur de ce revenu, alors nous ne sommes plus dans le cas MCAR.\nLe cas MCAR peut se présenter par exemple si des questionnaires, ou des pages ont été égarés ou détruits par inadvertance (effacées du disque rigide, etc.) Si les questionnaires manquants constituent un sous-ensemble choisi au hasard de tous les questionnaires, alors le processus est MCAR. L’hypothèse que les données manquantes sont complètement aléatoires est en général considérée comme trop restrictive.\n\nLes données manquantes de \\(X\\) sont dites données manquantes de façon aléatoire (MAR, de l’anglais missing at random) si la probabilité que la valeur de \\(X\\) soit manquante ne dépend pas de la valeur de \\(X\\) (qui n’est pas observée) une fois qu’on a contrôlé pour les autres variables.\n\nIl est possible par exemple que les femmes refusent plus souvent que les hommes de répondre à une question, par exemple de donner leur âge (et donc, le processus n’est pas MCAR). Si pour les femmes et les hommes, la probabilité que \\(X\\) est manquante ne dépend pas de la valeur de \\(X\\), alors le processus est MAR. Les probabilités d’avoir une valeur manquante sont différentes pour les hommes et les femmes mais cette probabilité ne dépend pas de la valeur de \\(X\\) elle-même. L’hypothèse MAR est donc plus faible que l’hypothèse MCAR.\n\nLes données manquantes de \\(X\\) sont dites manquantes de façon non-aléatoire (MNAR, de l’anglais missing not at random) si la probabilité que la valeur de \\(X\\) soit manquante dépend de la valeur de \\(X\\) elle-même.\n\nPar exemple, les gens qui ont un revenu élevé pourraient avoir plus de réticences à répondre à une question sur leur revenu. Un autre exemple est si une personne transgenre ne répond pas à la question genre (si on offre seulement deux choix, homme/femme) et aucune autre question ne se rattache au genre ou à l’identité sexuelle. La méthode de traitement que nous allons voir dans ce chapitre, l’imputation multiple, est très générale et est valide dans le cas MAR (et donc aussi dans le cas MCAR). Le cas MNAR est beaucoup plus difficile à traiter et ne sera pas considéré ici.\nIl n’est pas possible de tester l’hypothèse que le données sont manquantes de façon aléatoire ou complètement aléatoire; ce postulat doit donc être déterminé à partir du contexte et des variables auxiliaires disponibles."
  },
  {
    "objectID": "07-donneesmanquantes.html#méthodes-dimputation",
    "href": "07-donneesmanquantes.html#méthodes-dimputation",
    "title": "8  Données manquantes",
    "section": "8.2 Méthodes d’imputation",
    "text": "8.2 Méthodes d’imputation\nIl est important de noter que, dans bien des cas, les données manquantes ont une valeur logique: un client qui n’a pas de carte de crédit a un solde de 0! Tous ces cas devraient être traités en amon, d’où l’importance des validations d’usage et du nettoyage préliminaire de la base de données.\n\n8.2.1 Cas complets\nLa première idée naïve pour une analyse est de retirer les observations avec données manquantes pour conserver les cas complets (listwise deletion, ou complete case analysis).\nCette méthode consiste à garder seulement les observations qui n’ont aucune valeur manquante pour les variables utilisées dans l’analyse demandée. Dès qu’une variable est manquante, on enlève le sujet au complet. C’est la méthode utilisée par défaut dans la plupart des logiciels, dont R.\n\nSi le processus est MCAR, cette méthode est valide car l’échantillon utilisé est vraiment un sous-échantillon aléatoire de l’échantillon original. Par contre, ce n’est pas nécessairement la meilleure solution car on perd de la précision en utilisant moins d’observations.\nSi le processus est seulement MAR ou MNAR, cette méthode produit généralement des estimations biaisées des paramètres.\n\nEn général, l’approche des cas complet est la première étape d’une analyse afin d’obtenir des estimés initiaux que nous corrigerons pas d’autre méthode. Elle n’est vraiment utile que si la proportion d’observations manquantes est très faible et le processus est MCAR. Évidemment, la présence de valeurs manquantes mène à une diminution de la précision des estimateurs (caractérisée par une augmentation des erreurs-types) et à une plus faible puissance pour les tests d’hypothèse et donc ignorer l’information partielle (si seulement certaines valeurs des variables explicatives sont manquantes) est sous-optimal.\n\n\n8.2.2 Imputation simple\nL’imputation consiste à remplacer les valeurs manquantes pour boucher le trous. Pour paraphraser Dempster et Rubin (1983),\n\nLe concept d’imputation est à la fois séduisant et dangereux.\n\nAvec l’imputation simple, on remplace les valeurs manquantes par des ersatz raisonnables. Par exemple, on peut remplacer les valeurs manquantes d’une variable par la moyenne de cette variable dans notre échantillon. On peut aussi ajuster un modèle de régression avec cette variable comme variable dépendante et d’autres variables explicatives comme variables indépendantes et utiliser les valeurs prédites comme remplacement. Une fois que les valeurs manquantes ont été remplacées, on fait l’analyse avec toutes les observations.\nL’imputation par le mode ou la moyenne n’est pas recommandée parce qu’elle dilue la corrélation entre les variables explicatives et elle réduit la variabilité. Les modèles de régression mènent également à une-sous estimation de l’incertitude en raison cette fois-ci de l’augmentation de la corrélation, ce qui augmente mécaniquement la significativité des tests, contrairement à l’imputation aléatoire (droite). Le Figure 8.1 montre clairement cet état de fait.\n\n\n\n\n\nFigure 8.1: Différences entre méthodes d’imputation, avec imputation par la moyenne, par le biais d’une régression linéaire et par un modèle aléatoire de régression, de gauche à droite.\n\n\n\n\nEn quoi constitue l’imputation aléatoire recommandée ci-dessus? Considérons le cas d’une régression logistique pour une variable explicative binaire. Plutôt que d’assigner à la classe la plus probable, une prédiction aléatoire simule une variable 0/1 avec probabilité \\((1-\\widehat{p}_i, \\widehat{p}_i)\\). Pour un modèle de régression linéaire, la prédiction\nIl existe d’autres façons d’imputer les valeurs manquantes mais le problème de toutes ces approches est que l’on ne tient pas compte du fait que des valeurs ont été remplacées et on fait comme si c’était de vraies observations. Cela va en général sous-évaluer la variabilité dans les données. Par conséquent, les écarts-type des paramètres estimés seront en général sous-estimés et l’inférence (tests et intervalles de confiance) ne sera pas valide. Cette approche n’est donc pas recommandée.\nUne manière de tenter de reproduire correctement la variabilité dans les données consiste à ajouter un terme aléatoire dans l’imputation. C’est ce que fait la méthode suivante, qui possédera l’avantage de corriger automatiquement les écarts-type des paramètres estimés.\n\n\n8.2.3 Imputation multiple\nCette méthode peut être appliquée dans à peu près n’importe quelle situation et permet d’ajuster les écarts-type des paramètres estimés. Elle peut être appliquée lorsque le processus est MAR (et donc aussi MCAR).\nL’idée consiste à procéder à une imputation aléatoire, selon une certaine technique, pour obtenir un échantillon complet et à ajuster le modèle d’intérêt avec cet échantillon. On répète ce processus plusieurs fois et on combine les résultats obtenus.\n\n\n\n\n\n\n\n\n\nL’estimation finale des paramètres du modèle est alors simplement la moyenne des estimations pour les différentes répétitions et on peut également obtenir une estimation des écarts-type des paramètres qui tient compte du processus d’imputation.\nPlus précisément, supposons qu’on s’intéresse à un seul paramètre \\(\\theta\\) dans un modèle donné. Ce modèle pourrait être un modèle de régression linéaire, de régression logistique, etc. Le paramètre \\(\\theta\\) serait alors un des \\(\\boldsymbol{\\beta}\\) du modèle.\nSupposons qu’on procède à \\(K\\) imputations, c’est-à-dire, qu’on construit \\(K\\) ensemble de données complets à partir de l’ensemble de données initial contenant des valeurs manquantes. On estime alors les paramètres du modèle séparément pour chacun des ensembles de données imputés. Soit \\(\\widehat{\\theta}_k\\), l’estimé du paramètre \\(\\theta\\) pour l’échantillon \\(k \\in \\{1, \\ldots, K\\}\\) et \\(\\widehat{\\sigma}_k^2=\\mathsf{Va}(\\widehat{\\theta}_k)\\) l’estimé de la variance de \\(\\widehat{\\theta}_k\\) produite par le modèle estimé.\nL’estimation finale de \\(\\theta\\), dénotée \\(\\widehat{\\theta}\\), est obtenue tout simplement en faisant la moyenne des estimations de tous les modèles, c’est-à-dire, \\[\\begin{align*}\n\\widehat{\\theta} = \\frac{\\widehat{\\theta}_1 + \\cdots + \\widehat{\\theta}_K}{K}.\n\\end{align*}\\] Une estimation ajustée de la variance de \\(\\widehat{\\theta}\\) est \\[\\begin{align*}\n\\mathsf{Va}(\\hat{\\theta}) &= W+ \\frac{K+1}{K}B,\n\\\\ W &= \\frac{1}{K} \\sum_{k=1}^K \\widehat{\\sigma}^2_k = \\frac{\\widehat{\\sigma}_1^2 + \\cdots + \\widehat{\\sigma}_K^2}{K},\\\\\nB &= \\frac{1}{K-1} \\sum_{k=1}^K (\\widehat{\\theta}_k - \\widehat{\\theta})^2.\n\\end{align*}\\] Ainsi, le terme \\(W\\) est la moyenne des variances et \\(B\\) est la variance entre les imputations. Le terme \\((1+1/K)B\\) est celui qui vient corriger le fait qu’on travaille avec des données imputées et non pas des vraies données en augmentant la variance estimée du paramètre.\nC’est ici qu’on voit l’intérêt à procéder à de l’imputation multiple. Si on procédait à une seule imputation (même en ajoutant une part d’aléatoire pour essayer de reproduire la variabilité des données), on ne serait pas en mesure d’estimer la variance inter-groupe de l’estimateur. Notez que la formule présentée n’est valide que pour le cas unidimensionnel; l’estimation de la variance dans le cas multidimensionnel est différente (Little and Rubin 2019).\nIl faut également ajuster les formules pour le calcul des intervalles de confiance, valeurs-\\(p\\) et degrés de liberté. Le logiciel s’en chargera pour nous.\nLa méthode d’imputation multiple possède l’avantage d’être applicable avec n’importe quel modèle sous-jacent. Une fois qu’on a des échantillons complets (imputés), on ajuste le modèle comme d’habitude. Mais une observation imputée ne remplacera jamais une vraie observation. Il faut donc faire tout ce qu’on peut pour limiter le plus possible les données manquantes.\nIl faut utiliser son jugement. Par exemple, si la proportion d’observations perdues est petite (moins de 5%), ça ne vaut peut-être pas la peine de prendre des mesures particulières et on peut faire une analyse avec les données complètes seulement. S’il y a un doute, on peut faire une analyse avec les données complètes seulement et une autre avec imputations multiples afin de valider la première.\nSi, à l’inverse, une variable secondaire cause à elle seule une grande proportion de valeurs manquantes, on peut alors considérer l’éliminer afin de récupérer des observations. Par exemple, si vous avez une proportion de 30% de valeurs manquantes en utilisant toutes vos variables et que cette proportion baisse à 3% lorsque vous éliminez quelques variables peu importantes pour votre étude (ou qui peuvent être remplacées par d’autres jouant à peu près le même rôle qui elles sont disponibles), alors vous pourriez considérer la possibilité de les éliminer. Il est donc nécessaire d’examiner la configuration des valeurs manquantes avant de faire quoi que ce soit.\nPour l’imputation, nous utiliserons l’algorithme d’imputation multiple par équations chaînées (MICE).\nAvec \\(p\\) variables \\(X_1, \\ldots, X_p\\), spécifier un ensemble de modèles conditionnels pour chaque variable \\(X_j\\) en fonction de toutes les autres variables, \\(\\boldsymbol{X}_{-j}\\) et les valeurs observées pour cette variable, \\(X_{j, \\text{obs}}\\).\nL’idée est de remplir aléatoire tous les trous et ensuite d’utiliser des modèles d’imputation aléatoire pour chaque variable à tour de rôle. Après plusieurs cycles où chacune des variables explicatives (au plus le nombre de colonnes \\(p\\)) est imputée, l’impact de l’initialisation devrait être faible. On retourne alors une copie de la base de données.\n\nInitialisation: remplir les trous avec des données au hasard parmi \\(X_{j, \\text{obs}}\\) pour \\(X_{j, \\text{man}}\\)\nÀ l’itération \\(t\\), pour chaque variable \\(j=1, \\ldots, p\\), à tour de rôle:\n\ntirage aléatoire des paramètres \\(\\phi_j^{(t)}\\) du modèle pour \\(X_{j,\\text{man}}\\) conditionnel à \\(\\boldsymbol{X}_{-j}^{(t-1)}\\) et \\(X_{j, \\text{obs}}\\)\néchantillonnage de nouvelles observations \\(X^{(t)}_{j,\\text{man}}\\) du modèle avec paramètres \\(\\phi_j^{(t)}\\) conditionnel à \\(\\boldsymbol{X}_{-j}^{(t-1)}\\) et \\(X_{j, \\text{obs}}\\)\n\nRépéter le cycle"
  },
  {
    "objectID": "07-donneesmanquantes.html#example-dapplication-de-limputation",
    "href": "07-donneesmanquantes.html#example-dapplication-de-limputation",
    "title": "8  Données manquantes",
    "section": "8.3 Example d’application de l’imputation",
    "text": "8.3 Example d’application de l’imputation\nOn examine l’exemple de recommandations de l’association professionnelle des vachers de la section Section 5.2.3.\nLe but est d’examiner les effets des variables \\(X_1\\) à \\(X_6\\) sur les intentions d’achat; la base de données manquantes contient les observations. Il s’agit des mêmes données que celles du fichier logit1 mais avec des valeurs manquantes.\n\n\n\n\nTableau 8.1:  Tableau des configurations des données manquantes. \n \n  \n    \\(X_1\\) \n    \\(X_2\\) \n    \\(X_3\\) \n    \\(X_4\\) \n    \\(X_5\\) \n    \\(X_6\\) \n    \\(y\\) \n  \n \n\n  \n    1 \n    4 \n    0 \n    1 \n    35 \n    2 \n    0 \n  \n  \n    . \n    1 \n    0 \n    . \n    33 \n    3 \n    0 \n  \n  \n    2 \n    3 \n    1 \n    . \n    46 \n    3 \n    0 \n  \n  \n    5 \n    2 \n    1 \n    . \n    32 \n    1 \n    1 \n  \n  \n    3 \n    2 \n    1 \n    . \n    38 \n    3 \n    1 \n  \n  \n    . \n    4 \n    0 \n    0 \n    36 \n    3 \n    0 \n  \n  \n    . \n    3 \n    0 \n    . \n    35 \n    3 \n    0 \n  \n  \n    . \n    5 \n    1 \n    0 \n    26 \n    2 \n    0 \n  \n  \n    . \n    3 \n    1 \n    1 \n    39 \n    2 \n    1 \n  \n  \n    5 \n    2 \n    1 \n    . \n    38 \n    3 \n    0 \n  \n\n\n\n\n\n\nLes points (.) indiquent des valeurs manquantes. Le premier sujet n’a pas de valeur manquante. Le deuxième a une valeur manquante pour \\(X_1\\) (emploi) et \\(X_4\\) (éducation), etc.\nUne première façon de voir combien il y a de valeurs manquantes consiste à faire sortir les statistiques descriptives avec summary. Ainsi, il y 192 valeurs manquantes pour \\(X_1\\), 48 pour \\(X_2\\) et 184 pour \\(X_4\\). Les autres variables n’ont pas de valeurs manquantes, incluant la variable dépendante \\(Y\\). La procédure unidimensionnelle nous permet seulement de voir combien il y a de valeurs manquantes variable par variable.\n\n\n\n\nTableau 8.2:  Pourcentage de valeurs manquantes par variable. \n \n  \n      \n    x1 \n    x2 \n    x3 \n    x4 \n    x5 \n    x6 \n    y \n  \n \n\n  \n    nombre \n    192 \n    49 \n    0 \n    184 \n    0 \n    0 \n    0 \n  \n  \n    pourcentage \n    0.384 \n    0.098 \n    0 \n    0.368 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\n\ndata(manquantes, package = 'hecmulti')\nsummary(manquantes)\n# Pourcentage de valeurs manquantes\napply(manquantes, 2, function(x){mean(is.na(x))})\n# Voir les configurations de valeurs manquantes\nmd.pattern(manquantes)\n\nNous utiliserons le paquet R mice pour faire l’imputation.\n\nProcéder à plusieurs imputations aléatoires pour obtenir un échantillon complet (mice)\nAjuster le modèle d’intérêt avec chaque échantillon (with). 3. Combiner les résultats obtenus (pool et summary)\n\n\n\n\n\n\nFigure 8.2: Configurations des valeurs manquantes pour la base de données manquantes.\n\n\n\n\nLa Figure 8.2 donne une indication sur les différentes combinaisons de données complètes (cases bleues) et les observations manquantes (cases roses) avec leur fréquence. Les variables sont indiquées au dessus, les effectifs manquants en dessous, le nombre de cas de chaque combinaisons à gauche et le nombre de variables avec des valeurs manquantes à droite. Ainsi, il y a 180 sujets (36% de l’échantillon) avec aucune observation manquante. Il y en a 99 avec seulement \\(X_4\\) manquante et ainsi de suite. On voit donc, par exemple, que pour 14 sujets, à la fois \\(X_1\\) et \\(X_2\\) sont manquantes.\nLa recommandation d’usage est d’imputer au moins le pourcentage de cas incomplet, ici 64% donc 64 imputations. Si la procédure est trop coûteuse en calcul, on peut diminuer le nombre d’imputations, mais il faut au minimum 10 réplications pour avoir une bonne idée de la variabilité.\nOn peut comparer l’inférence avec toutes les variables explicatives pour les données sans valeurs manquantes (\\(n=500\\) observations), avec les cas complets uniquement (\\(n=180\\) observations). Le Tableau 8.3 présente les estimations des paramètres du modèle de régression logistique s’il n’y avait pas eu de valeurs manquantes, avec les cas complets et les résultats de l’imputation multiple.\nSi on ajuste un modèle à une base de données qui contient des valeurs manquantes, le comportement par défaut est de retirer les observations qui ont au moins une valeur manquante pour une des variables nécessaires à l’analyse (voir la sortie de glm(y ~ ., data = manquantes)). Il ne serait pas raisonnable de faire l’analyse avec seulement 180 observations et de laisser tomber les 320 autres. De plus, comme nous l’avons vu plus haut, ce n’est pas valide à moins que le processus ne soit MCAR. La partie du milieu du Tableau 8.3 présente les estimations obtenues. Plusieurs variables significatives à niveau \\(\\alpha=0.05\\) ne le sont plus (puisqu’il y a moins d’information quand on réduit le nombre d’observations). Il y a même pire: non seulement la variable \\(\\mathsf{I}(X_2=1)\\) est passée de significative à non significative, mais en plus l’estimé de son paramètre a changé de signe.\nNous allons donc faire l’analyse avec l’imputation multiple, en prenant la méthode d’imputation par défaut\n\nlibrary(mice)\n# Imputation multiple avec équations enchaînées\n# Intensif en calcul, réduire `m` si nécessaire\nimpdata <- mice(data = manquantes,\n                m = 50,\n                seed = 2021,\n                method = \"pmm\",\n                printFlag = FALSE)\n# Chaque copie est disponible (1, ..., 50)\ncomplete(impdata, action = 1)\n# ajuste les modèles avec les données imputées\n\nadj_im <- with(data = impdata,\n               expr = glm(y ~ x1 + x2 + x3 + x4 + x5 + x6,\n                          family = binomial(link = 'logit')))\n\n# combinaison des résultats \nfit <- pool(adj_im)\nsummary(fit)\n\nLa procédure mice du paquet éponyme crée les copies complètes du jeu de données. On peut ensuite appliquer une procédure quelconque et combiner les estimations avec pool.\n\n\n\n\nTableau 8.3:  Estimés, erreurs-type et valeurs-p des paramètres avec les 500 données complètes (gauche), avec les 180 cas complets (milieu) et avec l’imputation multiple (droite). \n \n\n\nDonnées complètes\nCas complets\nImputation multiple\n\n  \n      \n    \\(\\widehat{\\boldsymbol{\\beta}}\\) \n    \\(\\mathrm{se}(\\widehat{\\boldsymbol{\\beta}})\\) \n    valeur-\\(p\\) \n    \\(\\widehat{\\boldsymbol{\\beta}}\\) \n    \\(\\mathrm{se}(\\widehat{\\boldsymbol{\\beta}})\\) \n    valeur-\\(p\\) \n    \\(\\widehat{\\boldsymbol{\\beta}}\\) \n    \\(\\mathrm{se}(\\widehat{\\boldsymbol{\\beta}})\\) \n    valeur-\\(p\\) \n  \n \n\n  \n    cste \n    -6.89 \n    1.02 \n    0.00 \n    -5.25 \n    1.70 \n    0.00 \n    -6.57 \n    1.04 \n    0.00 \n  \n  \n    \\(x_1=1\\) \n    0.36 \n    0.48 \n    0.46 \n    -0.09 \n    0.85 \n    0.92 \n    0.55 \n    0.54 \n    0.31 \n  \n  \n    \\(x_1=2\\) \n    -0.47 \n    0.37 \n    0.21 \n    -0.57 \n    0.66 \n    0.39 \n    -0.13 \n    0.45 \n    0.78 \n  \n  \n    \\(x_1=3\\) \n    -0.31 \n    0.35 \n    0.37 \n    -0.47 \n    0.66 \n    0.47 \n    0.07 \n    0.44 \n    0.87 \n  \n  \n    \\(x_1=4\\) \n    -0.32 \n    0.40 \n    0.43 \n    -0.93 \n    0.74 \n    0.21 \n    -0.04 \n    0.48 \n    0.93 \n  \n  \n    \\(x_2=1\\) \n    1.33 \n    0.60 \n    0.03 \n    -0.74 \n    1.14 \n    0.52 \n    1.10 \n    0.65 \n    0.09 \n  \n  \n    \\(x_2=2\\) \n    1.15 \n    0.50 \n    0.02 \n    0.46 \n    0.91 \n    0.61 \n    1.03 \n    0.55 \n    0.06 \n  \n  \n    \\(x_2=3\\) \n    0.77 \n    0.48 \n    0.11 \n    -0.41 \n    0.89 \n    0.64 \n    0.52 \n    0.52 \n    0.31 \n  \n  \n    \\(x_2=4\\) \n    -1.11 \n    0.54 \n    0.04 \n    -2.74 \n    1.02 \n    0.01 \n    -1.04 \n    0.57 \n    0.07 \n  \n  \n    \\(x_3\\) \n    1.35 \n    0.26 \n    0.00 \n    0.80 \n    0.44 \n    0.07 \n    1.19 \n    0.27 \n    0.00 \n  \n  \n    \\(x_4\\) \n    1.83 \n    0.30 \n    0.00 \n    2.25 \n    0.58 \n    0.00 \n    1.52 \n    0.37 \n    0.00 \n  \n  \n    \\(x_5\\) \n    0.11 \n    0.02 \n    0.00 \n    0.11 \n    0.03 \n    0.00 \n    0.10 \n    0.02 \n    0.00 \n  \n  \n    \\(x_6=1\\) \n    2.41 \n    0.38 \n    0.00 \n    2.23 \n    0.66 \n    0.00 \n    2.26 \n    0.38 \n    0.00 \n  \n  \n    \\(x_6=2\\) \n    1.04 \n    0.25 \n    0.00 \n    0.83 \n    0.44 \n    0.06 \n    1.00 \n    0.25 \n    0.00 \n  \n\n\n\n\n\n\nOn peut remarquer que la précision est systématiquement meilleure avec l’imputation multiple; les erreurs-type pour l’imputation multiple sont plus petits que celle du modèle qui retire les données incomplètes.\nOn voit que la variable \\(X_3\\) (sexe) est significative avec l’imputation multiple. Son paramètre estimé est 1.19, comparativement à 1.349 s’il n’y avait pas eu de valeurs manquantes. La précision dans l’estimation avec l’imputation multiple est seulement un peu moins bonne (erreur-type de 0.27) que celle s’il n’y avait pas eu de manquantes (erreur type de 0.26). Le paramètre de \\(\\mathsf{I}(X_6=2)\\) redevient aussi significatif, alors qu’il ne l’était plus si on retirait les manquantes. Il est peu probable que les données soit \\(\\mathsf{MCAR}\\) et donc les résultats de l’analyse des cas complets seraient biaisés.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nLes données manquantes réduisent la quantité d’information disponible et augmentent l’incertitude.\nOn ne peut pas les ignorer (étude des cas complets) sans biaiser les interprétations et réduire la quantité d’information disponible.\nPour bien capturer l’incertitude et ne pas modifier les relations entre variables, il faut utiliser une méthode d’imputation aléatoire.\nAvec l’algorithme MICE, on utilise un modèle conditionnel pour chaque variable à tour de rôle.\nL’imputation multiple est préférée à l’imputation simple car elle permet d’estimer l’incertitude sous-jacente en raison des données manquantes.\nIl faut un traitement spécial pour les erreurs-type, degrés de liberté, valeurs-\\(p\\) et intervalles de confiance.\n\n\n\n\n\n\n\nLittle, Roderick J. A., and Donald B Rubin. 2019. “Statistical Analysis with Missing Data.” Edited by Wiley. https://doi.org/10.1002/9781119482260."
  }
]