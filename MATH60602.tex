\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Analyse multidimensionnelle appliquée},
            pdfauthor={Denis Larocque, Léo Belzile},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

% \usepackage[mathscr]{eucal}
\DeclareMathAlphabet{\mathcrl}{U}{rsfs}{m}{n}

% \DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage{booktabs}
\usepackage[left=1.2in, right= 1.2in]{geometry}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

% \AtBeginDocument{

% \geometry{left=1.2in, right= 1.2in}
\usepackage{fourier}
\usepackage[french]{babel}
\DecimalMathComma
% }
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Analyse multidimensionnelle appliquée}
\author{Denis Larocque, Léo Belzile}
\date{Version du 2020-03-13}

\usepackage{amsthm}
\newtheorem{theorem}{Théorème}[chapter]
\newtheorem{lemma}{Lemme}[chapter]
\newtheorem{corollary}{Corollaire}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Définition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Exemple}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercice}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remarque}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\eps}{\varepsilon}
\newcommand{\Rlang}{\textsf{R}}
\newcommand{\SAS}{\textsf{SAS}}
\newcommand{\Sp}{\mathscr{S}}
\renewcommand{\P}[1]{{\mathsf P}\left(#1\right)}
\newcommand{\E}[1]{{\mathsf E}\left(#1\right)}
\newcommand{\Va}[1]{{\mathsf{Var}}\left(#1\right)}
\newcommand{\Cor}[1]{{\mathsf{Cor}}\left(#1\right)}
\newcommand{\I}[1]{{\mathbf 1}_{#1}}
\newcommand{\expit}{\mathrm{expit}}
\newcommand{\logit}{\mathrm{logit}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\Hy}{\mathcal{H}}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{survol-du-cours}{%
\section{Survol du cours}\label{survol-du-cours}}

\hypertarget{analyse-factorielle-exploratoire}{%
\subsection{Analyse factorielle exploratoire}\label{analyse-factorielle-exploratoire}}

On dispose de \(p\) variables \(X_1, \ldots, X_p\). Peut-on expliquer les interrelations (la structure de corrélation) entre ces variables à l'aide d'un certain nombre (moins de \(p\)) de facteurs latents (non observés)?

L'analyse factorielle est souvent utilisée pour analyser des questionnaires (construction d'échelles) comme dans l'exemple suivant.

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-1}{}{\label{exm:unnamed-chunk-1} }Pour les besoins d'une enquête, on a demandé à 200 consommateurs adultes de répondre aux questions suivantes par rapport à un certain type de magasin:

Sur une échelle de 1 à 5,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  pas important
\item
  peu important
\item
  moyennement important
\item
  assez important
\item
  très important
\end{enumerate}

Pour vous, à quel point est-ce important\ldots

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  que le magasin offre de bons prix tous les jours?
\item
  que le magasin accepte les cartes de crédit majeures (Visa, Mastercard)?
\item
  que le magasin offre des produits de qualité?
\item
  que les vendeurs connaissent bien les produits?
\item
  qu'il y ait des ventes spéciales régulièrement?
\item
  que les marques connues soient disponibles?
\item
  que le magasin ait sa propre carte de crédit?
\item
  que le service soit rapide?
\item
  qu'il y ait une vaste sélection de produits?
\item
  que le magasin accepte le paiement par carte de débit?
\item
  que le personnel soit courtois?
\item
  que le magasin ait en stock les produits annoncés?
\end{enumerate}

Pouvons-nous identifier un nombre restreint de facteurs (concepts, dimensions) qui pourraient bien rendre compte de la structure de corrélation entre ces 12 variables?
\EndKnitrBlock{example}

Buts:

\begin{itemize}
\tightlist
\item
  Décrire et comprendre la structure de corrélation d'un ensemble de variables à l'aide d'un nombre restreint de concepts (appelés facteurs).
\item
  Réduire le nombre de variables en créant une nouvelle variable par facteur. Ces nouvelles variables pourront par la suite être utilisées dans d'autres analyses (régression linéaire multiple par exemple).
\end{itemize}

\hypertarget{suxe9lection-de-variables-et-de-moduxe8les}{%
\subsection{Sélection de variables et de modèles}\label{suxe9lection-de-variables-et-de-moduxe8les}}

Dans plusieurs situations, on doit développer un modèle de prévision. Par exemple, on pourrait devoir développer un modèle pour:

\begin{itemize}
\tightlist
\item
  Détecter les faillites des clients (ou des entreprises)
\item
  Cibler les clients qui seront intéressés par une offre promotionnelle
\item
  Détecter les fraudes (par carte de crédit ou dans les rapports de revenus)
\item
  Prévoir si un client va nous quitter.
\end{itemize}

Il y a en général plusieurs variables explicatives potentielles, et aussi plusieurs types de modèles possibles (régression linéaire, réseaux de neurones, arbres de régression ou de classification, etc.). Dans ce chapitre, nous verrons des principes généraux et des outils afin de sélectionner des modèles performants, ou bien un sous-ensemble de variables avec un bon pouvoir prévisionnel.

\hypertarget{ruxe9gression-logistique}{%
\subsection{Régression logistique}\label{ruxe9gression-logistique}}

On cherche à expliquer le comportement d'une variable binaire \(Y\) (\(0-1\)), à l'aide de \(p\) variables quelconques \(X_1, \ldots, X_p\).

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-2}{}{\label{exm:unnamed-chunk-2} }Une banque offre aux gens la possibilité de faire une demande de carte de crédit en ligne en promettant une approbation (conditionnelle) en quelques minutes seulement. Le tout est basé sur un modèle automatique de classification qui décide d'accorder ou non la carte (\(Y=1\) ou \(Y=0\)) en fonction des réponses fournies par les clients potentiels à différentes questions comme: quel est votre revenu annuel brut (\(X_1\)), avez-vous d'autres cartes de crédit (\(X_2\)), êtes-vous locataire ou propriétaire (\(X_3\)), etc\ldots
\EndKnitrBlock{example}

Buts:

\begin{itemize}
\tightlist
\item
  Comprendre comment et dans quelle mesure les variables \(\boldsymbol{X}\) influencent la catégorie d'appartenance de \(Y\).
\item
  Développer un modèle pour faire de la classification, c'est-à-dire, prévoir la catégorie d'appartenance de \(Y\) pour un nouveau sujet à partir des variables \(\boldsymbol{X}\).
\end{itemize}

\hypertarget{analyse-de-regroupements}{%
\subsection{Analyse de regroupements}\label{analyse-de-regroupements}}

On cherche à créer des groupes (« \emph{clusters} ») d'individus homogènes en utilisant \(p\) variables \(X_1, \ldots, X_p\).

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-3}{}{\label{exm:unnamed-chunk-3} }
Cette méthode est utilisée en marketing pour la \textbf{segmentation de marché}, qui consiste en

\begin{quote}
\ldots définir des sous-groupes réunissant des consommateurs qui partagent les mêmes préférences ou qui réagissent de façon semblable à des variables de marketing\footnote{d'Astous, A. (2000). \emph{Le projet de recherche en marketing}, 2e édition. Chenelière/McGraw-Hill. }
\end{quote}
\EndKnitrBlock{example}

But:

\begin{itemize}
\tightlist
\item
  Combiner des sujets en groupes (interprétables) de telle sorte que les individus d'un même groupe soient les plus semblables possible par rapport à certaines caractéristiques et que les groupes soient les plus différents possible.
\end{itemize}

\hypertarget{analyse-de-survie}{%
\subsection{Analyse de survie}\label{analyse-de-survie}}

On s'intéresse au temps avant qu'un événement survienne. Par exemple :

\begin{itemize}
\tightlist
\item
  Temps qu'un client demeure abonné à un service offert par notre compagnie.
\item
  Temps de survie d'un individu après avoir été diagnostiqué avec un certain type de cancer.
\item
  Temps qu'un employé demeure au service de la compagnie.
\item
  Temps qu'une franchise demeure en activité.
\item
  Temps avant la faillite d'une entreprise (ou d'un particulier).
\item
  Temps avant le prochain achat d'un client.
\end{itemize}

On observe chaque sujet jusqu'à ce que l'une des deux choses suivantes se produise: l'événement survient avant la fin de la période d'observation ou bien l'étude se termine et l'événement n'est toujours pas survenu. Dans le premier exemple, l'événement correspond au fait d'interrompre son abonnement. On dispose donc d'une variable temps \(T\) pour chaque individu qui est soit censurée, soit non censurée. Si l'individu a expérimenté l'événement avant la fin de la période d'observation, la valeur de est non censurée. Si l'événement n'est toujours pas survenu à la fin de la période d'observation, la valeur de \(T\) est censurée. Pour chaque individu, on dispose également d'un ensemble de variables explicatives \(X_1, \ldots, X_p\).

But:

\begin{itemize}
\tightlist
\item
  Étudier les effets des variables explicatives sur le temps de survie et obtenir des prévisions du temps de survie.
\end{itemize}

\hypertarget{donnuxe9es-manquantes}{%
\subsection{Données manquantes}\label{donnuxe9es-manquantes}}

Il arrive fréquemment d'avoir des valeurs manquantes dans notre échantillon.

Simplement ignorer les sujets avec des valeurs manquantes et faire l'analyse avec les autres sujets conduit généralement à des estimations biaisées et à de l'inférence invalide.

Dans ce chapitre, nous verrons une méthode très générale afin de traiter les données manquantes, l'imputation multiple. Nous verrons comment elle peut être utilisée dans un contexte d'inférence et dans un contexte de prévision.

\hypertarget{analyse-factorielle-exploratoire-1}{%
\chapter{Analyse factorielle exploratoire}\label{analyse-factorielle-exploratoire-1}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

On dispose de \(p\) variables \(X_1, \ldots, X_p\).

\begin{itemize}
\tightlist
\item
  Y a-t-il des groupements de variables?
\item
  Est-ce que les variables faisant partie d'un groupement semblent mesurer certains aspects d'un facteur commun (non observé)?
\end{itemize}

Un tel groupement peut être détecté si plusieurs variables sont très corrélées entre elles. Est-ce que la structure de corrélation entre les \(p\) variables peut être expliquée à l'aide d'un nombre restreint de facteurs?

\emph{Exemple de facteurs}: Habileté quantitative, habileté sociale, importance accordée à la qualité du service, importance accordée à la loyauté, habileté de leader, etc\ldots

L'analyse factorielle est aussi une méthode de réduction du nombre de variables. En effet, une fois qu'on a identifié les facteurs, on peut remplacer les variables individuelles par un résumé pour chaque facteur (qui est souvent la moyenne des variables qui font partie du facteur).

Pour faire une analyse factorielle, la taille d'échantillon devrait être d'au moins 10 fois le nombre de variables.

\hypertarget{rappels-sur-le-coefficient-de-corruxe9lation-linuxe9aire}{%
\section{Rappels sur le coefficient de corrélation linéaire}\label{rappels-sur-le-coefficient-de-corruxe9lation-linuxe9aire}}

On veut examiner la relation entre deux variables \(X_j\) et \(X_k\) et on dispose de \(n\) couples d'observations, où \(x_{i, j}\) (respectivement \(x_{i, k}\)) est la valeur de la variable \(X_j\) (\(X_k\)) pour le \(i\)e individu.

Le coefficient de corrélation linéaire entre \(X_j\) et \(X_k\), que l'on note \(r_{j, k}\), cherche à mesurer la force de la relation linéaire entre deux variables, c'est-à-dire à quantifier à quel point les observations sont alignées autour d'une droite. Le coefficient de corrélation est
\begin{align*}
r_{j, k} = \frac{\sum_{i=1}^n (x_{i, j}-\overline{x}_j)(x_{i, k} -\overline{x}_{k})}{\left\{\sum_{i=1}^n (x_{i, j}-\overline{x}_j)^2 \sum_{i=1}^n(x_{i, k} -\overline{x}_{k})^2\right\}^{1/2}}
\end{align*}

Les propriétés les plus importantes du coefficient de corrélation linéaire \(r\) sont les suivantes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(-1 \leq r \leq 1\);
\item
  \(r=1\) (respectivement \(r=-1\)) si et seulement si les \(n\) observations sont exactement alignées sur une droite de pente positive (négative). C'est-à-dire, s'il existe deux constantes \(a\) et \(b>0\) (\(b<0\)) telles que \(y_i=a+b x_i\) pour tout \(i=1, \ldots, n\).
\end{enumerate}

Règle générale,

\begin{itemize}
\tightlist
\item
  Plus la corrélation est près de 1, plus les points auront tendance à être alignés autour d'une droite de pente positive. Par conséquent, plus la valeur de \(X\) augmente, plus celle de \(Y\) aura tendance à augmenter et vice-versa.
\item
  Plus la corrélation est près de \(-1\), plus les points auront tendance à être alignés autour d'une droite de pente négative. Par conséquent, plus la valeur de \(X\) augmente, plus celle de \(Y\) aura tendance à diminuer et vice-versa.
\item
  Lorsque la corrélation est presque nulle, les points n'auront pas tendance à être alignés autour d'une droite. Il est très important de noter que cela n'implique pas qu'il n'y a pas de relation entre les deux variables. Cela implique seulement qu'il n'y a pas de \textbf{relation linéaire} entre les deux variables.
\end{itemize}

\hypertarget{exemple-de-questionnaire}{%
\section{Exemple de questionnaire}\label{exemple-de-questionnaire}}

Le questionnaire suivant porte sur une étude dans un magasin. Pour les besoins d'une enquête, on a demandé à 200 consommateurs adultes de répondre aux questions suivantes par rapport à un certain type de magasin sur une échelle de 1 à 5, où

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  pas important
\item
  peu important
\item
  moyennement important
\item
  assez important
\item
  très important
\end{enumerate}

Pour vous, à quel point est-ce important\ldots

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  que le magasin offre de bons prix tous les jours?
\item
  que le magasin accepte les cartes de crédit majeures (Visa, Mastercard)?
\item
  que le magasin offre des produits de qualité?
\item
  que les vendeurs connaissent bien les produits?
\item
  qu'il y ait des ventes spéciales régulièrement?
\item
  que les marques connues soient disponibles?
\item
  que le magasin ait sa propre carte de crédit?
\item
  que le service soit rapide?
\item
  qu'il y ait une vaste sélection de produits?
\item
  que le magasin accepte le paiement par carte de débit?
\item
  que le personnel soit courtois?
\item
  que le magasin ait en stock les produits annoncés?
\end{enumerate}

Une analyse factorielle cherchera à identifier automatiquement des groupes de variables qui sont fortement corrélées entre elles.

Les commandes \textbf{SAS} (ainsi que plusieurs commentaires) pour faire les analyses se trouvent dans le fichier \texttt{MATH60602\_cours3.sas}. Les statistiques descriptives ainsi que la matrice des corrélations sont obtenues en exécutant les lignes suivantes:

\begin{verbatim}
proc corr data=multi.factor2;
var x1-x12;
run;
\end{verbatim}

\begin{center}\includegraphics[width=0.9\linewidth]{figures/01-facto-e1} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{figures/01-facto-e2} \end{center}

\hypertarget{description-du-moduxe8le-danalyse-factorielle}{%
\section{Description du modèle d'analyse factorielle}\label{description-du-moduxe8le-danalyse-factorielle}}

On dispose d'observations sur \(p\) variables \(X_1, \ldots, X_p\). Le modèle d'analyse factorielle fait l'hypothèse que ces variables dépendent linéairement d'un plus petit nombre \(m\) de variables aléatoires, \(F_1, \ldots, F_m\), appelées facteurs communs et de \(p\) termes d'erreurs (ou facteurs spécifiques) \(\varepsilon_1, \ldots, \varepsilon_p\), de moyenne \({\mathsf E}\left(\varepsilon_i\right)=0\) et de variance \({\mathsf{Var}}\left(\varepsilon_i\right)=\psi_i\) pour \(i=1, \ldots, p\). Spécifiquement, le modèle est
\begin{align*}
X_1 &= \mu_1 + l_{11}F_1 + l_{12} F_2 + \cdots + l_{1m}F_m + \varepsilon_1\\
X_2 &= \mu_2 + l_{21}F_1 + l_{22} F_2 + \cdots + l_{2m}F_m + \varepsilon_2\\
&\vdots \\
X_p &= \mu_p + l_{p1}F_1 + l_{p2} F_2 + \cdots + l_{pm}F_m + \varepsilon_p, 
\end{align*}
où \(\mu_i\) est l'espérance de la variable aléatoire \(X_i\) (\(i=1, \ldots, p\)) et où \(l_{ij}\) est le chargement de la variable \(X_i\) sur le facteur \(F_j\) (\(i=1, \ldots, p\); \(j=1, \ldots, m\)).

Les espérances (\(\mu_i\)), les chargements (\(l_{ij}\)) et les variances (\(\psi_i\)) sont des quantités fixes, mais inconnues, tandis que les facteurs communs (\(F_i\)) et spécifiques (\(\varepsilon_i\)) sont des variables aléatoires non observables que l'on assume non corrélée aux facteurs \(F\) et entre elles.

Des hypothèses supplémentaires sont nécessaires afin de pouvoir utiliser ce modèle (contraintes d'identifiabilité des paramètres). Sans entrer dans les détails, mentionnons que l'une de ces hypothèses est que les facteurs sont non corrélés.

De plus, si les variables ont été préalablement standardisées de telle sorte que \({\mathsf E}\left(X_i\right)=0\) et \({\mathsf{Var}}\left(X_i\right)=1\) (note: ceci revient à utiliser la matrice de corrélation des observations dans l'analyse ce qui est fait par défaut dans \textbf{SAS}), alors \({\mathsf{Cor}}\left(X_i, F_j\right)=l_{ij}\), c'est-à-dire, le chargement de la variable \(X_i\) sur le chargement \(F_j\) est le coefficient de corrélation entre cette variable et ce facteur.

Sans aucune contrainte sur le modèle, la matrice de covariance de \(X_1, \ldots, X_p\) possède \(p(p+1)/2\) paramètres, soit \(p\) variances et \(p(p-1)/2\) termes de corrélation. Avec le modèle d'analyse factorielle, on suppose que l'on peut décrire cette structure en utilisant seulement \(p(m+1)\) paramètres (\(p\) variances spécifiques et \(pm\) chargements). Par exemple, avec \(p=50\) variables et \(m=6\) facteurs, on essaie de décrire la structure de covariance à l'aide de 350 paramètres au lieu de 1275.

Il existe plusieurs méthodes pour extraire les facteurs, c'est-à-dire pour estimer les paramètres du modèle (les \(\psi_i\) et les \(l_{ij}\)). Nous allons discuter de deux d'entre elles: la méthode du maximum de vraisemblance et la méthode des composantes principales. L'avantage de l'estimation par maximum de vraisemblance est qu'elle permet l'utilisation de critères d'information et de statistiques de tests pour guider le choix du nombre de facteurs. En revanche, l'estimation des paramètres requiert une optimisation numérique qui peut être délicate selon les cas de figure.

\hypertarget{rotation-des-facteurs}{%
\subsection{Rotation des facteurs}\label{rotation-des-facteurs}}

Dans le modèle d'analyse factorielle, on peut montrer que, lorsqu'il y a deux facteurs ou plus, il existe plusieurs configurations de facteurs qui donnent la même structure de covariance. En fait, les chargements peuvent seulement être déterminés à une transformation orthogonale prêt (note: une transformation orthogonale est une transformation qui préserve le produit scalaire; elle préserve ainsi toutes les distances et les angles entre deux vecteurs). Si les chargements provenant d'une méthode d'extraction des facteurs ne sont pas uniques, la matrice de corrélation estimée par le modèle est par contre unique.

Il existe plusieurs techniques de rotation de facteurs. Le but de ces techniques est d'essayer de trouver une solution qui fera en sorte que les facteurs seront facilement interprétables. La méthode la plus utilisée est la méthode \textbf{varimax}: elle produit une configuration de chargement en maximisant la variance de la somme des carrés des chargements pour les \(m\) facteurs. La méthode varimax tend à produire une configuration de facteurs tel que les chargements de chaque variable sont dispersés (des chargements élevés positifs ou négatifs et d'autres presque nuls).

Je vous suggère de toujours tenter d'interpréter la solution avec une rotation varimax. Si ce n'est pas suffisamment clair, il existe d'autres méthodes de rotation dont certaines (les rotations de type oblique) permettent la présence de corrélation entre les facteurs.

\hypertarget{estimation-des-facteurs}{%
\section{Estimation des facteurs}\label{estimation-des-facteurs}}

Les chargements estimés pour la solution à quatre facteurs, suite à la rotation varimax, sont obtenus avec le code SAS suivant:

\begin{verbatim}
proc factor data=multi.factor2 
 method=ml rotate=varimax nfact=4
 maxiter=500 flag=.3 hey;
 var x1-x12;
run;
\end{verbatim}

\begin{center}\includegraphics[width=0.7\linewidth]{figures/01-facto-e3} \end{center}

En général, on associe une variable à un groupe (facteur) si son chargement est supérieur à 0, 3 (en valeur absolue), ce qui donne

\begin{itemize}
\tightlist
\item
  Facteur 1: \(X_4\), \(X_8\) et \(X_{11}\)
\item
  Facteur 2: \(X_3\), \(X_6\), \(X_9\) et \(X_{12}\)
\item
  Facteur 3: \(X_2\), \(X_7\) et \(X_{10}\)
\item
  Facteur 4: \(X_1\) et \(X_5\).
\end{itemize}

Ces facteurs sont interprétables:

\begin{itemize}
\tightlist
\item
  Le facteur 1 représente l'importance accordée au service.
\item
  Le facteur 2 représente l'importance accordée aux produits.
\item
  Le facteur 3 représente l'importance accordée à la facilité de paiement.
\item
  Le facteur 4 représente l'importance accordée aux prix.
\end{itemize}

Dans cet exemple, les choses se sont bien passées et le nombre de facteurs que nous avons spécifié (4) semble être adéquat, mais ce n'est pas toujours aussi évident. Il est utile d'avoir des outils pour guider le choix du nombre de facteurs.

\hypertarget{choix-du-nombre-de-facteurs}{%
\section{Choix du nombre de facteurs}\label{choix-du-nombre-de-facteurs}}

Il existe différentes méthodes pour se guider dans le nombre de facteurs, \(m\), à utiliser. Cependant, le point important à retenir est que, peu importe le nombre choisi, il faut que les facteurs soient \textbf{interprétables}. Par conséquent, les méthodes qui
suivent ne devraient servir que de guide et non pas être suivies aveuglément.
La méthode du maximum de vraisemblance que nous avons utilisée dans l'exemple possède l'avantage de fournir trois critères pour choisir le nombre de facteurs appropriés. Ces critères sont:

\begin{itemize}
\tightlist
\item
  AIC (critère d'information d'Akaike)
\item
  BIC (critère d'information bayésien de Schwarz)
\item
  Le test du rapport de vraisemblance pour l'hypothèse nulle que le modèle de corrélation décrit le modèle factoriel avec \(m\) facteurs est adéquat, contre l'alternative qu'il n'est pas adéquat.
\end{itemize}

Les critères d'information servent à la sélection de modèles; ils seront traités plus en détail dans les chapitres qui suivent. Pour l'instant, il est suffisant de savoir que le modèle avec la valeur du critère AIC (ou BIC) la plus petite est considéré le « meilleur » (selon ce critère).

Les sorties suivantes proviennent du même programme SAS et correspondent au modèle factoriel avec quatre facteurs estimé par maximum de vraisemblance.

\begin{center}\includegraphics[width=0.7\linewidth]{figures/01-facto-e4} \end{center}

Pour choisir le nombre de facteurs avec les critères d'information, il faut ajuster le modèle en faisant varier le nombre de facteurs (option \texttt{nfact}) et extraire la valeur numérique correspondante.

Le tableau \ref{tab:ICtable} présente les valeurs estimées des critères d'information et des valeurs-\(p\) pour le test du rapport de vraisemblance pour cinq modèles. Le critère AIC suggère quatre facteurs, tandis que les deux autres critères (BIC et test du rapport de vraisemblance suggèrent plutôt trois facteurs.

\begin{longtable}[]{@{}crrr@{}}
\caption{\label{tab:ICtable} Critères d'information et valeurs-\emph{p} pour le modèle factoriel à \emph{m} facteurs}\tabularnewline
\toprule
m & AIC & BIC & valeur-\emph{p}\tabularnewline
\midrule
\endfirsthead
\toprule
m & AIC & BIC & valeur-\emph{p}\tabularnewline
\midrule
\endhead
1 & 228, 0 & 49, 9 & \textless{}0, 001\tabularnewline
2 & 99, 5 & -42, 3 & \textless{}0, 001\tabularnewline
3 & -20, 5 & -129, 3 & 0, 096\tabularnewline
4 & -34, 9 & -114, 1 & 0, 973\tabularnewline
5 & -24, 8 & -77, 6 & 0, 975\tabularnewline
\bottomrule
\end{longtable}

On peut considérer le modèle avec trois facteurs: les chargements (après rotation varimax) sont données dans la figure \ref{fig:fig1p5}.

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{figures/01-facto-e5} 

}

\caption{Estimés des chargements pour trois facteurs avec rotation varimax}\label{fig:fig1p5}
\end{figure}

Cette solution récupère les trois facteurs \emph{service}, \emph{produits} et \emph{paiement} de la solution précédente à quatre facteurs. Le facteur \emph{prix} (qui était formé de \(X_1\) et \(X_5\)) n'est plus présent: que faire avec ce dernier? Cela dépend du but de l'analyse et nous y reviendrons plus tard.

Pour terminer cette section, voici la description de deux autres
critères \emph{classiques} pour choisir le nombre de facteurs. Ces deux critères sont:

\begin{itemize}
\tightlist
\item
  Critère de Kaiser, un critère basé sur les valeurs propres. Avec une analyse en composantes principales basée sur la matrice des corrélations, la valeur propre associée à un facteur représente la partie de la variance totale qui est expliquée par ce facteur. Chaque variable compte pour un dans la variance totale. Le nombre de facteurs choisis est le nombre de valeurs propres supérieures à 1. L'idée est de garder seulement les facteurs qui expliquent plus de variance qu'une variable individuelle.
\item
  le diagramme d'éboulis: un graphique des valeurs propres ordonnées de la plus grande à la plus petite en fonction de \(1, \ldots, p\). Habituellement, ce graphe prendra la forme d'une chute assez importante suivie d'une stabilisation des valeurs propres. Avec ce critère, le nombre de facteurs est déterminé par le nombre de valeurs propres avant le début du coude où il a stabilisation apparente. L'idée est de choisir l'endroit où l'ajout d'un facteur supplémentaire n'apporte qu'un gain marginal faible. Ce critère est par contre subjectif et dépend de l'analyste. En ajoutant \texttt{scree} comme option à \texttt{proc\ factor}, on obtient le diagramme d'éboulis mais il est facile de le créer manuellement et le résultat est esthétiquement plus réussi.
\end{itemize}

Les sorties qui suivent proviennent du programme:

\begin{verbatim}
proc factor data=multi.factor2 method=principal
 scree rotate=varimax flag=.3;
 ods output Eigenvalues=eigen;
 var x1-x12;
run; 

proc sgplot data=eigen;
 scatter x=number y=eigenvalue;
 yaxis label="valeurs propres";
 xaxis label='nombre';
run;
\end{verbatim}

Cette fois-ci, c'est la méthode des composantes principales qui est utilisée; cette dernière consiste à estimer les chargements en utilisant les \(m\) premières valeurs propres et vecteurs propres de la matrice de corrélation. En ne spécifiant pas l'option \texttt{nfact}, \textbf{SAS} choisit le nombre de facteurs en utilisant par défaut le critère de Kaiser (valeurs propres supérieures à 1). Quatre facteurs sont retenus, tel qu'indiqué par la sortie au bas du tableau \ref{fig:fig1p7}. Pour le diagramme d'éboulis de la figure \ref{fig:fig1p6}, le choix est assez subjectif: il semble raisonnable de choisir trois ou quatre facteurs.

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{figures/01-facto-e7} 

}

\caption{Valeurs propres et proportion de variance}\label{fig:fig1p7}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{figures/01-facto-e6} 

}

\caption{Diagramme d'éboulis}\label{fig:fig1p6}
\end{figure}

On suggère d'utiliser \emph{de facto} les trois critères découlant de l'utilisation de la vraisemblance et de déterminer le nombre de facteurs à extraire selon différents critères avant d'examiner les modèles avec ce nombre de facteurs et ceux
avec un facteur de moins ou de plus. Au final, le plus important est de pouvoir interpréter raisonnablement les facteurs et donc le modèle retenu est souvent choisi selon le critère \textbf{Wow!}. On veut dire par là que la configuration de facteurs choisie est compréhensible.

\hypertarget{construction-duxe9chelles-uxe0-partir-des-facteurs}{%
\section{Construction d'échelles à partir des facteurs}\label{construction-duxe9chelles-uxe0-partir-des-facteurs}}

Si le seul but de l'analyse factorielle est de comprendre la structure de corrélation entre les variables, alors se limiter à l'interprétation des facteurs est suffisant.

Si par contre, le but est de réduire le nombre de variables pour pouvoir par la suite procéder à d'autres analyses statistiques, l'analyse factorielle peut alors servir de guide pour construire de nouvelles variables (échelles). En supposant que l'analyse factorielle a produit des facteurs qui sont interprétables et satisfaisants, la méthode de construction d'échelles la plus couramment utilisée consiste à construire \(m\) nouvelles variables, une par facteur. Pour un facteur donné, la nouvelle variable est simplement la moyenne des variables ayant des chargements élevés sur ce facteur (positifs ou négatifs, mais de même signe). Une autre méthode, les scores factoriels, sera présentée plus loin.

Lorsqu'on construit une échelle, il est important d'examiner sa cohérence interne. Ceci peut être fait à l'aide du coefficient alpha de Cronbach. Ce coefficient mesure à quel point chaque variable faisant partie d'une échelle est corrélée avec le total de toutes les variables pour cette échelle.
Plus le coefficient est élevé, plus les variables ont tendance à être corrélées entre elles. L'alpha de Cronbach est
\begin{align*}
\alpha=\frac{k}{k-1} \frac{S^2-\sum_{i=1}^k S_i^k}{S^2}, 
\end{align*}
où \(k\) est le nombre de variables dans l'échelle, \(S^2\) est la variance empirique de la somme des variables et \(S_i^2\) est la variance empirique de la \(i\)e variable. En pratique, on voudra que ce coefficient soit au moins égal à 0, 6 pour être satisfait de la cohérence interne de l'échelle.

Avec \textbf{SAS}, la procédure \texttt{corr} permet de calculer \(\alpha\).

\begin{verbatim}
/* pour le facteur service */
proc corr data=multi.factor2 alpha;
var x4 x8 x11;
run;
/* pour le facteur produits */
proc corr data=multi.factor2 alpha;
var x3 x6 x9 x12;
run;
/* pour le facteur paiement */
proc corr data=multi.factor2 alpha;
var x2 x7 x10;
run;
/* pour le facteur prix */
proc corr data=multi.factor2 alpha;
var x1 x5;
run;
\end{verbatim}









\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{figures/01-facto-e8} 

}

\caption{Alpha de Cronbach pour le facteur \emph{service}.}\label{fig:fig1p8}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{figures/01-facto-e9} 

}

\caption{Alpha de Cronbach pour le facteur \emph{produits}.}\label{fig:fig1p9}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{figures/01-facto-e10} 

}

\caption{Alpha de Cronbach pour le facteur \emph{paiement}.}\label{fig:fig1p10}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{figures/01-facto-e11} 

}

\caption{Alpha de Cronbach pour le facteur \emph{prix}.}\label{fig:fig1p11}
\end{figure}

Il faut utiliser le alpha brut. Ainsi, les alphas de Cronbach sont tous
satisfaisants (plus grand que 0, 6) sauf pour le facteur \emph{prix} (\(\alpha=0, 546\)). \textbf{SAS} fournit également la matrice des corrélations des variables de l'échelle ainsi que la valeur du alpha de Cronbach si on retirait une variable à la fois de l'échelle. Tout est donc cohérent. Les échelles provenant des facteurs \emph{service}, \emph{produits} et \emph{paiement}, sont satisfaisantes. Ces facteurs sont identifiés à la fois dans la solution à quatre, mais aussi dans la solution à troisfacteurs. Le facteur \emph{prix} est celui qui apparaît en plus dans la solution à quatre facteurs. Il a une interprétation claire, mais son faible alpha ferait en sorte qu'il serait discutable de travailler avec l'échelle \emph{prix} dans d'autres analyses (du moins avec selon l'usage habituel du alpha).

\hypertarget{compluxe9ments-dinformation}{%
\section{Compléments d'information}\label{compluxe9ments-dinformation}}

\hypertarget{variables-ordinales}{%
\subsection{Variables ordinales}\label{variables-ordinales}}

Théoriquement, une analyse factorielle ne devrait être faite qu'avec des
variables continues. Par contre, en pratique, on l'utilise souvent aussi avec des variables ordinales (comme pour l'exemple portant sur le questionnaire) et même avec des variables binaires (0-1).

Dans ce genre de situation, on peut aussi utiliser d'autres mesures d'associations au lieu du coefficient de corrélation linéaire. Par exemple, on peut utiliser la corrélation polychorique, qui est une mesure de corrélation entre deux variables ordinales. La corrélation tétrachorique correspond au cas spécial de deux variables binaires.

Ma suggestion est d'utiliser la corrélation linéaire ordinaire avec des variables ordinales (même binaires). Si les résultats ne sont pas satisfaisants, on peut alors essayer avec d'autres mesures d'associations.

On peut refaire l'analyse des données portant sur le magasin dans \textbf{SAS} en utilisant la corrélation polychorique calculées par la procédure \texttt{corr} et en passant la sortie à la procédure \texttt{factor}.

\begin{verbatim}
proc corr data=multi.factor2 polychoric out=poly_corr;
var x1-x12;
run;

proc factor data=poly_corr
 method=ml rotate=varimax nfact=4
 maxiter=500 flag=.3 hey;
 var x1-x12;
run;
\end{verbatim}

Les chargements sont donnés dans la figure \ref{fig:fig1p12}. Les facteurs obtenus sont les mêmes qu'en utilisant les corrélations linéaires.

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{figures/01-facto-e12} 

}

\caption{Chargements estimés pour la corrélation polychorique}\label{fig:fig1p12}
\end{figure}

\hypertarget{autres-muxe9thodes-dextractions-de-facteurs}{%
\subsection{Autres méthodes d'extractions de facteurs}\label{autres-muxe9thodes-dextractions-de-facteurs}}

Il n'y a pas de formule explicites pour l'estimation des paramètres avec la méthode du maximum de vraisemblance et un algorithme d'optimisation est nécessaire pour l'option des paramètres. Dans certains cas, l'algorithme peut terminer sans solution ou retourner un cas limite (où la variance est négative ou nulle). C'est le cas dans notre exemple avec quatre facteurs (solution de Heywood), bien que ce ne soit pas indiqué. La sortie \textbf{SAS} contient des informations sur la convergence de l'estimé: idéalement, on obtient la mention \texttt{Critère\ de\ convergence\ respecté}; autrement, essayez de varier le nombre. Un autre signe que l'algorithme n'a pas convergé est la présence de degrés de libertés négatifs pour le test du rapport de vraisemblance.

La méthode par les composantes principales (mentionnée lors de la présentation des valeurs propres et du diagramme d'éboulis a une solution explicite et peut donc dépanner si on n'arrive pas à obtenir le maximum de vraisemblance.

D'autres méthodes sont aussi disponibles dans \textbf{SAS} (voir la rubrique d'aide du logiciel) mais les deux méthodes mentionnées devraient être suffisantes pour la grande majorité des applications.

\hypertarget{autres-muxe9thodes-de-rotation-des-facteurs}{%
\subsection{Autres méthodes de rotation des facteurs}\label{autres-muxe9thodes-de-rotation-des-facteurs}}

Jusqu'à présent, nous avons utilisé la méthode de rotation orthogonale varimax. Il existe de nombreuses autres méthodes de rotations orthogonales telles, orthomax, quartimax, parsimax et equimax (voir la rubrique d'aide de \textbf{SAS}). Rappelez-vous que le modèle d'analyse factorielle de base suppose que les facteurs sont non corrélés. Les rotations de type obliques quant à elles permettent d'introduire de la corrélation entre les facteurs. Quelquefois, une telle rotation facilitera davantage l'interprétation des facteurs qu'une rotation orthogonale. \textbf{SAS} permet l'utilisation de plusieurs méthodes de rotation obliques qui sont documentées dans la rubrique d'aide. Notez qu'il faut être prudent lorsqu'on utilise une méthode de rotation oblique car il y aura trois matrices de chargements après rotation (coefficients de régression normalisés, corrélations semi-partielles ou corrélations). On suggère l'utilisation de la première, soit la représentation avec \textbf{coefficients de régression normalisés}. Il s'agit des coefficients de régression si on voulait prédire les variables à l'aide des facteurs. Ils indiquent donc à quel point chaque facteur est associé à chaque variable. Dans le cas d'une rotation orthogonale, ces trois matrices sont les mêmes et il s'agit de trois interprétations valides des chargements.

Le programme suivant fait une analyse factorielle avec quatre facteurs, mais en utilisant une rotation varimax oblique (option \texttt{rotate=obvarimax}).

\begin{verbatim}
proc factor data=multi.factor2
maxiter=500 flag=.3 hey;
var x1-x12;
run;
\end{verbatim}

La matrice des corrélations entres facteurs est donnée dans la figure \ref{fig:fig1p13} et les chargements sont présentés dans la figure \ref{fig:fig1p14}. On voit ici qu'on obtient les mêmes quatre facteurs qu'avec une rotation varimax orthogonale.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/01-facto-e13} 

}

\caption{Corrélation interfacteurs pour rotation varimax oblique}\label{fig:fig1p13}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/01-facto-e14} 

}

\caption{Chargements avec rotation oblique varimax}\label{fig:fig1p14}
\end{figure}

\hypertarget{scores-factoriels}{%
\subsection{Scores factoriels}\label{scores-factoriels}}

Avec les données de l'exemple, en nous basant sur les résultats de l'analyse factorielle, nous avons créé quatre nouvelles échelles (une par facteur) que l'on peut calculer pour chaque individu:

\begin{itemize}
\tightlist
\item
  \emph{service} = \((X_4+X_8+X_{11})/3\),
\item
  \emph{produit} = \((X_3+X_6+X_9+X_{12})/4\),
\item
  \emph{paiement} = \((X_2+X_7+X_{10})/3\),
\item
  \emph{prix} = \((X_1+X_5)/2\).
\end{itemize}

Par exemple, la variable \emph{prix} peut donc être vu comme une combinaison linéaire des 12
variables où seulement \(X_1\) et \(X_5\) reçoivent un poids (égal) différent de zéro. Une autre façon de créer de nouvelles variables consiste à calculer des scores factoriels (un pour chaque facteur) pour chaque individu. Par exemple, pour un individu \(i\) donné, le score factoriel pour le facteur \(k\) peut être prédit à l'aide de la formule
\begin{align*}
\hat{F}_{ik} &= \widehat{\mathbf{L}}^{\top}\mathbf{R}^{-1}\boldsymbol{z}\\&=
\widehat{\gamma}_{1, k} z_{i, 1} + \cdots + \widehat{\gamma}_{12, k}z_{i, 12}, 
\end{align*}
où \(z_{i, 1}, \ldots, z_{i, 12}\) sont les valeurs centrées et réduites des observations correspondant à l'individu et où \(\widehat{\gamma}_{1, k}, \ldots, \widehat{\gamma}_{12, k}\) sont des coefficients estimés à partir des chargements \(l_{ij}\) (après rotation) et de la matrice de corrélation des variables \(\mathbf{R}\), avec \(\widehat{\gamma}_{i, k}=\sum_{j=1}^p \hat{l}_{kj}r_{jk}\).

Ainsi, chacune des 12 variables originales contribue au calcul du score
factoriel. Les variables ayant des chargements plus élevés sur ce facteur auront tendance à avoir des poids (\(\widehat{\gamma}\)) plus élevés. Par contre, les scores factoriels ne sont pas uniques car ils dépendent des chargements utilisés (et donc à la fois de la méthode d'estimation et de la méthode de rotation).
On peut également utiliser les scores factoriels au lieu des 12 variables
originales dans des analyses subséquentes. Il est suggéré d'utiliser les nouvelles variables (échelles) obtenues en faisant les moyennes des variables identifiées comme faisant partie de chaque facteur pour les raisons suivantes:

\begin{itemize}
\tightlist
\item
  l'interprétation des scores factoriels est moins claire (chaque facteur dépend de toutes les variables)
\item
  les scores factoriels ne sont pas uniques (ils dépendent de la méthode d'estimation et de rotation).
\item
  les coefficients servant au calcul seront différents d'une étude à l'autre.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{figures/01-facto-e15} 

}

\caption{Coefficients du score normalisés}\label{fig:fig1p15}
\end{figure}

Pour obtenir les scores avec \textbf{SAS}, il suffit d'insérer l'option \texttt{score} à la procédure \texttt{factor}. L'option \texttt{out=...} permet de créer un fichier de données \textbf{SAS} qui contient la valeur des \(m\) scores pour chaque individu. Les scores factoriels pour l'exemples sont rapportés dans la figure \ref{fig:fig1p15}. On remarque que:

\begin{itemize}
\tightlist
\item
  pour le premier facteur, trois variables ont des poids importants (\(X_4\), \(X_8\) et \(X_{11}\)). Il s'agit donc d'un facteur très proche du facteur \emph{service}.
\item
  pour le deuxième facteur, les variables \(X_3\), \(X_6\), \(X_9\) et \(X_{12}\) ont des poids importants. Il s'agit donc d'un facteur très proche du facteur \emph{produits}.
\item
  pour le troisième facteur, les variables \(X_2\), \(X_7\), \(X_{10}\) ont des poids importants. Il s'agit donc d'un facteur très proche du facteur \emph{paiement}.
\item
  pour le quatrième facteur, seule la variable \(X_1\) a un poids important.
  On aurait pu s'attendre à ce que ce soit également le cas pour \(X_5\), en lien avec le facteur \emph{prix} --- ce facteur était moins clair selon le alpha de Cronbach.
\end{itemize}

Les corrélations entre les échelles (construites avec les moyennes) et les scores factoriels sont données dans la figure \ref{fig:fig1p16}. On remarque la forte corrélation entre le score factoriel et les échelles construites avec les moyennes pour les facteurs \emph{service}, \emph{produits} et \emph{paiement}. Cela veut dire qu'utiliser les échelles ou les scores factoriels ne devrait pas faire de différence dans des analyses subséquentes. Par contre, cette corrélation est plus faible (0,82) pour le facteur \emph{prix}.

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{figures/01-facto-e16} 

}

\caption{Corrélation entre scores et échelles}\label{fig:fig1p16}
\end{figure}

\hypertarget{suxe9lection-de-variables-et-de-moduxe8les-1}{%
\chapter{Sélection de variables et de modèles}\label{suxe9lection-de-variables-et-de-moduxe8les-1}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

Ce chapitre présente des principes, outils et méthodes très généraux pour choisir un « bon » modèle. Nous allons principalement utiliser la régression linéaire pour illustrer les méthodes en supposant que tout le monde connaît ce modèle de base. Les méthodes présentées sont en revanche très générales et peuvent être appliquées avec n'importe quel autre modèle (régression logistique, arbres de classification et régression, réseaux de neurones, analyse de survie, etc.)

L'expression « sélection de variables » fait référence à la situation où l'on cherche à sélectionner un sous-ensemble de variables à inclure dans notre modèle à partir d'un ensemble de variables \(X_1, \ldots, X_p\). Le terme variable ici inclut autant des variables distinctes que des transformations d'une ou plusieurs variables.

Par exemple, supposons que les variables \texttt{age}, \texttt{sexe} et \texttt{revenu} sont trois variables explicatives disponibles. Nous pourrions alors considérer choisir entre ces trois variables. Mais aussi, nous pourrions considérer inclure \(\mathrm{age}^2\), \(\mathrm{age}^3\), \(\log(\mathrm{age})\), etc. Nous pourrions aussi considérer des termes d'interactions entre les variables, comme \(\mathrm{age} \cdot \mathrm{revenu}\) ou \(\mathrm{age}\cdot\mathrm{revenu}\cdot\mathrm{sexe}\). Le problème est alors de trouver un bon sous-ensemble de variables parmi toutes celles considérées.

L'expression « sélection de modèle » est un peu plus générale. D'une part, elle inclut la sélection de variables car, pour une famille de modèles spécifiques (régression linéaire par exemple), choisir un sous-ensemble de variables revient à choisir un modèle. D'autre part, elle est plus générale car elle peut aussi faire référence à la situation où l'on cherche à trouver le meilleur modèle parmi des modèles de natures différentes. Par exemple, on pourrait choisir entre une régression linéaire, un arbre de régression, une forêt aléatoire, un réseau de neurones, etc.

\hypertarget{suxe9lection-de-variables-et-de-moduxe8les-selon-les-buts-de-luxe9tude}{%
\section{Sélection de variables et de modèles selon les buts de l'étude}\label{suxe9lection-de-variables-et-de-moduxe8les-selon-les-buts-de-luxe9tude}}

Nous disposons d'une variable réponse \(Y\) et d'un ensemble de variables explicatives \(X_1, \ldots, X_p\). L'attitude à adopter dépend des buts de l'étude.

1e situation : On veut développer un modèle pour faire des prédictions sans qu'il soit important de tester formellement les effets des paramètres individuels.

Dans ce cas, on désire seulement que notre modèle soit performant pour prédire des valeurs futures de \(Y\). On peut alors baser notre choix de variable (et de modèle) en utilisant des outils qui nous guiderons quant aux performances prédictives futures du modèle (voir \(\mathsf{AIC}\), \(\mathsf{BIC}\) et validation croisée plus loin). On pourra enlever ou rajouter des variables et des transformations de variables au besoin afin d'améliorer les performances prédictives. Les méthodes que nous allons voir concernent essentiellement ce contexte.

2e situation : On veut développer un modèle pour estimer les effets de certaines variables sur notre \(Y\) et tester des hypothèses de recherche spécifiques concernant certaines variables.

Dans ce cas, il est préférable de spécifier le modèle dès le départ selon des considérations scientifiques et de s'en tenir à lui. Faire une sélection de variables dans ce cas est dangereux car on ne peut pas utiliser directement les valeurs-\emph{p} des tests d'hypothèses (ou les intervalles de confiance sur les paramètres) concernant les paramètres du modèle final car elles ne tiennent pas compte de la variabilité due au processus de sélection de variables.

Une bonne planification de l'étude est alors cruciale afin de collecter les bonnes variables, de spécifier le ou les bons modèles, et de s'assurer d'avoir suffisamment d'observations pour ajuster le ou les modèles désirés.

Si procéder à une sélection de variables est quand même nécessaire dans ce contexte, il est quand même possible de le faire en divisant l'échantillon en deux. La sélection de variables pourrait être alors effectuée avec le premier échantillon. Une fois qu'un modèle est retenu, on pourrait alors réajuster ce modèle avec le deuxième échantillon (sans faire de sélection de variables cette fois-ci). L'inférence sur les paramètres (valeurs-\emph{p}, etc.) sera alors valide. Le désavantage ici qu'il faut avoir une très grande taille d'échantillon au départ afin d'être en mesure de le diviser en deux.

\hypertarget{mieux-vaut-plus-que-moins}{%
\section{Mieux vaut plus que moins}\label{mieux-vaut-plus-que-moins}}

Il est préférable d'avoir un modèle un peu trop complexe qu'un modèle trop simple. Plaçons-nous dans le contexte de la régression linéaire et supposons que le vrai modèle est inclus dans le modèle qui a été ajusté. Il y a donc des variables en trop dans le modèle qui a été ajusté. Le modèle ajusté est surspécifié.

Par exemple, supposons que le vrai modèle est \(Y=\beta_0+\beta_1X_1+\varepsilon\) mais que c'est le modèle \(Y=\beta_0+\beta_1X_1+\beta_2X_2+\varepsilon\) qui a été ajusté. Dans ce cas, règle générale, les estimateurs des paramètres et les prédictions provenant du modèle sont sans biais. Mais leurs variances estimées seront un peu plus élevées car on estime des paramètres pour des variables superflues.

Supposons à l'inverse qu'il manque des variables dans le modèle ajusté et que le modèle ajusté est sous-spécifié. Par exemple, supposons que le vrai modèle est \(Y=\beta_0+\beta_1X_1+\beta_2X_2+\varepsilon\), mais que c'est le modèle \(Y=\beta_0+\beta_1X_1+\varepsilon\) qui a été ajusté. Dans ce cas, généralement, les estimateurs des paramètres et les prédictions sont biaisés.

Ainsi, il est généralement préférable d'avoir un modèle légèrement surspécifié qu'un modèle sous-spécifié. Plus généralement, il est préférable d'avoir un peu trop de variables dans le modèle que de prendre le risque d'omettre une ou plusieurs variables importantes. Encore plus généralement, il est préférable d'avoir un modèle un peu trop complexe que d'avoir un modèle trop simple.

Il faut faire attention et ne pas tomber dans l'excès et avoir un modèle trop complexe (avec trop de variables inutiles) car il pourrait souffrir de surajustement (\emph{over-fitting}). Les exemples qui suivent illustreront ce fait.

\hypertarget{trop-beau-pour-uxeatre-vrai}{%
\section{Trop beau pour être vrai}\label{trop-beau-pour-uxeatre-vrai}}

Cette section traite de l'optimisme de l'évaluation d'un modèle lorsqu'on utilise les mêmes données qui ont servies à l'ajuster pour évaluer sa performance. Un principe fondamental lorsque vient le temps d'évaluer la performance prédictive d'un modèle est le suivant : si on utilise les mêmes observations pour évaluer la performance d'un modèle que celles qui ont servi à l'ajuster (à estimer le modèle et ses paramètres), on va surestimer sa performance. Autrement dit, notre estimation de l'erreur que fera le modèle pour prédire des observations futures sera biaisée à la baisse. Ainsi, il aura l'air meilleur que ce qu'il est en réalité. C'est comme si on demandait à un cinéaste d'évaluer son dernier film. Comme c'est son film, il n'aura généralement pas un regard objectif. C'est pourquoi on aura tendance à se fier à l'opinion d'un critique.

On cherchera donc à utiliser des outils et méthodes qui nous donneront l'heure juste (une évaluation objective) quant à la performance prédictive d'un modèle.

\hypertarget{principes-guxe9nuxe9raux}{%
\section{Principes généraux}\label{principes-guxe9nuxe9raux}}

Les idées présentées ici seront illustrées à l'aide de la régression linéaire. Par contre, elles sont valides dans à peu près n'importe quel contexte de modélisation.

Plaçons-nous d'abord dans un contexte plus général que celui de la régression linéaire. Supposons que l'on dispose de \(n\) observations indépendantes sur (\(Y, X_1, \ldots, Xp\)) et que l'on a ajusté un modèle \(\widehat{f}(X_1, \ldots, X_p)\), avec ces données, pour prédire une variable continue \(Y\).

Ce modèle peut être un modèle de régression linéaire,
\begin{align*}
\widehat{f}(X_1, \ldots, X_p) = \widehat{\beta}_0 + \widehat{\beta}_1X_1 + \cdots + \widehat{\beta}_pX_p
\end{align*}
mais il pourrait aussi avoir été construit selon d'autres méthodes (réseau de neurones, arbre de régression, forêt aléatoire, etc.) Une manière de quantifier la performance prédictive du modèle est l'erreur quadratique moyenne de généralisation (\emph{generalization mean squared error}),
\begin{align*}
\mathsf{EMQ}=\mathsf{E}\left[\left\{(Y-\widehat{f}(X_1, \ldots, X_p)\right\}^2\right]
\end{align*}
lorsque (\(Y, X_1, \ldots, X_p\)) est choisi au hasard dans la population. Cette quantité mesure l'erreur (la différence au carré entre la vraie valeur de \(Y\) et la valeur prédite par le modèle) que fait le modèle en moyenne pour l'ensemble de la population. Plus cette quantité est petite, meilleur est le modèle. Le problème est que l'on ne peut pas la calculer, car on ne connaît pas toute la population. Tout au plus peut-on essayer de l'estimer ou bien d'estimer une fonction qui, sans l'estimer directement, classifiera les modèles dans le même ordre qu'elle.

Une première idée est d'estimer \(\mathsf{EMQ}\) avec l'erreur quadratique moyenne de l'échantillon d'apprentissage (\emph{training mean squared error}),
\begin{align*}
\widehat{\mathsf{EMQ}}_a= \frac{1}{n}\sum_{i=1}^n \left\{Y_i-\widehat{f}(X_{i1}, \ldots, X_{ip})\right\}^2.
\end{align*}
Cette quantité est tout simplement l'équivalent du \(\mathsf{EMQ}\), mais est calculée en utilisant seulement notre échantillon.

Malheureusement, selon le principe fondamental de la section précédente, cette quantité n'est pas un bon estimateur de l'\(\mathsf{EMQ}\). En effet, comme on utilise les mêmes observations que celles qui ont estimé le modèle, l'\(\widehat{\mathsf{EMQ}}_a\) aura tendance à toujours diminuer lorsqu'on augmente la complexité du modèle (par exemple, lorsqu'on augmente le nombre de paramètres). L'\(\widehat{\mathsf{EMQ}}_a\) tend à surestimer la qualité du modèle en sous-estimant l'\(\mathsf{EMQ}\). C'est-à-dire, le modèle a l'air meilleur qu'il ne l'est en réalité.

\hypertarget{choix-dun-moduxe8le-polynomial-en-ruxe9gression-linuxe9aire}{%
\subsection{Choix d'un modèle polynomial en régression linéaire}\label{choix-dun-moduxe8le-polynomial-en-ruxe9gression-linuxe9aire}}

Cet exemple simple servira à illustrer le fait qu'on ne peut utiliser directement les mêmes données qui ont servi à ajuster un modèle pour évaluer sa performance.

Nous disposons de 100 observations sur une variable cible \(Y\) et d'une seule variable explicative \(X\). Le fichier \texttt{selection1\_train.xls} contient les données. Nous voulons considérer des modèles polynomiaux (en \(X\)) afin d'en trouver un bon pour prédire \(Y\). Un modèle polynomial est un modèle de la forme \(Y=\beta_0 + \beta_1X+\cdots+\beta_kX^k+\varepsilon\). Le cas \(k=1\) correspond à un modèle linéaire simple, \(k=2\) à un modèle cubique, \(k=3\) à un modèle cubique, etc. Notre but est de déterminer l'ordre (\(k\)) du polynôme qui nous donnera un bon modèle. Voici d'abord le graphe de ces 100 observations.

\begin{center}\includegraphics{MATH60602_files/figure-latex/02-graphedonneestest-1} \end{center}

Il s'agit de l'échantillon d'apprentissage. Ces données ont été obtenues par simulation et le vrai modèle sous-jacent (celui qui a généré les données) est le modèle cubique, c'est-à-dire le modèle d'ordre \(k=3\).

Afin de simuler une population, j'ai généré selon le même modèle 100 000 observations supplémentaires. Ces observations ne vont pas servir à estimer les modèles mais seulement à évaluer leur performance afin d'avoir une estimation sans biais. Ces données se trouvent dans \texttt{selection1\_test.xls}

J'ai ajusté tour à tour à tour les modèles polynomiaux jusqu'à l'ordre 10, avec l'échantillon d'apprentissage de taille 100. C'est-à-dire, le modèle linéaire avec un polynôme d'ordre \(k=1\) (linéaire), \(k=2\) (quadratique), etc., jusqu'à \(k=10\). J'ai ensuite obtenu la valeur de l'erreur moyenne quadratique d'apprentissage pour chacun de ces modèles. J'ai ensuite utilisé ces modèles afin de prédire les 100 000 autres observations (la population) et calculé les 100 000 observations de l'échantillon test pour obtenir une très bonne approximation de l'erreur quadratique moyenne de généralisation.

Voici le graphe de l'\(\widehat{\mathsf{EMQ}}_a\) et de l'\(\mathsf{EMQ}\) de généralisation en fonction de l'ordre (\(k\)) du modèle utilisé.

\begin{verbatim}
## Loading required package: lattice
\end{verbatim}

\begin{center}\includegraphics{MATH60602_files/figure-latex/02-plotEMQa-1} \end{center}

On voit clairement que l'\(\widehat{\mathsf{EMQ}}_a\) diminue en fonction de l'ordre sur l'échantillon d'apprentissage. C'est-à-dire, plus le modèle est complexe, plus l'erreur observée sur l'échantillon d'apprentissage est petite. Mais cela est trompeur. La courbe \(\mathsf{EMQ}\) donne l'heure juste. Il s'agit d'une estimation de la performance réelle des modèles sur de nouvelles données. On voit que le meilleur modèle est donc le modèle cubique (\(k=3\)). Ce qui n'est pas surprenant car il s'agit du modèle que j'ai utilisé pour générer les données. On peut aussi remarquer d'autres éléments intéressants. Premièrement, on obtient un bon gain en performance (\(\mathsf{EMQ}\)) en passant de l'ordre \(2\) à l'ordre \(3\). Ensuite, la perte de performance en passant de l'ordre \(3\) à \(4\), et ensuite à des ordres supérieurs n'est pas si sévère, même si elle est présente. Cela illustre empiriquement qu'il est préférable d'avoir un modèle un peu trop complexe que d'avoir un modèle trop simple. Il serait beaucoup plus grave pour la performance de choisir le modèle avec \(k=2\) que celui avec \(k=4\).

En pratique par contre, on n'a pas accès à la population : les 100 000 observations qui ont servi à estimer l'\(\mathsf{EMQ}\) théorique ne seront pas disponible. Si on a seulement l'échantillon d'apprentissage, soit 100 observations dans notre exemple, comment faire alors pour choisir le bon modèle? C'est ce que nous verrons à partir de la section suivante.

Mais avant cela, nous allons discuter un peu plus en détail au sujet de la régression linéaire et d'une mesure très connue, le coefficient de détermination (\(R^2\)). Supposons que l'on a ajusté un modèle de régression linéaire
\begin{align*}
\widehat{f}(X_1, \ldots, X_p) = \widehat{Y}=\widehat{\beta}_0 + \widehat{\beta}_1X_1+ \cdots + \widehat{\beta}_p X_p.
\end{align*}
La somme du carré des erreurs (\(\mathsf{SCE}\)) pour notre échantillon est
\begin{align*}
\mathsf{SCE}=\sum_{i=1}^n (Y_i - \widehat{\beta}_0 + \widehat{\beta}_1X_1+ \cdots + \widehat{\beta}_p X_p)^2 = \sum_{i=1}^n (Y_i-\widehat{Y}_i)^2.
 \end{align*}
On peut démontrer que si on ajoute une variable quelconque au modèle, la valeur de la somme du carré des erreurs va nécessairement baisser. Il est facile de se convaincre de cela. En régression linéaire, les estimations sont obtenues par la méthode des moindres carrés qui consiste justement à minimiser la \(\mathsf{SCE}\). Ainsi, en ajoutant une variable \(X_{p+1}\) au modèle, la \(\mathsf{SCE}\) ne peut que baisser car, dans le pire des cas, le paramètre de la nouvelle variable sera \(\widehat{\beta}_{p+1}=0\) et on retombera sur le modèle sans cette variable. C'est pourquoi, la quantité \(\widehat{\mathsf{EMQ}}_a=\mathsf{SCE}/n\) ne peut être utilisée comme outil de sélection de modèles en régression linéaire.

Nous venons d'ailleurs d'illustrer cela avec notre exemple sur les modèles polynomiaux. En effet, augmenter l'ordre du polynôme de \(1\) revient à ajouter une variable. Le coefficient de détermination (\(R^2\)) est souvent utilisé comme mesure de qualité du modèle. Il peut s'interpréter comme étant la proportion de la variance de \(Y\) qui est expliquée par le modèle.

Le coefficient de détermination est
\begin{align*}
R^2=\{\mathsf{cor}(\boldsymbol{y}, \widehat{\boldsymbol{y}})\}^2 = 1-\frac{\mathsf{SCE}}{\mathsf{SCT}},
\end{align*}
où \(\mathsf{SCT}=\sum_{i=1}^n (Y_i-\overline{Y})^2\) est la somme des carrés totale calculée en centrant les observations. La somme des carrés totale, \(\mathsf{SCT}\), ne varie pas en fonction du modèle.
Ainsi, on voit que le \(R^2\) va nécessairement augmenter lorsqu'on ajoute une variable au modèle (car la \(\mathsf{SCE}\) diminue). C'est pourquoi on ne peut pas l'utiliser comme outil de sélection de variables.

Le problème principal que nous avons identifié jusqu'à présent afin d'être en mesure de bien estimer la performance d'un modèle est le suivant : si on utilise les mêmes observations pour évaluer la performance d'un modèle que celles qui ont servi à l'ajuster on va surestimer sa performance.

Il existe deux grandes approches pour contourner ce problème lorsque le but est de faire de la sélection de variables ou de modèle :

\begin{itemize}
\tightlist
\item
  utiliser les données de l'échantillon d'apprentissage (en échantillon) et pénaliser \(\widehat{\mathsf{EMQ}}_a\) pour tenir compte de la complexité du modèle (\(\mathsf{AIC}\), \(\mathsf{BIC}\)).
\item
  tenter d'estimer l'\(\mathsf{EMQ}\) directement sur d'autres données (hors échantillon) en utilisant des méthodes de rééchantillonnage, notamment la validation croisée et la division de l'échantillon.
\end{itemize}

\hypertarget{crituxe8res-dinformation}{%
\section{Critères d'information}\label{crituxe8res-dinformation}}

Plaçons-nous dans le contexte de la régression linéaire pour l'instant.
Nous avons déjà utilisé les critères \(\mathsf{AIC}\) et \(\mathsf{BIC}\) en analyse factorielle. Il s'agit de mesures qui découlent d'une méthode d'estimation des paramètres, la méthode du maximum de vraisemblance (\emph{maximum likelihood}).

Il s'avère que les estimateurs des paramètres obtenus par la méthode des moindres carrés en régression linéaire sont équivalents à ceux provenant de la méthode du maximum de vraisemblance si on suppose la normalité des termes d'erreurs du modèle. Ainsi, dans ce cas, nous avons accès aux \(\mathsf{AIC}\) et \(\mathsf{BIC}\), deux critères d'information définis pour les modèles dont la fonction objective est la vraisemblance (qui mesure la probabilité des observations sous le modèle postulé suivant une loi choisie par l'utilisateur). La fonction de vraisemblance \(\mathcal{L}\) et la log-vraisemblance \(\ell\) mesurent l'adéquation du modèle.

Supposons que nous avons ajusté un modèle avec \(p\) paramètres en tout (\textbf{incluant} l'ordonnée à l'origine). En régression linéaire, le critère d'information d'Akaike, \(\mathsf{AIC}\), est
\begin{align*}
\mathsf{AIC} &=-2 \ell(\widehat{\boldsymbol{\beta}}, \widehat{\sigma}^2) +2p=n \ln (\mathsf{SCE}) - n\ln(n) + 2p,
\end{align*}
tandis que le critère d'information bayésien de Schwartz, \(\mathsf{BIC}\), est défini par
\begin{align*}
\mathsf{BIC} &=-2 \ell(\widehat{\boldsymbol{\beta}}, \widehat{\sigma}^2) + p\ln(n)=n \ln (\mathsf{SCE}) - n\ln(n) + p\ln(n)
\end{align*}
Plus la valeur du \(\mathsf{AIC}\) (ou du \(\mathsf{BIC}\)) est petite, meilleur est l'adéquation. Que se passE-t-il lorsqu'on ajoute un paramètre à un modèle? D'une part, la somme du carré des erreurs va méchaniquement diminuer, et donc la quantité \(n \ln (\mathsf{SCE}/n)\) va diminuer. D'autre part, la valeur de \(p\) augmente de \(1\). Ainsi, le \(\mathsf{AIC}\) peut soit augmenter, soit diminuer, lorsqu'on ajoute un paramètre; idem pour le \(\mathsf{BIC}\). Par exemple, le \(\mathsf{AIC}\) va diminuer seulement si la baisse de la somme du carré des erreurs est suffisante pour compenser le fait que le terme \(2p\) augmente à \(2 (p+1)\).

Ces critères pénalisent l'ajout de variables afin de se prémunir contre le surajustement. De plus, le \(\mathsf{BIC}\) pénalise plus que le \(\mathsf{AIC}\). Par conséquent, le critère \(\mathsf{BIC}\) va choisir des modèles contenant soit le même nombre, soit moins de paramètres que le \(\mathsf{AIC}\).

Les critères \(\mathsf{AIC}\) et \(\mathsf{BIC}\) peuvent être utilisés comme outils de sélection de variables en régression linéaire mais aussi beaucoup plus généralement avec d'autres méthodes basées sur la vraisemblance (analyse factorielle, régression logistique, etc.) En fait, n'importe quel modèle dont les estimateurs proviennent de la méthode du maximum de vraisemblance produira ces quantités. Nous donnerons des formules générales pour le \(\mathsf{AIC}\) et le \(\mathsf{BIC}\) dans le chapitre sur la régression logistique.

Le critère \(\mathsf{BIC}\) est le seul de ces critères qui est convergent. Cela veut dire que si l'ensemble des modèles que l'on considère contient le vrai modèle, alors la probabilité que le critère \(\mathsf{BIC}\) choisissent le bon model tend vers 1 lorsque \(n\) tend vers l'infini. Il faut mettre cela en perspective : il est peu vraisemblable que \(Y\) ait été généré exactement selon un modèle de régression linéaire, car le modèle de régression n'est qu'une approximation de la réalité. Certains auteurs trouvent que le \(\mathsf{BIC}\) est quelquefois trop sévère (il choisit des modèles trop simples) pour les tailles d'échantillons finies. Dans certaines applications, cette parsimonie est utile, mais il n'est pas possible de savoir d'avance lequel de ces deux critères (\(\mathsf{AIC}\) et \(\mathsf{BIC}\)) sera préférable pour un problème donné.

Avant de revenir à l'exemple, voici la description d'une modification du coefficient de détermination, le \(R^2\) ajusté, qui permet (contrairement au \(R^2\)) de faire de la sélection de variables. En régression linéaire, le \(R^2\) ajusté est
\begin{align*}
R^2_a=1-\frac{\mathsf{SCE}/(n-p)}{\mathsf{SCT}/(n-1)}.
\end{align*}
Lorsqu'on ajoute une variable, la somme du carré des erreurs (\(\mathsf{SCE}\)) diminue mais c'est aussi le cas de la quantité \((n-p)\). Ainsi, le \(R^2\) ajusté peut soit augmenter, soit diminuer lorsqu'on ajoute une variable. On peut donc l'utiliser pour choisir le modèle. Plus \(R^2_a\) est élevé, mieux c'est. Ce critère est moins sévère que le \(\mathsf{AIC}\), Ainsi, en général, il va choisir un modèle avec le même nombre ou bien avec plus de paramètres que le \(\mathsf{AIC}\). Pour résumer, on aura la situation suivante :
\[ \#(\mathsf{BIC}) \leq \#(\mathsf{AIC}) \leq \#(R^2_a),\]
où \(\#\) représente le nombre de paramètres du modèle linéaire.

Il est facile d'obtenir les quantités \(R^2_a\), \(\mathsf{AIC}\) et \(\mathsf{BIC}\) avec la procédure \texttt{glmselect} dans \textbf{SAS}. Le fichier \texttt{selection1\_intro.sas} contient les programmes. La sortie qui suit provient des commandes :

\begin{verbatim}
proc glmselect data=multi.selection1_train;
model y=x x*x x*x*x /selection=none;
run;
\end{verbatim}

Il s'agit du modèle cubique (d'ordre 3) en \(x\).

\begin{center}\includegraphics[width=0.7\linewidth]{figures/02-select-e1} \end{center}

\begin{center}\includegraphics[width=0.7\linewidth]{figures/02-select-e2} \end{center}

Le tableau qui suit résume ces quantités pour tous les modèles de l'ordre 1 à l'ordre 10.

\begin{table}

\caption{\label{tab:02-table1}Mesures d'adéquation du modèle linéaire et estimés de l'erreur}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & \(\mathsf{EMQ}\) & \(\widehat{\mathsf{EMQ}}_a\) & \(R^2\) & \(R^2_a\) & \(\mathsf{AIC}\) & \(\mathsf{BIC}\)\\
\midrule
1 & 3191.29 & 3674.20 & 0.65 & 0.65 & 1110.70 & 1118.51\\
2 & 3132.67 & 2879.24 & 0.73 & 0.72 & 1088.32 & 1098.74\\
3 & 2697.40 & 2620.05 & 0.75 & 0.74 & 1080.88 & 1093.91\\
4 & 2766.68 & 2581.70 & 0.75 & 0.74 & 1081.41 & 1097.04\\
5 & 2771.05 & 2580.86 & 0.75 & 0.74 & 1083.38 & 1101.61\\
\addlinespace
6 & 2779.66 & 2577.60 & 0.75 & 0.74 & 1085.25 & 1106.09\\
7 & 2780.21 & 2577.49 & 0.75 & 0.74 & 1087.24 & 1110.69\\
8 & 2797.35 & 2531.00 & 0.76 & 0.74 & 1087.42 & 1113.48\\
9 & 2811.07 & 2527.85 & 0.76 & 0.73 & 1089.30 & 1117.96\\
10 & 2848.81 & 2519.14 & 0.76 & 0.73 & 1090.95 & 1122.22\\
\bottomrule
\end{tabular}
\end{table}

Les colonnes \(\mathsf{EMQ}\) et \(\widehat{\mathsf{EMQ}}_a\) ont déjà été expliquées à la section précédente et ont été représentées graphiquement.
On voit que le augmente toujours au fur et à mesure qu'on ajoute une variable (augmente l'ordre du polynôme). Les critères \(\mathsf{AIC}\) et \(\mathsf{BIC}\) choisissent le modèle cubique (\(k=3\)), c'est-à-dire le bon modèle. Le \(R^2\) ajusté quant à lui choisit le modèle d'ordre \(4\) (qui est le deuxième meilleur selon le \(\mathsf{EMQ}\)). N'oubliez pas que ces trois critères sont calculés avec l'échantillon d'apprentissage (\(n=100\)), mais en pénalisant l'ajout de variables. On est ainsi en mesure de contrecarrer le problème provenant du fait qu'on ne peut pas utiliser directement le \(\widehat{\mathsf{EMQ}}_a\).

Le \(\mathsf{AIC}\) et le \(\mathsf{BIC}\) sont des critères très utilisés et très généraux. Ils sont disponibles dès qu'on utilise la méthode du maximum de vraisemblance est utilisée comme méthode d'estimation. Le \(R^2\) ajusté a une portée plus limitée car il est spécialisé à la régression linéaire.

\hypertarget{division-de-luxe9chantillon-et-validation-croisuxe9e}{%
\section{Division de l'échantillon et validation croisée}\label{division-de-luxe9chantillon-et-validation-croisuxe9e}}

La deuxième grande approche après celle consistant à pénaliser le \(\widehat{\mathsf{EMQ}}_a\) consiste à tenter d'estimer le \(\mathsf{EMQ}\) directement. Nous allons voir deux telles méthodes ici, la division de l'échantillon et la validation croisée (\emph{cross-validation}).

Ces deux méthodes s'attaquent directement au problème qu'on ne peut utiliser (sans ajustement) les mêmes données qui ont servi à estimer les paramètres d'un modèle pour estimer sa performance. Pour ce faire, l'échantillon de départ est divisé en deux, ou plusieurs parties, qui vont jouer des rôles différents.

\hypertarget{division-de-luxe9chantillon}{%
\subsection{Division de l'échantillon}\label{division-de-luxe9chantillon}}

Cette idée est très simple. Nous avons un échantillon de taille n. Nous pouvons le diviser au hasard en deux parties de tailles respectives \(n_1\) et \(n_2\) (\(n_1+n_2=n\)),

\begin{itemize}
\tightlist
\item
  un échantillon d'apprentissage (\emph{training}) de taille \(n_1\) et
\item
  un échantillon de validation de taille \(n_2\).
\end{itemize}

L'échantillon d'apprentissage servira à estimer les paramètres du modèle. L'échantillon de validation servira à estimer la performance prédictive (par exemple estimer l'\(\mathsf{EMQ}\)) du modèle. Comme cet échantillon n'a pas servi à estimer le modèle lui-même, il est formé de « nouvelles » observations qui permettent d'évaluer d'une manière réaliste la performance du modèle. Comme il s'agit de nouvelles observations, on n'a pas à pénaliser la complexité du modèle et on peut directement utiliser le critère de performance choisi, par exemple, l'erreur quadratique moyenne, c'est-à-dire, la moyenne des erreurs au carré pour l'échantillon de validation. Cette quantité est une estimation valable de l'\(\mathsf{EMQ}\) de ce modèle. On peut faire la même chose pour tous les modèles en compétition et choisir celui qui a la meilleure performance sur l'échantillon de validation.

Cette approche possède plusieurs avantages. Elle est facile à implanter. Elle est encore plus générale que les critères \(\mathsf{AIC}\) et \(\mathsf{BIC}\). En effet, ces critères découlent de la méthode d'estimation du maximum de vraisemblance. Plusieurs autres types de modèles ne sont pas estimés par la méthode du maximum de vraisemblance (par exemple, les arbres, les forêts aléatoires, les réseaux de neurones, etc.) La performance de ces modèles peut toujours être estimée en divisant l'échantillon. Cette méthode peut donc servir à comparer des modèles de familles différentes. Par exemple, choisit-on un modèle de régression linéaire, une forêt aléatoire ou bien un réseau de neurones?

Cette approche possède tout de même un désavantage. Elle nécessite une grande taille d'échantillon au départ. En effet, comme on divise l'échantillon, on doit en avoir assez pour bien estimer les paramètres du modèle (l'échantillon d'apprentissage) et assez pour bien estimer sa performance (l'échantillon de validation).

La méthode consistant à diviser l'échantillon en deux (apprentissage et validation) afin de sélectionner un modèle est valide. Par contre, si on veut une estimation sans biais de la performance du modèle choisi (celui qui est le meilleur sur l'échantillon de validation), on ne peut pas utiliser directement la valeur observée de l'erreur de ce modèle sur l'échantillon de validation. Elle risque de sous-évaluer l'erreur. En effet, supposons qu'on a 10 échantillons et qu'on ajuste 10 fois le même modèle séparément sur les 10 échantillons. Nous aurons alors 10 estimations différentes de l'erreur du modèle. Il est alors évident que de choisir la plus petite d'entre elles sous-estimerait la vraie erreur du modèle. C'est un peu ce qui se passe lorsqu'on choisit le modèle qui minimise l'erreur sur l'échantillon de validation. Le modèle lui-même est un bon choix, mais l'estimation de son erreur risque d'être sous-évaluée.

Une manière d'avoir une estimation de l'erreur du modèle retenu consiste à diviser l'échantillon de départ en trois (plutôt que deux). Aux échantillons d'apprentissage et de validation, s'ajoute un échantillon « test ». Cet échantillon est laissé de côté durant tout le processus de sélection du modèle qui est effectué avec les deux premiers échantillons tel qu'expliqué plus haut. Une fois un modèle retenu (par exemple celui qui minimise l'erreur sur l'échantillon de validation), on peut alors évaluer sa performance sur l'échantillon test qui n'a pas encore été utilisé jusque là. L'estimation de l'erreur du modèle retenu sera ainsi valide. Il est évident que pour procéder ainsi, on doit avoir une très grande taille d'échantillon au départ.

\hypertarget{validation-croisuxe9e}{%
\subsection{Validation croisée}\label{validation-croisuxe9e}}

Si la taille d'échantillon n'est pas suffisante pour diviser l'échantillon en deux et procéder comme nous venons de l'expliquer, la validation croisée est une bonne alternative. Cette méthode permet d'imiter le processus de division de l'échantillon.

Voici les étapes à suivre pour faire une validation croisée à \(K\) groupes (\emph{\(K\)-fold cross-validation}) :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Diviser l'échantillon au hasard en \(K\) parties \(P_1, P_2, \ldots, P_K\) de taille contenant toutes à peu près le même nombre d'observations.
\item
  Pour \(j = 1\) à \(K\),

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Enlever la partie \(j\).
  \item
    Estimer les paramètres du modèle en utilisant les observations des \(K-1\) autres parties combinées.
  \item
    Calculer la mesure de performance (par exemple la somme du carré des erreurs) de ce modèle pour le groupe \(P_j\).
  \end{enumerate}
\item
  Faire la somme des \(K\) estimations de performance pour obtenir une mesure de performance finale et repondérer au besoin.
\end{enumerate}

On recommande habituellement de prendre entre \(K=5\) et \(10\) groupes (le choix de 10 groupes est celui qui revient le plus souvent en pratique). Si on prend \(K=10\) groupes, alors chaque modèle est estimé avec 90\% des données et on prédit ensuite le 10\% restant. Comme on passe en boucle les 10 parties, chaque observation est prédite une et une seule fois à la fin. Il est important de souligner que les groupes sont formés de façon aléatoire et donc que l'estimé que l'on obtient peut être très variable, surtout si la taille de l'échantillon d'apprentissage est petite. Il arrive également que le modèle ajusté sur un groupe ne puisse pas être utilisé pour prédire les observations mises de côté, notamment si des varibles catégorielles sont présentes. Un échantillonage stratifié permet de pallier à cette lacune, mais ce problème se présente en pratique quand certaines classes ont peu d'observations.

Le cas particulier \(K=n\) (en anglais \emph{leave-one-out cross validation}, ou \(\mathsf{LOOCV}\)) consiste à enlever une seule observation, à estimer le modèle avec les \(n-1\) autres et à valider à l'aide de l'observation laissée de côté et on recommence pour chaque observation. Pour les modèles linéaires, il existe des formules explicites qui nous permettent d'éviter d'ajuster \(n\) régressions par moindre carrés.

Le fichier \texttt{selection3\_cv.sas} contient une macro SAS permettant de faire une validation croisée pour un modèle de régression linéaire.
Revenons à notre exemple où une seule variable explicative est disponible et où l'on cherche à déterminer un bon modèle polynomial. Le Tableau \ref{tab:02-table2} est le même que le Tableau \ref{tab:02-table1} mais avec une colonne en plus, la dernière, \(\mathsf{VC} (K=10)\). Il s'agit des estimations du \(\mathsf{EMQ}\) obtenues avec la validation croisée à 10 groupes. Notez que si vous exécutez le programme, vous n'obtiendrez pas les mêmes valeurs car il y a un élément aléatoire dans ce processus. La colonne représente la moyenne de 100 réplications.

\begin{table}

\caption{\label{tab:02-table2}Mesures d'adéquation du modèle linéaire et estimés de l'erreur, incluant la validation croisée.}
\centering
\begin{tabular}[t]{lrrrrrrr}
\toprule
  & \(\mathsf{EMQ}\) & \(\widehat{\mathsf{EMQ}}_a\) & \(R^2\) & \(R^2_a\) & \(\mathsf{AIC}\) & \(\mathsf{BIC}\) & \(\mathsf{VC} (K=10)\)\\
\midrule
1 & 3191.29 & 3674.20 & 0.65 & 0.65 & 1110.70 & 1118.51 & 3675.37\\
2 & 3132.67 & 2879.24 & 0.73 & 0.72 & 1088.32 & 1098.74 & 2897.94\\
3 & 2697.40 & 2620.05 & 0.75 & 0.74 & 1080.88 & 1093.91 & 2675.51\\
4 & 2766.68 & 2581.70 & 0.75 & 0.74 & 1081.41 & 1097.04 & 2666.16\\
5 & 2771.05 & 2580.86 & 0.75 & 0.74 & 1083.38 & 1101.61 & 2711.11\\
\addlinespace
6 & 2779.66 & 2577.60 & 0.75 & 0.74 & 1085.25 & 1106.09 & 2757.13\\
7 & 2780.21 & 2577.49 & 0.75 & 0.74 & 1087.24 & 1110.69 & 2787.95\\
8 & 2797.35 & 2531.00 & 0.76 & 0.74 & 1087.42 & 1113.48 & 2845.78\\
9 & 2811.07 & 2527.85 & 0.76 & 0.73 & 1089.30 & 1117.96 & 2895.61\\
10 & 2848.81 & 2519.14 & 0.76 & 0.73 & 1090.95 & 1122.22 & 2976.04\\
\bottomrule
\end{tabular}
\end{table}

Le modèle cubique (ordre 3) est aussi choisi par la validation croisée, en moyenne (comme il l'était par le \(\mathsf{AIC}\) et le \(\mathsf{BIC}\)). Le graphe qui suit trace les valeurs de l'estimation par validation croisée (courbe de validation croisée) et aussi le \(\mathsf{EMQ}\). On voit que l'estimation par validation croisée suit assez bien la forme du \(\mathsf{EMQ}\) (qu'il est supposé estimer). Les boîtes à moustache permettent d'apprécier la variabilité des estimés de l'erreur moyenne quadratique telles qu'estimée par validation croisée avec 10 groupes.

\begin{center}\includegraphics{MATH60602_files/figure-latex/plotcv-1} \end{center}

\hypertarget{cibler-les-clients-pour-lenvoi-dun-catalogue}{%
\section{Cibler les clients pour l'envoi d'un catalogue}\label{cibler-les-clients-pour-lenvoi-dun-catalogue}}

Nous allons présenter un exemple classique de commercialisation de bases de données qui nous servira à illustrer la sélection de modèles, la régression logistique et la gestion de données manquantes.

Le contexte est le suivant : une entreprise possède une grande base de données client. Elle désire envoyer un catalogue à ses clients mais souhaite maximiser les revenus d'une telle initiative. Il est évidemment possible d'envoyer le catalogue à tous les clients mais ce n'est possiblement pas optimal. La stratégie envisagée est la suivante :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Envoyer le catalogue à un échantillon de clients et attendre les réponses. Le coût de l'envoi d'un catalogue est de 10\$.
\item
  Construire un modèle avec cet échantillon afin de décider à quels clients (parmi les autres) le catalogue devrait être envoyé, afin de maximiser les revenus.
\end{enumerate}

Plus précisément, on s'intéresse aux clients de 18 ans et plus qui ont au moins un an d'historique avec l'entreprise et qui ont fait au moins un achat au cours de la dernière année. Il y a 101 000 clients dans la base de données. La première étape de la stratégie consiste à envoyer le catalogue à un échantillon de 1000 clients. Par la suite, un modèle sera construit avec ces 1000 clients afin de cibler lesquels des 100 000 clients restants seront choisis pour recevoir le catalogue. Les 1000 clients forment l'échantillon d'apprentissage. Pour les 1000 clients de l'échantillon d'apprentissage, les deux variables cibles suivantes sont disponibles :

\begin{itemize}
\tightlist
\item
  \texttt{yachat}, une variable binaire qui indique si le client a acheté quelque chose dans le catalogue égale à 1 si oui et 0 sinon.
\item
  \texttt{ymontant}, le montant de l'achat si le client a acheté quelque chose.
\end{itemize}

Les 10 variables suivantes sont disponibles pour tous les clients et serviront de variables explicatives pour les deux variables cibles. Il s'agit de :

\begin{itemize}
\tightlist
\item
  \texttt{x1}: sexe de l'individu, soit homme (0) ou femme (1);
\item
  \texttt{x2}: l'âge (en année);
\item
  \texttt{x3}: variable catégorielle indiquant le revenu, soit moins de 35 000\$ (1), entre 35 000\$ et 75 000\$ (2) ou plus de 75 000\$ (3);
\item
  \texttt{x4}: variable catégorielle indiquant la région où habite le client (de 1 à 5);
\item
  \texttt{x5}: conjoint : le client a-t-il un conjoint (0=non, 1=oui);
\item
  \texttt{x6}: nombre d'année depuis que le client est avec la compagnie;
\item
  \texttt{x7}: nombre de semaines depuis le dernier achat;
\item
  \texttt{x8}: montant (en dollars) du dernier achat;
\item
  \texttt{x9}: montant total (en dollars) dépensé depuis un an;
\item
  \texttt{x10}: nombre d'achats différents depuis un an.
\end{itemize}

Les données se trouvent dans le fichier \texttt{DBM.sas7bdat}. Lors d'une vraie application, nous aurions seulement les valeurs des variables cibles \texttt{yachat} et \texttt{ymontant} pour l'échantillon d'apprentissage (car eux seuls ont reçu le catalogue). Dans notre exemple, elles sont fournies pour tous les clients afin de pouvoir évaluer la performance des différentes stratégies testées. Les modèles seront déterminés (sélectionnés et ajustés) en utilisant seulement l'échantillon d'apprentissage (1000 clients). Les 100 000 autres clients serviront d'échantillon test pour évaluer la performance des modèles et, plus précisément, afin d'évaluer les revenus (ou d'autres mesures de performance) si ces modèles avaient été utilisés. L'échantillon test nous donnera donc l'heure juste quant aux mérites des différentes approches que nous allons comparer.

Voici d'abord des statistiques descriptives pour l'échantillon d'apprentissage.

\begin{center}\includegraphics[width=0.6\linewidth]{figures/02-select-e3} \end{center}

Il y a donc \(46,6\)\% de femmes parmi les 1000 clients de l'échantillon. De plus, \(39,7\)\% ont un revenu de moins de 35 000\$, \(33,7\)\% sont entre 35 000\$ et 75 000\$ et \(26,6\)\% ont plus de 75 000\$. \(42,5\)\% de ces clients qui ont un conjoint.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/02-select-e4} \end{center}

Le nombre d'achats différents depuis un an par ces clients varie entre 1 et 14. Un peu plus de la moitié (\(51,4\)\%) ont fait 5 achats ou moins. Parmi les 1000 clients de l'échantillon d'apprentissage, 210 ont acheté quelque chose dans le catalogue. La variable yachat sera l'une des variables que nous allons chercher à modéliser en vue d'obtenir des prédictions.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/02-select-e5} \end{center}

L'âge des 1000 clients de l'échantillon d'apprentissage varie entre 20 et 70 avec une moyenne de \(37,1\) ans. En moyenne, ces clients ont acheté pour \(229,3\)\$ depuis un an. Le dernier achat de ces clients remonte, en moyenne, à 10 semaines. Nous chercherons également à modéliser la variable \texttt{ymontant}. Seuls 210 clients ont acheté quelque chose dans le catalogue et les statistiques rapportées correspondent seulement à ces derniers, car la variable \texttt{ymontant} est manquante si le client n'a rien acheté dans le catalogue. On pourrait également remplacer ces valeurs par des zéros et les modéliser, mais nous aborderons cet aspect ultérieurement. Les clients qui ont acheté quelque chose ont dépensé en moyenne \(67,3\)\$, et au minimum \(25\)\$. Les histogrammes de quelques unes de ces variables permet de mieux visualiser la répartition des observations.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/02-select-e6} \end{center}

Il y a plusieurs façons d'utiliser l'échantillon d'apprentissage afin de mieux cibler les clients à qui envoyer le catalogue et maximiser les revenus. En voici quelques unes.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  On pourrait développer un modèle afin d'estimer la probabilité qu'un client achète quelque chose si on lui envoie un catalogue. Plus précisément, on peut développer un modèle pour \(\Pr(\texttt{yachat}=1)\). Comme la variable \texttt{yachat} est binaire, un modèle possible est la régression logistique, que nous décrirons au chapitre suivant. Ainsi, en appliquant le modèle aux 100 000 clients restant, on pourra cibler les clients susceptibles d'acheter (ceux avec une probabilité élevée).
\item
  Une autre façon serait de tenter de prévoir le montant d'argent dépensé. Nous venons de voir la distribution de la variable \texttt{ymontant}. Il y a deux situations, ceux qui ont acheté et ceux qui n'ont pas achetés. En conditionnant sur le fait d'avoir acheté quelque chose, il est possible de décomposer le problème de la manière suivante :
\end{enumerate}

\begin{align*} 
{\mathsf E}\left(\texttt{ymontant}\right) &= {\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right) {\mathsf P}\left(\texttt{yachat}=1\right) \\& \qquad \qquad \qquad +
{\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=0\right) {\mathsf P}\left(\texttt{yachat}=0\right) \\
 &= {\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right) {\mathsf P}\left(\texttt{yachat}=1\right)
\end{align*}
En mots, la moyenne du montant dépensé est égale à la moyenne du montant dépensé étant donné qu'il y a eu achat, fois la probabilité qu'il ait eu achat.

On peut donc estimer \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\) et \({\mathsf P}\left(\texttt{yachat}=1\right)\), pour ensuite les combiner et avoir une estimation de \({\mathsf E}\left(\texttt{ymontant}\right)\). Le développement du modèle pour \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\) peut se faire avec la régression linéaire, en utilisant seulement les clients qui ont acheté dans l'échantillon d'apprentissage, car ymontant est une variable continue dans ce cas. Le développement du modèle pour \({\mathsf P}\left(\texttt{yachat}=1\right)\) peut se faire avec la régression logistique, tel que mentionné plus haut, en utilisant tous les 1000 clients de l'échantillon d'apprentissage. En fait, nous verrons plus loin qu'il est possible d'estimer conjointement les deux modèles avec un modèle Tobit. En appliquant le modèle aux 100 000 clients restants, on pourra cibler les clients qui risquent de dépenser un assez grand montant.

Comme nous n'avons pas encore vu la régression logistique, nous allons nous limiter à illustrer les méthodes qui restent à voir dans ce chapitre avec la régression linéaire en cherchant à développer un modèle pour \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\), le montant d'argent dépensé par les clients qui ont acheté quelque chose.

Le fichier \texttt{prepare\_DBM.sas} contient des commandes afin de préparer les données aux analyses qui seront présentées dans les sections qui suivent. En particulier, nous avons deux variables explicatives catégorielles. Il s'agit de revenu (\texttt{x3}) et région (\texttt{x4}). Il faut coder d'une manière appropriée afin de pouvoir les incorporer dans les modèles. La manière habituelle est de créer des variables indicatrices (binaires) qui indiquent si la variable prend ou non une valeur particulière. En général, si une variable catégorielle possède \(K\) valeurs possibles, il est suffisant de créer \(K-1\) indicatrices, en laissant une modalité comme référence. Par exemple, pour \texttt{X3}, nous allons créer deux variables,

\begin{itemize}
\tightlist
\item
  \texttt{x31}: variable binaire égale à 1 si \texttt{x3} égale 1 et 0 sinon,
\item
  \texttt{x32}: variable binaire égale à 1 si \texttt{x3} égale 2 et 0 sinon.
\end{itemize}

Ainsi, la valeur 3 est celle de référence. Ces deux indicatrices sont suffisantes pour récupérer toute l'information comme le démontre le tableau @\ref(tab:02-dummy).

\begin{longtable}[]{@{}ccc@{}}
\caption{\label{tab:02-dummy} Valeur des indicateurs en fonction du niveau de la variable catégorique}\tabularnewline
\toprule
\texttt{x3} & \texttt{x31} & \texttt{x32}\tabularnewline
\midrule
\endfirsthead
\toprule
\texttt{x3} & \texttt{x31} & \texttt{x32}\tabularnewline
\midrule
\endhead
1 & 1 & 0\tabularnewline
2 & 0 & 1\tabularnewline
3 & 0 & 0\tabularnewline
\bottomrule
\end{longtable}

En pratique, il suffit d'incorporer les indicatrices (\texttt{x31} et \texttt{x32}) dans le modèle comme variables explicatives et de ne plus utiliser la variable originale \texttt{x3}. On peut aussi procéder ainsi pour la variable \texttt{x4}, en créant quatre indicatrices.

\hypertarget{recherche-automatique-du-meilleur-moduxe8le}{%
\section{Recherche automatique du meilleur modèle}\label{recherche-automatique-du-meilleur-moduxe8le}}

Lorsque nous voulons comparer un petit nombre de modèles, il est relativement aisé d'obtenir les critères (\(\mathsf{AIC}\), \(\mathsf{BIC}\) ou autre) pour tous les modèles et de choisir le meilleur. C'était le cas dans l'exemple du choix de l'ordre du polynôme où il y avait seulement 10 modèles en compétitions.
Mais lorsqu'il y a plusieurs variables en jeu, le nombre de modèles potentiel augmente très rapidement.

En fait, supposons qu'on a \(p\) variables distinctes disponibles. Avant même de considérer les transformations des variables et les interactions entre elles, il y a déjà modèles possibles. En effet, chaque variable est soit incluse ou pas (deux possibilités) et donc il y a \(2^p=2\times 2 \times \cdots \times 2\) (\(p\) fois) modèles en tout à considérer. Ce nombre augmente très rapidement comme en témoigne le tableau \ref{tab:02-table3}.

\begin{table}

\caption{\label{tab:02-table3}Nombres de modèles en fonction du nombre de paramètres $p$.}
\centering
\begin{tabular}[t]{rr}
\toprule
$p$ & nombre de paramètres\\
\midrule
5 & 32\\
10 & 1024\\
15 & 32768\\
20 & 1048576\\
25 & 33554432\\
\addlinespace
30 & 1073741824\\
\bottomrule
\end{tabular}
\end{table}

Ainsi, si le nombre de variables est restreint, il est possible de comparer tous les modèles potentiels et de choisir le meilleur (selon un critère). II existe même des algorithmes très efficaces qui permettent de trouver le meilleur modèle sans devoir examiner tous les modèles possibles. Le nombre de variables qu'il est possible d'avoir dépend de la puissance de calcul et augmente d'année en année. Par contre, dans plusieurs applications, il ne sera pas possible de comparer tous les modèles et il faudra effectuer une recherche limitée.

Faire une recherche exhaustive parmi tous les modèles possibles s'appelle sélection de tous les sous-ensembles (\emph{best subsets}). La procédure \texttt{reg} de \textbf{SAS} permet de faire cela pour la régression linéaire.

\hypertarget{recherche-automatique-de-tous-les-sous-ensembles}{%
\section{Recherche automatique de tous les sous-ensembles}\label{recherche-automatique-de-tous-les-sous-ensembles}}

On veut trouver un bon modèle pour prévoir la valeur de \texttt{ymontant} des clients qui ont acheté quelque chose. On a vu qu'il y a 210 clients qui ont acheté dans l'échantillon d'apprentissage. Nous allons chercher à développer un « bon » modèle avec ces 210 clients. Dans ce premier exemple, nous allons seulement utiliser les 10 variables explicatives de base (14 variables avec les indicatrices). Le code suivant montre comment faire une sélection de variables selon le critère du \(R^2\) et demande à \textbf{SAS} de présenter le modèle à \(k\) variables (\(k=1, \ldots, 14\)) qui a le plus grand \(R^2\); voir \texttt{selection2\_all\_subset.sas} pour plus de détails.

\begin{verbatim}
proc reg data=trainymontant;
model ymontant=x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 
  x7 x8 x9 x10 / selection=rsquare best=1 aic bic;
run;
\end{verbatim}

Ainsi, le modèle linéaire simple qui a le plus grand \(R^2\) est celui qui inclut le conjoint (\texttt{x5}). Le meilleur modèle (selon le \(R^2\)) parmi tous les modèles avec deux variables est celui avec \texttt{x5} et \texttt{x6}.

Pour un nombre de variables fixé, le meilleur modèle selon le \(R^2\) est aussi le meilleur selon les critères d'information \(\mathsf{AIC}\) et \(\mathsf{BIC}\), pour ce nombre fixé de variables. Pour vous convaincre de cette affirmation, fixons le nombre de variables et restreignons-nous seulement aux modèles avec ce nombre de variables. Comme = \(1 - \mathsf{SCE}/\mathsf{SCT}\) et que \(\mathsf{SCT}\) est une constante indépendante du modèle, le modèle avec le plus grand coefficient de détermination, \(R^2\), est aussi celui avec la plus petite somme du carré des erreurs (\(\mathsf{SCE}\)). Comme \(\mathsf{AIC}=n (\ln (\mathsf{SCE}/n)) + 2p\), ce sera aussi celui avec le plus petit \(\mathsf{AIC}\) car la pénalité \(2p\) est la même si on fixe le nombre de variables; la même remarque est valide pour le \(\mathsf{BIC}\).

Ainsi, pour trouver le meilleur modèle globalement (sans fixer le nombre de variables), il suffit de trouver le modèle à \(k\) variables explicatives ayant le coefficient de détermination le plus élevé pour tous les nombres de variables fixés et d'ensuite de trouver celui qui minimise le \(\mathsf{AIC}\) (ou le \(\mathsf{BIC}\)) parmi ces modèles. Cette astuce est utile dans la mesure où \textbf{SAS} ne permet pas de faire cette même recherche avec les critères d'information.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/02-select-e7} \end{center}

Dans l'exemple, on voit que le modèle avec les variables \texttt{x1} \texttt{x2} \texttt{x31} \texttt{x44} \texttt{x5} \texttt{x6} \texttt{x7} \texttt{x8} \texttt{x9} et \texttt{x10} est celui qui minimise le \(\mathsf{AIC}\) globalement (\(\mathsf{AIC}=660.15\)). Le modèle choisi par le \(\mathsf{BIC}\) contient seulement sept variables explicatives (plutôt que 10), soit \texttt{x1} \texttt{x31} \texttt{x5} \texttt{x6} \texttt{x7} \texttt{x8} \texttt{x10}.

Nous allons utiliser les 100 000 autres clients pour évaluer la performance réelle des modèles qui nous sont suggérés par nos différents critères. En pratique, nous ne pourrions pas faire cela car la valeur de la variable cible ne serait pas connue pour ces clients. En fait, dans une vraie application, nous utiliserions plutôt les modèles pour obtenir des prédictions pour les clients à « scorer ». Les valeurs des variables cibles pour les 100 000 clients nous permettront de voir à quel point différentes stratégies auraient été profitables si elles avaient été mises en place. Parmi, les 100 000 clients restants, il y en a 23 179 qui auraient acheté quelque chose si on leur avait envoyé le catalogue. Ces 23 179 observations vont nous servir pour estimer l'erreur moyenne quadratique (théorique) des modèles retenus par nos critères (voir le fichier \texttt{selection2\_all\_subset.sas} pour les manipulations).

Voici l'estimation de l'erreur moyenne quadratique (moyenne des carrés des erreurs) pour les deux modèles retenus par le \(\mathsf{AIC}\) et le \(\mathsf{BIC}\). Le tableau \ref{tab:02-gmse-base} contient aussi l'estimation de l'erreur moyenne quadratique si on utilise toutes les variables (14 en incluant les indicatrices) sans faire de sélection.

\begin{longtable}[]{@{}ccl@{}}
\caption{\label{tab:02-gmse-base} Estimation de l'erreur moyenne quadratique sur l'échantillon test avec les variables de base. Les meilleurs modèles selon les critères d'informations découlent d'une recherche exhaustive de tous les sous-ensembles.}\tabularnewline
\toprule
nombre de variables & \(\mathsf{EMQ}\) & méthode\tabularnewline
\midrule
\endfirsthead
\toprule
nombre de variables & \(\mathsf{EMQ}\) & méthode\tabularnewline
\midrule
\endhead
14 & 25,69 & toutes les variables\tabularnewline
10 & 24,72 & exhaustive - \(\mathsf{AIC}\)\tabularnewline
7 & 23,83 & exhaustive - \(\mathsf{BIC}\)\tabularnewline
\bottomrule
\end{longtable}

On voit que le modèle choisi par le \(\mathsf{BIC}\) est le meilleur des trois, car l'erreur moyenne quadratique sur l'échantillon test est de \(3,6\)\% inférieure à celle du modèle choisi par le \(\mathsf{AIC}\). Ces deux méthodes font mieux que le modèle qui inclut toutes les variables sans faire de sélection.

Nous avons seulement inclus les variables de base pour ce premier essai. Il est possible qu'ajouter des variables supplémentaires améliore la performance du modèle. Pour cet exemple, nous allons considérer les variables suivantes:

\begin{itemize}
\tightlist
\item
  les variables continues au carré, comme \(\texttt{age}^2\).
\item
  toutes les interactions d'ordre deux entre les variables de base, comme \(\texttt{sexe}\cdot\texttt{age}\).
\end{itemize}

Toutes ces variables sont créées dans \texttt{prepare\_DBM.sas}. Aux variables de base (10 variables explicatives, mais 14 avec les indicatrices pour les variables catégorielles), s'ajoutent ainsi 90 autres variables. Il y a donc 104 variables explicatives potentielles. Notez qu'il y a des interactions entre chacune des variables indicatrices et chacune des autres variables, mais il ne sert à rien de calculer une interaction entre deux indicatrices d'une même variable (car une telle variable est zéro pour tous les individus). De même, il ne sert à rien de calculer le carré d'une variable binaire.

Lancer une sélection exhaustive de tous les sous-modèles avec 104 variables risque de prendre un temps énorme. Que faire alors? Il y a plusieurs possibilités. Nous pourrions faire une recherche limitée avec les méthodes que nous allons voir à partir de la section suivante. Nous pourrions aussi combiner les deux approches. Supposons que notre ordinateur permet de faire une recherche exhaustive de tous les sous-modèles avec 40 variables. Nous pourrions alors commencer avec une recherche limitée pour trouver un sous-ensemble de 40 « bonnes » variables et faire une recherche exhaustive, mais en se restraignant à ces 40 variables.

\hypertarget{muxe9thodes-classiques-de-suxe9lection-ascendante-descendante-et-suxe9quentielle}{%
\section{Méthodes classiques de sélection ascendante, descendante et séquentielle}\label{muxe9thodes-classiques-de-suxe9lection-ascendante-descendante-et-suxe9quentielle}}

Les méthodes de sélection ascendante, descendante et séquentielle sont des algorithmes gloutons qui permettent de choisir des variables. Elles ont été développées à une époque où la puissance de calcul était bien moindre, et où il était impossible de faire une recherche exhaustive des sous-modèles. Avec l'approche classique, ces méthodes font une recherche séquentielle guidée parmi un nombre limité de modèles, à l'aide des valeurs-\emph{p} du test-\emph{t} pour la significativité des paramètres individuels du modèle avec \(p\) prédicteurs potentiels \(X_1,\ldots, X_p\). Les procédures \texttt{glmselect} et \texttt{reg} permettent une sélection de modèle avec une approche séquentielle, ascendante ou descendante.

\hypertarget{suxe9lection-ascendante}{%
\subsection{Sélection ascendante}\label{suxe9lection-ascendante}}

L'idée de la sélection ascendante est de tester l'ajout de chaque variable individuellement et d'ajouter celle qui est la plus significative selon le test-\emph{t} si elle a une valeur-\emph{p} assez petite.

\begin{itemize}
\tightlist
\item
  \emph{Initialisation}: le modèle linéaire de départ est celui qui n'inclut que l'ordonnée à l'origine, \(Y=\beta_0+\varepsilon\), où \(\varepsilon\) est une erreur centrée.
\item
  \emph{Critère d'entrée}: \(c\), valeur-\emph{p} minimale à partir de laquelle une variable peut être incluse dans le modèle (\texttt{proc\ reg} utilise par défaut 0.5).
\item
  \emph{Boucle} soit \(X_{(1)}, \ldots, X_{(k)}\), les variables explicatives à l'étape \(k<p\).

  \begin{itemize}
  \tightlist
  \item
    pour chaque \(j\) (\(j=\{1,\ldots, p\}\setminus \{(1), \ldots (k)\}\)), on ajuste tour à tour le modèle \(Y=\beta_0+\sum_{i=1}^k \beta_i X_{(i)} + \beta_{k+1}X_{j}\) et on calcule la valeur-\emph{p} du test-\emph{t} pour les hypothèses \(\mathcal{H}_0: \beta_{k+1}=0\) contre l'alternative bilatérale \(\mathcal{H}_1: \beta_{k+1} \neq 0\).
  \item
    Soit \(p_{\min}\) la plus petite des \(p-k\) valeurs-\emph{p} qui correspond à \(X_{(k+1)}\), disons.

    \begin{itemize}
    \tightlist
    \item
      si \(p_{\min}<c\), continuer la procédure.
    \item
      si \(p_{\min} \geq c\), retourner le modèle \(Y=\beta_0 + \sum_{i=1}^k \beta_iX_{(i)}+\varepsilon\).
    \end{itemize}
  \end{itemize}
\end{itemize}

On continue ainsi à ajouter des variables jusqu'à ce que le critère d'entrée ne soit pas satisfait. Si on se rend jusqu'au bout, on va terminer avec le modèle complet qui contient toutes les variables.

\hypertarget{suxe9lection-descendante}{%
\subsection{Sélection descendante}\label{suxe9lection-descendante}}

\begin{itemize}
\tightlist
\item
  \emph{Initialisation}: le modèle linéaire de départ est celui qui inclut toutes les variables explicatives, \(Y=\beta_0+\sum_{j=1}^p \beta_j X_{(j)}+\varepsilon\), où \(\varepsilon\) est une erreur centrée.
\item
  \emph{Critère de sortie}: \(c\), valeur-\emph{p} maximale à partir de laquelle une variable peut être excluse du modèle (\texttt{proc\ reg} utilise par défaut 0.1).
\item
  \emph{Boucle} soit \(X_{(1)}, \ldots, X_{(p-k)}\), les variables explicatives présentes dans le modèle à l'étape \(k<p\).

  \begin{itemize}
  \tightlist
  \item
    pour chaque \(j\) (\(j =1, \ldots, p-k\)), on calcule la valeur-\emph{p} du test-\emph{t} \(\mathcal{H}_0: \beta_{j}=0\) contre l'alternative bilatérale \(\mathcal{H}_1: \beta_{j} \neq 0\).
  \item
    si toutes ces valeurs sont inférieures à \(c\), on retourne le modèle \(Y=\beta_0 + \sum_{j=1}^{p-k} \beta_j X_{(j)}\).
  \item
    sinon, on enlève la variable qui a la plus grande valeur-\emph{p} (disons \(X_{(p-k)}\)), on réajuste le modèle sans cette variable et on recommence la procédure.
  \end{itemize}
\end{itemize}

L'idée est l'inverse de la méthode ascendante. On va tester le retrait de chaque variable individuellement et retirer celle qui est la moins significative, si sa valeur-\emph{p} est assez grande. Si la procédure se termine après \(p\) itérations, aucune variable explicative n'est retenue.

\hypertarget{muxe9thode-suxe9quentielle}{%
\subsection{Méthode séquentielle}\label{muxe9thode-suxe9quentielle}}

Il s'agit d'une méthode hybride entre ascendante et descendante. On sélectionne un critère d'entrée et de sortie pour chacune des deux (0.15 dans \texttt{proc\ reg}) et on début la recherche à partir du modèle ne contenant que l'ordonnée à l'origine. À chaque étape, on fait une étape ascendante suivie de une (ou plusieurs) étapes descendantes. On continue ainsi tant que le modèle retourné par l'algorithme n'est pas identique à celui de l'étape précédente. Le dernier modèle est celui retenu.

Avec la méthode séquentielle, une fois qu'on entre une variable (étape ascendante), on fait autant d'étapes descendante afin de retirer toutes les variables qui satisfont le critère de sortie (il peut ne pas y en avoir). Une fois cela effectué, on refait une étape ascendante pour voir si on peut ajouter une nouvelle variable.

Remarques sur ces méthodes: avec la méthode ascendante, une fois qu'une variable est dans le modèle, elle y reste. Avec la méthode descendante, une fois qu'une variable est sortie du modèle, elle ne peut plus y entrer. Avec la méthode séquentielle, une variable peut entrer dans le modèle et sortir plus tard dans le processus. Par conséquent, parmi les trois, la méthode séquentielle est généralement préférable aux méthodes ascendante et descendante, car elle inspecte potentiellement un plus grand nombre de modèles.

On peut soi-même spécifier les critères d'entrée et de sortie. Plus le critère d'entrée est élevé, plus il y aura de variables dans le modèle final. De même, plus le critère de sortie est élevé, plus il y aura de variables dans le modèle.

Utilisons la méthode de sélection séquentielle classique avec des critères d'entrée et de sortie de 0.15 et les 104 variables. Le code suivant, extrait de \texttt{selection2\_all\_subset.sas}, donne la syntaxe \textbf{SAS}.

\begin{verbatim}
proc reg data=trainymontant;
model ymontant=
x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10
cx2 cx6 cx7 cx8 cx9 cx10
i_x2_x1 i_x2_x5 i_x2_x31 i_x2_x32 i_x2_x41 i_x2_x42 i_x2_x43 i_x2_x44
i_x2_x7 i_x2_x6 i_x2_x8 i_x2_x9 i_x2_x10
i_x1_x5 i_x1_x31 i_x1_x32 i_x1_x41 i_x1_x42 i_x1_x43 i_x1_x44
i_x1_x7 i_x1_x6 i_x1_x8 i_x1_x9 i_x1_x10
i_x5_x31 i_x5_x32 i_x5_x41 i_x5_x42 i_x5_x43 i_x5_x44
i_x5_x7 i_x5_x6 i_x5_x8 i_x5_x9 i_x5_x10
i_x31_x41 i_x31_x42 i_x31_x43 i_x31_x44
i_x31_x7 i_x31_x6 i_x31_x8 i_x31_x9 i_x31_x10
i_x32_x41 i_x32_x42 i_x32_x43 i_x32_x44
i_x32_x7 i_x32_x6 i_x32_x8 i_x32_x9 i_x32_x10
i_x41_x7 i_x41_x6 i_x41_x8 i_x41_x9 i_x41_x10
i_x42_x7 i_x42_x6 i_x42_x8 i_x42_x9 i_x42_x10
i_x43_x7 i_x43_x6 i_x43_x8 i_x43_x9 i_x43_x10
i_x44_x7 i_x44_x6 i_x44_x8 i_x44_x9 i_x44_x10
i_x7_x6 i_x7_x8 i_x7_x9 i_x7_x10
i_x6_x8 i_x6_x9 i_x6_x10
i_x8_x9 i_x8_x10
i_x9_x10 / selection=stepwise sle=.15 sls=.15 ;
run;
\end{verbatim}

Notez que les variables \texttt{cx2} à \texttt{cx10} sont les carrés des variables \texttt{x2} à \texttt{x10} que nous avons créées au préalable. De plus, les variables débutant par \emph{i} sont les interactions entre les variables binaires et les variables continues. Par exemple, \texttt{i\_x2\_x1} est l'interaction entre \texttt{x1} et \texttt{x2}, c'est-à-dire le produit des deux.

La sortie \textbf{SAS} est assez volumineuse car elle retrace toutes étapes de la sélection séquentielle. L'historique montre qu'à l'étape 1, la variable \texttt{i\_x5\_x6} a été ajoutée, suivie de \texttt{i\_x31\_x10} à l'étape 2. Un peu plus loin, à l'étape 6, \texttt{i\_x5\_x6} est retirée et ainsi de suite. Il y a eu 40 étapes en tout et, à la fin, il reste 22 variables (parmi les 104) dans le modèle final. Le \(R^2\) du modèle final est \(0,966\).

\begin{center}\includegraphics[width=0.9\linewidth]{figures/02-select-e8} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{figures/02-select-e9} \end{center}

Voici les estimés des paramètres du modèle final retenu. On voit bien que toutes les valeurs-\emph{p} (qui ne sont pas valides à cause de la sélection de modèles) sont toutes inférieures à \(0,15\).

\begin{center}\includegraphics[width=0.7\linewidth]{figures/02-select-e10} \end{center}

\begin{center}\includegraphics[width=0.7\linewidth]{figures/02-select-e11} \end{center}

La performance de ce modèle a, comme pour les modèles précédents, été évaluée avec l'échantillon test de 23 179 observations. Le tableau \ref{tab:02-comparaisonseqclas} présente la performance de la méthode de sélection séquentielle classique et celle du modèle dans lequel les 104 variables sont incluses sans faire de sélection.

\begin{longtable}[]{@{}ccl@{}}
\caption{\label{tab:02-comparaisonseqclas} Comparaison des méthodes selon l'erreur moyenne quadratique pour la méthode de sélection séquentielle classique et pour le modèle incluant toutes les variables, les termes quadratiques et les interactions.}\tabularnewline
\toprule
nombre de variables & \(\mathsf{EMQ}\) & méthode\tabularnewline
\midrule
\endfirsthead
\toprule
nombre de variables & \(\mathsf{EMQ}\) & méthode\tabularnewline
\midrule
\endhead
104 & 19,63 & toutes les variables\tabularnewline
22 & 12,25 & séquentielle classique\tabularnewline
\bottomrule
\end{longtable}

On voit donc qu'utiliser toutes les 104 variables sans faire de sélection fait mieux (\(\mathsf{EMQ}=19,63\)) que les modèles précédents basés sur les 10 variables originales. Mais faire une sélection séquentielle classique permet une amélioration très importante de la performance (\(\mathsf{EMQ}=12,25\)). On voit que dans cet exemple, utiliser les 104 variables fait du surajustement (\emph{over-fitting}).

Le choix de \(0,15\) comme critère d'entrée et de sortie est assez arbitraire. Il est fort possible que d'autres valeurs donnent de meilleurs résultats. Mais il n'est pas évident de les choisir.

Une façon de contourner le problème de devoir spécifier les critères d'entrée et de sortie est de procéder en deux étapes. Supposons que notre ordinateur permet de faire une recherche exhaustive de tous les sous-modèles avec près de 60 variables. L'idée est alors de passer de 104 à un sous-ensemble d'environ 60 variables, avec une sélection séquentielle gloutonne, et d'ensuite utiliser une recherche exhaustive avec ce sous-ensemble de variables. Plus précisément:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  On fait une sélection séquentielle classique avec des valeurs élevées pour les critères d'entrée et de sortie afin que le modèle retenu contienne le nombre voulu de variables (par exemple, 60).
\item
  En utilisant seulement ce sous-ensemble de variables, on choisit le meilleur modèle selon le \(\mathsf{AIC}\) ou le \(\mathsf{BIC}\) en faisant une recherche exhaustive de tous les sous-modèles.
\end{enumerate}

En fixant, les critères d'entrée et de sortie à \(0,6\) pour la recherche séquentielle, le modèle retenu aura 56 variables. Il est possible de faire une recherche exhaustive avec 56 variables sur un ordinateur portable avec \textbf{SAS}. Le \(\mathsf{AIC}\) est mène à un modèle avec 38 de ces 56 variables. Le \(\mathsf{BIC}\) est quant à lui beaucoup plus parcimonieux et choisit 15 de ces variables pour le modèle final. Encore une fois, ces deux modèles sont testés sur les 23 179 clients restants. Les résultats sont présentés dans le tableau \ref{tab:02-comparaisonseqexha}.

\begin{longtable}[]{@{}ccl@{}}
\caption{\label{tab:02-comparaisonseqexha} Comparaison des méthodes selon l'erreur moyenne quadratique pour la méthode de sélection séquentielle suivie d'une recherche exhaustive.}\tabularnewline
\toprule
\begin{minipage}[b]{0.29\columnwidth}\centering
nombre de variables\strut
\end{minipage} & \begin{minipage}[b]{0.34\columnwidth}\centering
\(\mathsf{EMQ}\)\strut
\end{minipage} & \begin{minipage}[b]{0.29\columnwidth}\raggedright
méthode\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.29\columnwidth}\centering
nombre de variables\strut
\end{minipage} & \begin{minipage}[b]{0.34\columnwidth}\centering
\(\mathsf{EMQ}\)\strut
\end{minipage} & \begin{minipage}[b]{0.29\columnwidth}\raggedright
méthode\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.29\columnwidth}\centering
38\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering
14,83\strut
\end{minipage} & \begin{minipage}[t]{0.29\columnwidth}\raggedright
séquentielle classique, recherche exhaustive avec 56 variables \((\mathsf{AIC})\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.29\columnwidth}\centering
15\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering
11,96\strut
\end{minipage} & \begin{minipage}[t]{0.29\columnwidth}\raggedright
séquentielle classique, recherche exhaustive avec 56 variables \((\mathsf{BIC})\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

La stratégie consistant à sélectionner un sous-ensemble de 56 variables avec la méthode séquentielle classique pour ensuite faire une recherche exhaustive de tous les sous-modèles possibles avec ces 56 variables, selon le \(\mathsf{BIC}\), donne le meilleur résultat jusqu'à présent (\(\mathsf{EMQ}=11,96\)). Le \(\mathsf{AIC}\) fait moins bien dans ce cas, avec une erreur moyenne quadratique estimée de \(14,83\). Nous verrons à la section suivante qu'il est possible de faire une recherche séquentielle en utilisant d'autres critères que la valeur-\emph{p} du test-\emph{t} pour faire ajouter ou enlever des variables.

\hypertarget{recherche-suxe9quentielle-automatique-limituxe9e}{%
\section{Recherche séquentielle automatique limitée}\label{recherche-suxe9quentielle-automatique-limituxe9e}}

L'idée de la procédure séquentielle classique est d'inclure ou d'exclure une variable à la fois sur la base des valeurs-\emph{p}. La procédure \texttt{glmselect} permet de faire une sélection séquentielle en utilisant d'autres critères, comme le \(\mathsf{AIC}\) ou le \(\mathsf{BIC}\). Cette procédure permet de contrôler très finement le processus de sélection de variables. Le code qui suit fait une recherche séquentielle avec les particularités suivantes. À chaque étape ascendante de la procédure séquentielle, c'est la variable qui améliore le plus le \(\mathsf{AIC}\) (\texttt{select=aic}) qui est entrée. De plus, à chaque étape descendante de la procédure séquenctielle, c'est la (ou les) variable(s) qui détériore(nt) le plus le \(\mathsf{AIC}\) qui est (sont) retirée(s). À la toute fin du processus, c'est le modèle qui a le meilleur \(\mathsf{BIC}\) (\texttt{choose=BIC}) qui est retenu.

\begin{verbatim}
proc glmselect data=trainymontant;
model ymontant= /*(mettre les 104 variables ici) */
/  selection=stepwise(select=aic choose=bic)  ; 
score data=testymontant out=predglmselectaicbic p=predymontant;
run;
\end{verbatim}

Voici l'historique de la procédure séquentielle avec cette combinaison.

\begin{center}\includegraphics[width=0.7\linewidth]{figures/02-select-e12} \end{center}

\begin{center}\includegraphics[width=0.7\linewidth]{figures/02-select-e13} \end{center}

À l'étape 1, la variable \texttt{i\_x5\_x6} est ajoutée au modèle de base car c'est celle qui fait diminuer le plus le \(\mathsf{AIC}\). À l'étape 2, la variable \texttt{i\_x31\_x10} est ajoutée, À l'étape 6, la variable \texttt{i\_x5\_x6} est retirée car cela fait baisser le \(\mathsf{AIC}\). Notez que le \(\mathsf{AIC}\) décroit toujours d'une étape à l'autre. \textbf{SAS} garde aussi la trace du \(\mathsf{BIC}\) car le modèle final sera choisi selon ce critère. Finalement le processus séquentiel se termine à l'étape 40, car il n'y a plus moyen de faire dimiuer le \(\mathsf{AIC}\). Le modèle final retenu est celui de l'étape 18, car c'est celui qui a le \(\mathsf{BIC}\) le plus petit parmi tous ces modèles (\(\mathsf{BIC}=484.22\)).

Voici différentes statistiques ainsi que les estimations des paramètres de ce modèle qui contient 10 variables.

\begin{center}\includegraphics[width=0.65\linewidth]{figures/02-select-e14} \end{center}

\begin{center}\includegraphics[width=0.7\linewidth]{figures/02-select-e15} \end{center}

Il s'avère que ce modèle performe très bien avec une erreur moyenne quadratique estimé à \(10,08\) sur les 23 179 clients de l'échantillon test. Il s'agit du meilleur jusqu'à maintenant.

\hypertarget{moyenne-de-moduxe8les}{%
\section{Moyenne de modèles}\label{moyenne-de-moduxe8les}}

Une idée importante et moderne en statistique est qu'il est souvent préférable de combiner plusieurs modèles plutôt que d'en choisir un seul. La technique des forêts aléatoires (\emph{random forests}) est une des meilleures techniques de prédiction disponibles de nos jours. Elle est basée sur cette idée, en combinant plusieurs arbres de classification (ou de régression) individuels. C'est une des techniques de base en exploitation de données.

Ici, nous allons voir comment cette idée peut être appliquée à notre contexte. Toutes les méthodes que nous avons vues jusqu'à maintenant font une sélection « rigide » de variables, dans le sens que chaque variable est soit sélectionnée pour faire partie du modèle, soit elle ne l'est pas. C'est donc tout ou rien pour chaque variable. Il y a beaucoup de variabilité associée à une telle forme de sélection. Une variable peut avoir été très près d'être choisie, mais elle ne l'a pas été et est éliminée complètement. Construire plusieurs modèles et en faire la moyenne permet d'adoucir le processus de sélection car une variable peut alors être partiellement sélectionnée.

Supposons qu'on dispose de deux échantillons et qu'on fasse une sélection de variables séparément pour les deux échantillons, avec l'une des approches que nous avons vues jusqu'à maintenant. Il est alors très probable qu'on ne va pas avoir exactement les mêmes variables sélectionnées pour les deux échantillons. Supposons ensuite qu'on fasse la moyenne des coefficients pour les deux modèles. Si une variable, disons \(X_1\), a été choisie les deux fois, alors la moyenne des deux coefficients devrait estimer en quelque sorte un effet global pour cette variable. Si une autre variable, disons \(X_2\), n'a pas été choisie du tout pour les deux échantillons, alors la moyenne de ses deux coefficients est nulle. Mais si une variable, disons, \(X_3\), a été choisie pour seulement l'un des deux échantillons, alors la moyenne de ses deux coefficients est la moitié du coefficient pour le modèle dans lequel elle a été choisie (car l'autre est zéro). Ainsi, cette variable est donc représentée par une « moitié » d'effet dans la moyenne des modèles. Donc au lieu d'être totalement là ou totalement absente, elle est présente en fonction de sa probabilité d'être sélectionnée. Ceci diminue de beaucoup la variabilité engendrée par une sélection « rigide » de variables et permet souvent de produire un modèle fort raisonnable.

Le problème est que l'on n'a pas plusieurs échantillons mais un seul. Une solution possible est de générer nous-mêmes des échantillons différents à partir de l`échantillon original. Cela peut être fait avec l'autoamorçage (\emph{bootstrap}). Un échantillon d'autoamorçage est tout simplement un échantillon choisi au hasard et \textbf{avec remise} dans l'échantillon original. Ainsi, une même observation peut être sélectionnée plus d'une fois tandis qu'une autre peut ne pas être sélectionnée du tout.

L'idée est alors la suivante :

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Générer plusieurs échantillons par autoamorçage nonparamétrique à partir de l`échantillon original.
\item
  Faire une sélection de variables pour chaque échantillon.
\item
  Faire la moyenne des paramètres de ces modèles.
\end{enumerate}

La procédure \texttt{glmselect} a une commande expérimentale, \texttt{modelaverage}, qui permet de faire une moyenne de modèles. Comme elle est expérimentale, les particularités (options et sorties) de cette commande risquent de changer au cours des versions à venir. Le code suivant permet de faire une moyenne de modèles.

\begin{verbatim}
proc glmselect data=trainymontant seed=57484765;
model ymontant= 
  ... /* mettre les 104 variables ici */
/ selection=stepwise(select=bic choose=bic) ; 
score data=testymontant out=predaverage p=predymontant;
modelaverage nsamples=500 sampling=urs subset(best=500);
run;
\end{verbatim}

Chaque modèle est construit à l'aide d'un échantillon aléatoire avec remise (\texttt{sampling=urs}). Il y aura 500 échantillons, et donc modèles, en tout (\texttt{nsamples=500}). L'option \texttt{subset(best=500)} indique à \textbf{SAS} de faire la moyenne des paramètres des 500 modèles. Notez l'option \texttt{seed} qui permet de reproduire les résultats, car elle fixe une valeur pour le générateur de nombre aléatoire (qui sera utilisé pour générer les échantillons d'autoamorçage). Cette fois-ci la sélection se fait avec le critère \(\mathsf{BIC}\) à tous les niveaux (\texttt{select=bic\ choose=bic}).

\begin{center}\includegraphics[width=0.9\linewidth]{figures/02-select-e16} \end{center}

Ce tableau présente les variables qui ont été choisies dans an moins 20\% des modèles, c'est-à-dire, dans au moins 100 des 500 modèles ici. Il y a deux variables qui ont été retenues dans tous les modèles, \texttt{i\_x1\_x6} et \texttt{i\_x31\_x8}. Le tableau rapporte aussi la moyenne des estimations pour ces paramètres.

Il s'avère que cette approche performe très bien sur l'échantillon test de 23 179 clients avec une erreur moyenne quadratique estimé de \(10,57\). Le tableau \ref{tab:02-modelcomparaisonfull} résume la performance des différentes méthodes que nous avons utilisé sur notre échantillon test.

\begin{longtable}[]{@{}lccl@{}}
\caption{\label{tab:02-modelcomparaisonfull} Comparaison des méthodes selon l'erreur moyenne quadratique.}\tabularnewline
\toprule
\begin{minipage}[b]{0.18\columnwidth}\raggedright
variables\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
nombre de variables\strut
\end{minipage} & \begin{minipage}[b]{0.26\columnwidth}\centering
\(\mathsf{EMQ}\)\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright
méthode\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.18\columnwidth}\raggedright
variables\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
nombre de variables\strut
\end{minipage} & \begin{minipage}[b]{0.26\columnwidth}\centering
\(\mathsf{EMQ}\)\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright
méthode\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.18\columnwidth}\raggedright
de base\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
14\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
25,69\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
toutes les variables\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
10\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
24,72\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
exhaustive - \(\mathsf{AIC}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
7\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
23,83\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
exhaustive - \(\mathsf{BIC}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
interactions et termes quadratiques\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
104\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
19,63\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
toutes les variables\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
22\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
12,25\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
séquentielle classique\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
38\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
14,83\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
séquentielle classique, recherche exhaustive avec 56 variables \((\mathsf{AIC})\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
15\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
11,96\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
séquentielle classique, recherche exhaustive avec 56 variables \((\mathsf{BIC})\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
10\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
10,08\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
séquentielle avec critère \(\mathsf{AIC}\) (choix selon le \(\mathsf{BIC}\))\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\centering
10,57\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
moyenne de modèles\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Dans cet exemple, la méthode séquentielle de \texttt{glmselect} avec les options \texttt{select=aic} et \texttt{choose=bic} aurait donné le meilleur résultat pour prévoir le montant acheté des clients restants (de ceux qui auraient acheté quelque chose). Le deuxième meilleur aurait été la moyenne des modèles.

Il y aurait plusieurs autres approches/combinaisons qui pourraient être testées. Le but de ce chapitre était simplement de présenter les principes de base en sélection de modèles et de variables ainsi que certaines approches pratiques. Il y a d'autres approches intéressantes, tels le LASSO et LARS (\emph{least-angle regression}) qui sont disponibles dans \texttt{glmselect}. Ces méthodes sont dans la même mouvance moderne que celle qui consiste à faire la moyenne de plusieurs modèles, en performant à la fois une sélection de variables et en permettant d'avoir des parties d'effet par le rétrécissement (\emph{shrinkage}). De récents développements théoriques permettent de corriger les valeurs-\emph{p} pour faire de l'inférence post-sélection.

Il faut bien comprendre qu'il ne s'agit que d'un seul exemple: il ne faut surtout pas conclure que la méthode séquentielle de \texttt{glmselect} avec les options \texttt{select=aic} et \texttt{choose=bic} sera toujours la meilleure. En fait, il est impossible de prévoir quelle méthode donnera les meilleurs résultats.

\hypertarget{ruxe9gression-logistique-1}{%
\chapter{Régression logistique}\label{ruxe9gression-logistique-1}}

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

En régression linéaire, on cherche à expliquer le comportement d'une variable quantitative \(Y\) que l'on peut traiter comme étant continue (elle peut prendre suffisamment de valeurs différentes).

Supposons à présent que l'on veut expliquer le comportement d'une variable \(Y\) prenant seulement deux valeurs que l'on va noter 0 et 1.

Exemples :

\begin{itemize}
\tightlist
\item
  Est-ce qu'un client potentiel va répondre favorablement à une offre promotionnelle?
\item
  Est-ce qu'un client est satisfait du service après-vente?
\item
  Est-ce qu'un client va faire faillite ou non au cours des trois prochaines années.
\end{itemize}

En général, on cherchera à expliquer le comportement d'une variable binaire \(Y\) en utilisant un modèle basé sur p variables quelconques \(X_1, \ldots, X_p\).

Notre but sera de faire de l'inférence, de la prédiction, ou les deux à la fois, soit

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Comprendre comment et dans quelles mesures les variables \(\boldsymbol{X}\) influencent \(Y\) (ou bien la probabilité que \(Y=1\)).
\item
  Prédiction : développer un modèle pour prévoir des valeurs de \(Y\) futures à partir des variables \(\boldsymbol{X}\).
\end{enumerate}

\hypertarget{moduxe8le-de-ruxe9gression-logistique}{%
\section{Modèle de régression logistique}\label{moduxe8le-de-ruxe9gression-logistique}}

Avec une variable réponse continue, le modèle de régression linéaire,
\begin{align*}
 Y = \beta_0 + \beta_1X_1 + \cdots + \beta_p X_p + \varepsilon,
\end{align*}
avec \({\mathsf E}\left(\varepsilon\mid \boldsymbol{X}\right)=0\) et \({\mathsf{Var}}\left(\varepsilon\mid \boldsymbol{X}\right)=\sigma^2\), peut être écrit de manière équivalente comme \({\mathsf E}\left(Y \mid \boldsymbol{X}\right) = \beta_0 + \beta_1X_1 + \cdots + \beta_pX_p\) et \({\mathsf{Var}}\left(Y \mid \boldsymbol{X}\right)=\sigma^2.\)

Si \(Y\) est binaire (0/1), on peut facilement vérifier que
\begin{align*}
{\mathsf E}\left(Y \mid \boldsymbol{X}\right) = {\mathsf P}\left(Y=1 \mid  \boldsymbol{X}\right),
\end{align*}
soit la probabilité que \(Y\) égale 1 étant donné les valeurs des variables explicatives. Pour simplifier la notation, posons \(p = {\mathsf P}\left(Y=1 \mid \boldsymbol{X}\right)\) en se rappelant que \(p\) est une fonction des variables explicatives.

À première vue, on peut se demander pourquoi ne pas utiliser le même modèle que la régression linéaire, c'est-à-dire
\begin{align*}
\eta=\beta_0 + \beta_1X_1 + \cdots + \beta_p X_p.
\end{align*}

Le problème est que \(p\) est une probabilité. Par conséquent \(p\) prend seulement des valeurs entre 0 et 1 alors que rien n'empêche \(\eta\) de prendre des valeurs dans \(\mathbb{R}=(-\infty, \infty)\). Une façon de résoudre ce problème consiste à appliquer une transformation à \(p\) de telle sorte que la quantité transformée puisse prendre toutes les valeurs entre \(-\infty\) et \(\infty\).
Le modèle de régression logistique est défini à l'aide de la transformation \(\mathrm{logit}\),
\[\mathrm{logit}(p) = \ln\left( \frac{p}{1-p}\right)=\eta=\beta_0 + \beta_1X_1 + \cdots + \beta_p X_p,\]
où \(\ln\) est le logarithme naturel.

En régression linéaire, on suppose que l'espérance de \(Y\) étant donné les valeurs des variables explicatives est une combinaison linéaire de ces dernières. En régression logistique, on suppose que le logit de la probabilité que \(Y=1\) étant donné les valeurs des variables explicatives est une combinaison linéaire de ces dernières.

Une simple manipulation algébrique permet d'exprimer ce modèle en terme de la probabilité \(p\),
\begin{align*}
 p &= \mathrm{expit}(\eta) = \frac{\exp(\eta)}{1+\exp(\eta)}
= \frac{1}{1+\exp(-\eta)}.
\end{align*}
On peut voir qu'à mesure que le prédicteur linéaire \(\eta=\beta_0+\beta_1X_1 + \cdots + \beta_pX_p\) augmente, la probabilité augmente.
Si le coefficient \(\beta_j\) est négatif, \(p\) diminuera à mesure que \(X_j\) augmente.

\begin{center}\includegraphics{MATH60602_files/figure-latex/logitplot-1} \end{center}

Pour une variable binaire \(Y\), le quotient \(p/(1-p)\) est appelé \textbf{cote} et représente le ratio de la probabilité de succès (\(Y=1\)) sur la probabilité d'échec (\(Y=0\)),
\begin{align*}
 \mathsf{cote}(p) = \frac{p}{1-p} = \frac{{\mathsf P}\left(Y=1 \mid \boldsymbol{X}\right)}{{\mathsf P}\left(Y=0 \mid \boldsymbol{X}\right)}.
\end{align*}

Par exemple, une cote de 4 veut dire qu'il y a 4 fois plus de chance que \(Y\) soit égale à \(1\) par rapport à \(0\). Une cote de 0,25 veut dire le contraire, il y a 4 fois moins de chance que \(Y=1\) par rapport à \(0\) ou bien, de manière équivalente, il y a 4 fois plus de chance que \(Y=0\) par rapport à \(1\). Le Tableau \ref{tab:03-cotes} donne un aperçu de cotes pour quelques probabilités \(p\).

\begin{longtable}[]{@{}rccccccccc@{}}
\caption{\label{tab:03-cotes} Cote et probabilité de succès}\tabularnewline
\toprule
\begin{minipage}[b]{0.06\columnwidth}\raggedleft
\({\mathsf P}\left(Y=1\right)\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,1\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,2\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,3\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,4\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,5\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,6\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,7\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,8\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,9\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.06\columnwidth}\raggedleft
\({\mathsf P}\left(Y=1\right)\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,1\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,2\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,3\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,4\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,5\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,6\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,7\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,8\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
0,9\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
cote\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0,11\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0,25\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0,43\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0,67\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1,5\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
2,33\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
4\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
9\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedleft
cote (frac.)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(\frac{1}{9}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(\frac{1}{4}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(\frac{3}{7}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(\frac{2}{3}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(1\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(\frac{3}{2}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(\frac{7}{3}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(4\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\(9\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{estimation-des-paramuxe8tres}{%
\section{Estimation des paramètres}\label{estimation-des-paramuxe8tres}}

\hypertarget{principes-de-base}{%
\subsection{Principes de base}\label{principes-de-base}}

On dispose d'un échantillon de taille \(n\) sur les variables \((Y, X_1, \ldots, X_p)\), dans le tableau
\begin{align*}
 \begin{pmatrix}
 x_{11} & x_{12} & \cdots & x_{1p} & y_1 \\
 x_{21} & \ddots & \cdots & x_{2p} & y_2 \\
 \vdots & \ddots & \ddots & \ddots & \vdots \\
 x_{n1} & x_{n2} & \cdots & x_{np} & y_n \\
 \end{pmatrix}
\end{align*}
À l'aide de ces observations, on peut estimer les paramètres \(\boldsymbol{\beta} = (\beta_0, \beta_1 ,\ldots, \beta_p)\) du modèle de régression logistique
\begin{align*}
\mathrm{logit}(p) = \ln \left( \frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_pX_p.
\end{align*}
On obtient ainsi les estimés des paramètres \(\widehat{\boldsymbol{\beta}}\), desquels découle une estimation de \({\mathsf P}\left(Y=1\right)\) pour les valeurs \(X_1=x_1, \ldots, X_p=x_p\) d'un individu donné,
\begin{align*}
 \widehat{p} = \mathrm{expit}(\widehat{\beta}_0 + \cdots + \widehat{\beta}_pX_p).
\end{align*}

Un modèle ajusté peut ensuite être utilisé pour faire de la classification (prédiction) pour de nouveaux individus pour lesquels la variable réponse \(Y\) n'est pas observée. Pour ce faire, on choisit un point de coupure \(c\) (souvent \(c=0,5\) mais pas toujours) et on classifie les observations en deux groupes:

\begin{itemize}
\tightlist
\item
  Si \(\widehat{p}< c\), alors \(\widehat{Y}=0\) (c'est-à-dire, on assigne cette observation à la catégorie 0).
\item
  Si \(\widehat{p} \geq c\), alors \(\widehat{Y}=1\) (c'est-à-dire, on assigne cette observation à la catégorie 1).
\end{itemize}

On reviendra en détail sur cet aspect dans une section suivante.

La méthode d'estimation des paramètres habituellement utilisée est la méthode du maximum de vraisemblance. Pour les applications, il est suffisant de savoir manipuler trois quantités importantes: la log-vraisemblance, le \(\mathsf{AIC}\) et le \(\mathsf{BIC}\). Les deux critères d'information, que nous avons couvert dans les chapitres précédent, servent à la sélection de modèles tandis que la log-vraisemblance \(\ell\)servira à construire un test d'hypothèse.

\hypertarget{muxe9thode-du-maximum-de-vraisemblance}{%
\subsection{Méthode du maximum de vraisemblance}\label{muxe9thode-du-maximum-de-vraisemblance}}

Cette sous-section est facultative. Elle donne plus de détails sur la méthode du maximum de vraisemblance et les quantités en découlant (\(\mathsf{AIC}\), \(\mathsf{BIC}\) et \(\ell(\widehat{\boldsymbol{\beta}})\)).

La méthode du maximum de vraisemblance (\emph{maximum likelihood}) est possiblement la méthode d'estimation la plus utilisée en statistique. En général, pour un échantillon donné et un modèle avec des paramètres inconnus \(\boldsymbol{\theta}\), on peut calculer la « probabilité » d'avoir obtenu les observations de notre échantillon selon les paramètre. Si on traite cette « probabilité » comme étant une fonction des paramètres du modèle, \(\boldsymbol{\theta}\), on l'appelle alors la vraisemblance (\emph{likelihood}). La méthode du maximum de vraisemblance consiste à trouver les valeurs des paramètres qui maximisent la vraisemblance. On cherche donc les estimations qui sont les plus vraisemblables étant donné nos observations.

En pratique, il est habituellement plus simple de chercher à maximiser le log de la vraisemblance (ce qui revient au même car le log est une fonction croissante) et on nomme cette fonction la log-vraisemblance (\emph{log-likelihood}).

Vous connaissez déjà des exemples d'estimateurs du maximum de vraisemblance. La moyenne d'un échantillon est l'estimateur du maximum de vraisemblance pour la moyenne de la population \(\mu\) si les observations représentent un échantillon aléatoire simple tiré d'une loi normale.

Dans le cas d'un modèle de régression linéaire multiple \(Y = \beta_0 + \sum_{j=1}^p \beta_jX_j + \varepsilon\) avec les erreurs \(\varepsilon\sim \mathcal{N}(0, \sigma^2)\) des termes indépendants et identiquement distributions, alors la log-vraisemblance du modèle pour un échantillon de taille \(n\) est
\begin{align*}
 \ell(\boldsymbol{\beta}, \sigma^2) =- \frac{n}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (Y_i- \beta_0 - \beta_1 X_{1i} - \cdots - \beta_pX_{ip})^2.
\end{align*}
Puisque le premier terme ne dépend pas des paramètres \(\boldsymbol{\beta}\), il est clair que maximiser cette fonction de \(\boldsymbol{\beta}\) revient à minimiser \(\sum_{i=1}^n (Y_i- \beta_0 - \beta_1 X_{1i} - \cdots - \beta_pX_{ip})^2\), et ce critère est exactement le même que celui des moindres carrés. Par conséquent, les estimations des paramètres \(\boldsymbol{\beta}\) provenant de la méthode des moindres carrés peuvent être vues comme étant des estimateurs du maximum de vraisemblance sous l'hypothèse de normalité des observations. De plus, il est même possible d'écrire une formule explicite pour ces estimations.

Dans le cas de la régression logistique, la fonction de log-vraisemblance s'écrit
\begin{align*}
 \ell(\boldsymbol{\beta}) = \sum_{i=1}^n Y_i ( \beta_0 + \beta_1 X_{i1} + \cdots + \beta_p X_{ip}) - \sum_{i=1}^n \ln\left\{1+\exp(\beta_0 + \cdots + \beta_pX_{ip})\right\}
\end{align*}

Contrairement au cas de la régression linéaire, on ne peut trouver une fonction explicite pour les valeurs des paramètres qui maximisent cette fonction. Des méthodes numériques doivent alors être utilisées pour l'optimisation. Une fois la maximisation accomplie, on obtient les estimés du maximum de vraisemblance, \(\widehat{\boldsymbol{\beta}}\). On peut alors calculer la valeur maximale (numérique) de la log-vraisemblance, \(\ell(\widehat{\boldsymbol{\beta}})\). La quantité \(-2\ell(\widehat{\boldsymbol{\beta}})\) (\texttt{-2\ Log\ L}) est rapportée dans les sorties \textbf{SAS}. Par analogie avec la régression linéaire la valeur de la log-vraisemblance évaluée à \(\widehat{\boldsymbol{\beta}}\), \(\ell(\widehat{\boldsymbol{\beta}})\), augmente toujours lorsqu'on ajoute des régresseurs et c'est pourquoi on ne pourra pas l'utiliser comme outil de sélection de variables.

Les critères d'information sont fonctions de a log-vraisemblance et sont
\begin{align*}
 \mathsf{AIC} & = -2 \ell(\widehat{\boldsymbol{\beta}}) + 2(p+1)\\
 \mathsf{BIC} & = -2 \ell(\widehat{\boldsymbol{\beta}}) + \ln(n)(p+1)
\end{align*}

Ces définitions sont utilisables dans plusieurs situations lorsque le modèle est ajusté par la méthode du maximum de vraisemblance. En particulier, elles sont utilisées par \textbf{SAS} en régression logistique. Tout comme en régression linéaire et analyse factorielle, ces deux critères pourront être utilisés pour faire de la sélection de modèles si on calcule les estimateurs du maximum de vraisemblance.

\hypertarget{exemple-du-professional-rodeo-cowboys-association}{%
\section{\texorpdfstring{Exemple du \emph{Professional Rodeo Cowboys Association}}{Exemple du Professional Rodeo Cowboys Association}}\label{exemple-du-professional-rodeo-cowboys-association}}

L'exemple suivant est inspiré de l'article

\begin{quote}
Daneshvary, R. et Schwer, R. K. (2000) The Association Endorsement and Consumers' Intention to Purchase. \emph{Journal of Consumer Marketing} \textbf{17}, 203-213.
\end{quote}

Dans cet article, les auteurs cherchent à voir si le fait qu'un produit soit recommandé par le \emph{Professional Rodeo Cowboys Association} (PRCA) a un effet sur les intentions d'achats. On dispose de 500 observations sur les variables suivantes:

\begin{itemize}
\tightlist
\item
  \(Y\): seriez-vous intéressé à acheter un produit recommandé par le PRCA

  \begin{itemize}
  \tightlist
  \item
    \(\texttt{0}\): non
  \item
    \(\texttt{1}\): oui
  \end{itemize}
\item
  \(X_1\): quel genre d'emploi occupez-vous?

  \begin{itemize}
  \tightlist
  \item
    \(\texttt{1}\): à la maison
  \item
    \(\texttt{2}\): employé
  \item
    \(\texttt{3}\): ventes/services
  \item
    \(\texttt{4}\): professionnel
  \item
    \(\texttt{5}\): agriculture/ferme
  \end{itemize}
\item
  \(X_2\): revenu familial annuel

  \begin{itemize}
  \tightlist
  \item
    \(\texttt{1}\): moins de 25 000
  \item
    \(\texttt{2}\): 25 000 à 39 999
  \item
    \(\texttt{3}\): 40 000 à 59 999
  \item
    \(\texttt{4}\): 60 000 à 79 999
  \item
    \(\texttt{5}\): 80 000 et plus
  \end{itemize}
\item
  \(X_3\): sexe

  \begin{itemize}
  \tightlist
  \item
    \(\texttt{0}\): homme
  \item
    \(\texttt{1}\): femme
  \end{itemize}
\item
  \(X_4\): avez-vous déjà fréquenté une université?

  \begin{itemize}
  \tightlist
  \item
    \(\texttt{0}\): non
  \item
    \(\texttt{1}\): oui
  \end{itemize}
\item
  \(X_5\): âge (en années)
\item
  \(X_6\): combien de fois avez-vous assisté à un rodéo au cours de la dernière année?

  \begin{itemize}
  \tightlist
  \item
    \(\texttt{1}\): 10 fois ou plus
  \item
    \(\texttt{2}\): entre six et neuf fois
  \item
    \(\texttt{3}\): cinq fois ou moins
  \end{itemize}
\end{itemize}

Le but est d'examiner les effets de ces variables sur l'intentions d'achat (\(Y\)). Les données se trouvent dans le fichier \texttt{logit1.sas7bdat}.

\hypertarget{moduxe8le-avec-une-seule-variable-explicative}{%
\subsection{Modèle avec une seule variable explicative}\label{moduxe8le-avec-une-seule-variable-explicative}}

Faisons tout d'abord une analyse en utilisant seulement \(X_5\) (âge) comme variable explicative. L'ajustement du modèle de régression incluant uniquement \(X_5\) sera effectuée en exécutant le programme

\begin{verbatim}
proc logistic data=multi.logit1 ;
model y(ref='0') = x5 / clparm=pl clodds=pl expb;
run;
\end{verbatim}

Le fichier \texttt{logit1\_intro.sas} contient ce programme et décrit plus en détail les différentes options. La syntaxe \texttt{y(ref=\textquotesingle{}0\textquotesingle{})} sert à spécifier la catégorie de référence, zéro, de la variable réponse \(Y\): le modèle décrit donc \({\mathsf P}\left(y=1 \mid X_5\right)\).

Voici une partie de la sortie

\begin{center}\includegraphics[width=0.63\linewidth]{figures/03-logistic-e1} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e2} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e3} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e4} \end{center}

\begin{itemize}
\tightlist
\item
  On voit qu'il y a 272 personnes (\(\texttt{0}\)) qui ne sont pas intéressées à acheter un produit recommandé par le PRCA et 228 personnes (\(\texttt{1}\)) qui le sont.
\item
  Les estimés des paramètres sont \(\widehat{\beta}_0 = -3,05\) et \(\widehat{\beta}_{\texttt{age}}=0,0749\).
\item
  Un intervalle de confiance de niveau 95\% pour l'effet de l'âge est {[}\(0,0465; 0,1043\){]}.
\item
  Le modèle ajusté est \(\mathrm{logit}\{{\mathsf P}\left(Y=1 \mid X_5=x_5\right)\} = -3,05 + 0,0749 x_5\). On peut également exprimer ce modèle directement en terme de la probabilité de succès,
  \begin{align*}
  {\mathsf P}\left(Y=1 \mid X_5=x_5\right) &= \mathrm{expit}(-3,05 + 0,0749 x_5) \\&= \frac{1}{1+\exp(3,05 - 0,0749 x_5)}
  \end{align*}
  Le graphe de cette fonction pour \(X_5\) allant de 18 à 59 ans, respectivement les valeurs minimales et maximales observées dans l'échantillon, montre que le lien entre l'âge et \(p\) est presque linéaire entre 20 et 60 ans. On décèle tout de même la forme sigmoide de la fonction \(\mathrm{logit}\) aux deux extrémités.
\end{itemize}

\begin{center}\includegraphics{MATH60602_files/figure-latex/logitplot2-1} \end{center}

\begin{itemize}
\tightlist
\item
  La valeur-\(p\) pour \(\widehat{\beta}_{\texttt{age}}\) (\texttt{Pr\ \textgreater{}\ khi-2}), correspondant aux test des hypothèses \(\mathcal{H}_0: \beta_{\texttt{age}}=0\) versus \(\mathcal{H}_1: \beta_{\texttt{age}} \neq 0\), est plus petite que \(10^{-4}\) et donc l'effet de la variable âge est statistiquement différent de zéro. Plus l'âge augmente, plus la probabilité d'être intéressé à acheter un produit recommandé par le PRCA augmente.
\item
  Le tableau \texttt{Test\ de\ l\textquotesingle{}hypothèse\ nulle\ globale\ :\ BETA=0} contient les résultats de trois tests pour l'hypothèse nulle que tous les paramètres sont nuls, contre l'alternative qu'au moins un des paramètres est différent de zéro. Comme il y a un seul paramètre ici, ces tests reviennent à tester l'effet de la variable âge. Le test de Wald est le même que celui que nous venons de voir dans le tableau des coefficients.
\end{itemize}

\hypertarget{interpruxe9tation-du-paramuxe8tre}{%
\subsection{Interprétation du paramètre}\label{interpruxe9tation-du-paramuxe8tre}}

Si une variable est modélisée à l'aide d'un seul paramètre (pas de terme quadratique et pas d'interaction avec d'autre covariables), une valeur positive du paramètre indique une association positive avec \(p\) alors qu'une valeur négative indique le contraire.

Ainsi, le signe du paramètre donne le sens de l'association. Si le coefficient \(\beta_j\) de la variable \(X_j\) est positif, alors plus la variable augmente, plus \({\mathsf P}\left(Y=1\right)\) augmente. Inversement, Si le coefficient \(\beta_j\) est négatif, plus la variable augmente, plus \({\mathsf P}\left(Y=1\right)\) diminue.

En régression linéaire, l'interprétation de coefficient \(\beta_j\) est simple: lorsque la variable \(X_j\) augmente de un, la variable \(Y\) augmente en moyenne de \(\beta_j\), toute chose étant égale par ailleurs. Cette interprétation ne dépend pas de la valeur de \(X_j\). En régression logistique, comme le modèle est nonlinéaire en fonction de \({\mathsf P}\left(Y=1\right)\) (courbe sigmoide), l'augmentation ou la dimininution de \({\mathsf P}\left(Y=1\mid \boldsymbol{X}\right)\) pour un changement d'une unité de \(X_j\) dépend de la valeur de cette dernière. C'est pourquoi il est parfois plus utile d'utiliser la cote pour interpréter globalement l'effet d'une variable.

Dans notre exemple, on peut exprimer le modèle ajusté en termes de cote,
\begin{align*}
 \frac{{\mathsf P}\left(Y=1 \mid X_5=x_5\right)}{{\mathsf P}\left(Y=0 \mid X_5=x_5\right)} = \exp(-3,05)\exp(0,0749x_5).
\end{align*}
Ainsi, lorsque \(X_5\) augmente d'une année, la cote est multipliée par \(\exp(0,0749) = 1,078\) peut importe la valeur de \(x_5\). Pour deux personnes dont la différence d'âge est un an, la cote de la personne plus âgée est 7,8\% plus élevée. On peut aussi quantifier l'effet d'une augmentation d'un nombre d'unités quelconque. Par exemple, pour chaque augmentation de 10 ans de \(X_5\), la cote est multiplié par \(1,078^{10} = 2,12\), soit une augmentation de 112\%.

La cote est rapportée à la dernière colonne du tableau des coefficients. En général, si on veut une interprétation globale de l'effet d'une variable, il faudra baser l'interprétation sur l'exponentielle du coefficient, \(\exp(\widehat{\beta})\). \textbf{SAS} dénomme cette quantité rapport de cote (\emph{odds ratio}).

Un des avantages d'utiliser la vraisemblance comme fonction objective est que les intervalles de confiance et les estimateurs basés sur la vraisemblance (profilée) sont invariant aux reparametrisations. Ainsi, l'intervalle de confiance à niveau 95\% pour \(\exp(\beta_{\texttt{age}})\) est obtenu en prenant l'exponentielle des bornes de l'intervalle pour \(\beta_{\texttt{age}}\), {[}\(\exp(0,0465); \exp(0,1043)\){]}, soit {[}\(1,048; 1,110\){]} tel que rapporté dans la sortie. Ce n'est \textbf{pas} le cas des intervalles de Wald qui ont la forme \(\widehat{\beta} \pm 1.96 \mathrm{se}(\widehat{\beta})\).
Comme l'exponentielle est une transformation monotone croissante, on a \(\beta>0\) si et seulement si \(\exp(\beta)>1\), etc. On peut ainsi utiliser les intervalles de confiance pour tester l'hypothèse \(\mathcal{H}_0: \beta_j=0\) ou de façon équivalente \(\mathcal{H}_0: \exp(\beta_j)=1\) à niveau 95\%.

\hypertarget{moduxe8le-avec-toutes-les-variables-explicatives}{%
\subsection{Modèle avec toutes les variables explicatives}\label{moduxe8le-avec-toutes-les-variables-explicatives}}

Ajustons à présent le modèle avec toutes les variables explicatives. Rappelez-vous que la variable \(X_1\) (quel genre d'emploi occupez-vous) a cinq catégories, \(X_2\) (revenu familial annuel) a cinq catégories, et \(X_6\) (combien de fois avez-vous assisté à un rodéo au cours de la dernière année) a trois catégories. Il faut donc spécifier à \textbf{SAS} de les traiter comme des variables catégorielles dans le modèle. Notez qu'on pourrait aussi traiter \(X_2\) comme continue car elle est ordinale et possède tout de même cinq modalités, mais on la traitera comme variable nominale.

\begin{verbatim}
proc logistic data=multi.logit1 ;
class x1(ref=last) x2(ref=last) x6 / param=ref;
model y(ref='0') =x1-x6 / clparm=pl clodds=pl expb;
run;
\end{verbatim}

Dans \textbf{SAS}, les variables incluses dans la commande \texttt{class} sont modélisées à l'aide d'un ensemble de variables indicatrices. Cette commande nous évite de créer nous-même les indicatrices; cette option est disponible dans la plupart des procédures \textbf{SAS}, bien que la procédure \texttt{reg} est une exception notable.

On peut changer la catégorie de référence (\texttt{ref=}) qui est par défaut la dernière modalité (en ordre alphanumérique). L'option \texttt{param=ref} pour \texttt{class} permet d'imprimer un tableau indiquant le code pour les variables indicatrices.
Les variables incluses dans la commande \texttt{class} sont modélisées à l'aide d'un ensemble de variables indicatrices. Prenons l'exemple de la variable \(X_1\): la modalité de référence est (\texttt{5}), soit agriculture est spécifiée dans le tableau \texttt{Informations\ sur\ les\ niveaux\ de\ classe}.

\begin{center}\includegraphics[width=0.75\linewidth]{figures/03-logistic-e9} \end{center}

Le fichier \texttt{logit1\_intro.sas} contient le code pour ajuster le même modèle sans la commande \texttt{class}, c'est-à-dire en créant nous-mêmes les variables indicatrices pour inclure les variables explicatives catégorielles. Vous pouvez l'exécuter afin de vous convaincre qu'il s'agit du même modèle. Les estimés seront les mêmes.

\begin{center}\includegraphics[width=0.65\linewidth]{figures/03-logistic-e5} \end{center}

\begin{center}\includegraphics[width=0.75\linewidth]{figures/03-logistic-e6} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{figures/03-logistic-e7} \end{center}

\begin{center}\includegraphics[width=1\linewidth]{figures/03-logistic-e8} \end{center}

Le modèle ajusté est
\begin{align*}
 \mathrm{logit}\{{\mathsf P}\left(Y=1 \mid \boldsymbol{X}=\boldsymbol{x}\right)\} &= -6,89 + 0,36{\mathbf 1}_{X_1=1} - 0,47{\mathbf 1}_{X_1=2} - 0,31{\mathbf 1}_{X_1=3} - 0,32{\mathbf 1}_{X_1=4} \\& \qquad 
+ 1,33{\mathbf 1}_{X_2=1} + 1,15{\mathbf 1}_{X_2=2} + 0,77{\mathbf 1}_{X_2=3} - 1,11{\mathbf 1}_{X_2=4} \\&\qquad 
+ 1,35X_3+ 1,83X_4
+ 0,11X_5
+ 2,41{\mathbf 1}_{X_6=1} + 1,04{\mathbf 1}_{X_6=2}
\end{align*}

Notez que les variables \({\mathbf 1}_{X_1=1}\) (\texttt{x11}), \({\mathbf 1}_{X_1=21}\) (\texttt{x12}), \({\mathbf 1}_{X_1=3}\) (\texttt{x13}) et \({\mathbf 1}_{X_1=4}\) (\texttt{x14}) représentent les quatre indicatrices pour la variable \(X_1\) (et de même pour \(X_2\) et \(X_6\)). L'interprétation se fait comme en régression linéaire multiple. Ici, il n'y a pas de terme quadratique, ni d'interaction. Les paramètres estimés représentent donc l'effet de la variable correspondante sur le logit une fois que les autres variables sont dans le modèle, et demeurent fixes.

Prenons le coefficient associé à l'âge (\(X_5\)) comme exmple. Le paramètre estimé est \(\widehat{\beta}_{\texttt{age}}=0,1095\) et il est significativement différent de zéro. Ainsi, plus l'âge augmente, plus \({\mathsf P}\left(Y=1\mid \boldsymbol{X}\right)\) augmente, toutes autres choses étant égales par ailleurs. Pour chaque augmentation d'un an de \(X_5\), la cote est multipliée par \(\exp(0,1095)=1,116\), lorsque les autres variables demeurent fixes.

N'oubliez pas la nuance suivante concernant l'interprétation d'un test lorsque plusieurs variables explicatives font partie du modèle. Si un paramètre n'est pas significativement différent de zéro, cela ne veut pas dire qu'il n'y a pas de lien entre la variable correspondante et \(Y\). Cela veut seulement dire qu'il n'y a pas de lien significatif une fois que les autres variables sont dans le modèle.

Prenons l'exemple de la variable \(X_6\), qui représente le nombre de fois où l'individu a assisté à un rodéo au cours de la dernière année. Cette variable est modélisée à l'aide de deux variables indicatrices, \({\mathbf 1}_{X_6=1}\) égale à un si \(X_6=1\) et zéro autrement, et \({\mathbf 1}_{X_6=2}\) égale à un si \(X_6=2\) et zéro sinon. La catégorie de référence est \(X_6=3\), c'est-à-dire les personnes ayant assisté cinq fois ou moins à un rodéo au cours de la dernière année. Pour tester la significativité globale d'une variable catégorielle qui est modélisée avec plusieurs indicatrices, il faut aller dans le tableau \texttt{Analyse\ des\ effets\ Type\ 3}. On voit que la statistique de test est \(42,9364\) et que la valeur-\(p\) associée est négligeable: la variable \(X_6\) est donc globalement significative. En fait, il s'agit du test conjoint sur toutes les indicatrices associées à cette variable. Plus précisément, il s'agit du test de l'hypothèse nulle \(\mathcal{H}_0: \beta_{6_{\texttt{1}}}=\beta_{6_{\texttt{2}}}=0\) versus la contre-hypothèse qu'au moins un de ces deux paramètres est différent de zéro.

L'interprétation des variables catégorielles est analogue à celle faite en régression linéaire. On peut aussi interpréter individuellement les paramètres des indicatrices: pour \({\mathbf 1}_{X_6=1}\), lorsque les autres variables demeurent fixes, les personnes ayant assisté 10 fois ou plus à un rodéo au cours de la dernière année voient leur cote multipliée par \(\exp(2,4122)=11,158\) par rapport aux personnes ayant assisté cinq fois ou moins. Ce paramètre est significativement différent de zéro car sa valeur-\(p\) est négligeable (tableau \texttt{Analyse\ des\ valeurs\ estimées\ du\ maximum\ de\ vraisemblance}); l'intervalle de confiance à 95\% pour le rapport de cotes, basé sur la vraisemblance profilée, est {[}\(5,456; 23,882\){]} et un n'est pas dans l'intervalle. Ainsi, il y a une différence significative entre les gens qui ont assisté à 10 rodéos ou plus et les gens qui ont assisté à 5 rodéos ou moins, pour ce qui est de l'intérêt à acheter un produit recommandé par le PRCA.

On procède de la même façon pour \({\mathbf 1}_{X_6=2}\): lorsque les autres variables demeurent fixes, les personnes ayant assisté entre six et neuf fois à un rodéo au cours de la dernière année voient leur cote multipliée par \(2,842\) par rapport aux personnes ayant assisté cinq fois ou moins. Ce paramètre est aussi significativement différent de zéro. Il y a donc une progression. Plus une personne a assisté à un grand nombre de rodéo au cours de la dernière année, plus elle est intéressée à acheter un produit recommandé par la PRCA.

Si on désire comparer les deux modalités \(X_6=1\) et \(X_6=2\), il suffit de changer la modalité de référence dans la commande \texttt{class} et d'exécuter le modèle à nouveau. Une alternative est de calculer le rapport (de rapport) de cotes pour ces deux modalités.

\hypertarget{test-du-rapport-de-vraisemblance}{%
\subsection{Test du rapport de vraisemblance}\label{test-du-rapport-de-vraisemblance}}

Les tests correspondants aux valeurs-\(p\) dans le tableau des paramètres sont des tests de Wald. Ces tests feront l'affaire dans la plupart des applications. Par contre, il existe un autre test qui est généralement plus puissant, c'est-à-dire qu'il sera meilleur pour détecter que \(\mathcal{H}_0\) n'est pas vraie lorsque c'est effectivement le cas. Ce test est le test du rapport de vraisemblance (\emph{likelihood ratio test}). Il découle de la méthode d'estimation du maximum de vraisemblance et est donc généralement applicable lorsqu'on estime les paramètres avec cette méthode. Il est basé sur la quantité \(\ell\) que nous avons vue plus tôt.

La procédure consiste à ajuster deux modèles \textbf{imbriqués}:

\begin{itemize}
\tightlist
\item
  Le premier modèle, le modèle complet, contient tous les paramètres et l'estimateur du maximum de vraisemblance \(\widehat{\boldsymbol{\beta}})\).
\item
  Le deuxième modèle correspondant à l'hypothèse nulle \(\mathcal{H}_0\), le modèle réduit, contient tous les paramètres avec les restrictions imposées sous \(\mathcal{H}_0\); on dénote l'estimateur du maximum de vraisemblance \(\widehat{\boldsymbol{\beta}}_0\)
\end{itemize}

Le test est basé sur la statistique
\begin{align*}
 D = -2\{\ell(\widehat{\boldsymbol{\beta}}_0)-\ell(\widehat{\boldsymbol{\beta}})\}
\end{align*}
ou la différence entre \texttt{-2\ Log\ L} pour le modèle réduit et \texttt{-2\ Log\ L} pour le modèle complet. Cette différence \(D\), lorsque l'hypothèse \(\mathcal{H}_0\) est vraie suit approximativement une loi khi-deux avec un nombre de degrés de liberté égal au nombre de paramètre testé (le nombre de restrictions sous \(\mathcal{H}_0\)). On peut donc calculer la valeur-\(p\) en utilisant la distribution du khi-deux.

Prenons comme exemple le test de la significativité de \(X_6\), qui est modélisée à l'aide deux variables binaires \({\mathbf 1}_{X_6=1}\) et \({\mathbf 1}_{X_6=2}\) et dont les paramètres correspondants sont \(\beta_{6_{\texttt{1}}}\) et \(\beta_{6_{\texttt{2}}}\). Nous avons déjà étudié la sortie pour le test de Wald de significativité globale de \(X_6\), soit le test de l'hypothèse \(\mathcal{H}_0: \beta_{6_{\texttt{1}}}=\beta_{6_{\texttt{2}}}=0\) versus l'alternative qu'au moins un de ces deux paramètres est différent de zéro. La statistique de test (de Wald) est 42,93 et la valeur-\(p\) est moins de \(10^{-4}\). Pour effectuer le test du rapport de vraisemblance, il suffit de retirer la variable \(X_6\) et de réajuster le modèle à nouveau avec toutes les autres variables; cette manipulation est effectuée dans \texttt{logit1\_intro.sas}. On obtient donc
\texttt{-2\ Log\ L} de 516,196 pour le modèle complet sans contrainte et \(566,447\) pour le modèle excluant la variable \(X_6\).

La différence \(D = 566,447 - 516,196 = 50,25\). Il s'agit de la statistique du test de rapport de vraisemblance. La valeur-\(p\) peut-être obtenue de la loi du khi-deux avec 2 degrés de liberté via le code suivant permet d'imprimer la valeur-\(p\), qui est \(1,22 \times 10^{-11}\).

\begin{verbatim}
data pval;
pval=1-CDF('CHISQ', 566.447 - 516.196, 2);
run;
proc print data=pval;
run;
\end{verbatim}

Comme la statistique du test de rapport de vraisemblance \(D=50,25\) est encore plus grande est encore plus grande que la statistique de Wald (\(42,9364\)), qui suit la même loi de probabilité sous \(\mathcal{H}_0\), cela indique que le test du rapport de vraisemblance est encore plus significatif que le test de Wald. Cela ne fait pas de différence ici mais, dans certains cas, il est possible que le test de Wald ne soit pas significatif (valeur-\(p\) plus grande que \(0,05\)) tandis que le test du rapport de vraisemblance le soit (valeur-\(p\) inférieure à \(0,05\)).

\hypertarget{multicolinuxe9arituxe9}{%
\subsection{Multicolinéarité}\label{multicolinuxe9arituxe9}}

Rappelez-vous que le terme multicolinéarité fait référence à la situation où les variables explicatives sont très corrélées entre elles ou bien, plus généralement, à la situation où une (ou plusieurs) variable(s) explicative(s) est (sont) très corrélée(s) à une combinaison linéaire des autres variables explicatives.

L'effet potentiellement néfaste de la multicolinéarité est le même qu'en régression linéaire, c'est-à-dire, elle peut réduire la précision des estimations des paramètres (augmenter leurs écarts-types estimés).

En pratique, le problème est qu'il devient difficile de départager l'effet individuel d'une variable explicative lorsqu'elle est fortement corrélée avec d'autres variables explicatives.

Comme la multicolinéarité est une propriété des variables explicatives (le \(Y\) n'intervient pas) on peut utiliser les mêmes outils qu'en régression linéaire pour tenter de la détecter, par exemple, le facteur d'inflation de la variance (\emph{variance inflation factor}). Cette quantité ne dépend que des variables explicatives \(\boldsymbol{X}\), pas du modèle ou de la variable réponse.

La multicolinéarité est surtout un problème lorsque vient le temps d'interpréter et tester l'effet des paramètres individuels. Si le but est seulement de faire de la classification (prédiction) et que l'interprétation des paramètres individuels n'est pas cruciale alors il n'y a pas lieu de se soucier de la multicolinéarité. Il faut alors plutôt comparer correctement la performance de classification des modèles en utilisant des méthodes permettant d'obtenir un bon modèle tout en se protégeant contre le surajustement. Certaines de ces méthodes (division de l'échantillon, validation croisée) ont déjà été présentées.

\hypertarget{classification-et-pruxe9diction-uxe0-laide-de-la-ruxe9gression-logistique}{%
\section{Classification et prédiction à l'aide de la régression logistique}\label{classification-et-pruxe9diction-uxe0-laide-de-la-ruxe9gression-logistique}}

La finalité du modèle de régression logistique est fréquemment l'obtention de prédictions. Une fois qu'on a ajusté un modèle, on peut l'utiliser pour prévoir la valeur de \(Y\) pour de nouvelles observations. Ceci consiste à assigner une classe (\(0\) ou \(1\)) à ces observations (pour lesquels \(Y\) est inconnue) à partir des valeurs prises par \(X_1, \ldots, X_p\).

Le modèle ajusté nous fournit une estimation de \({\mathsf P}\left(Y=1 \mid \boldsymbol{X}=\boldsymbol{x}\right)\) pour des valeurs \(X_1=x_1, \ldots, X_p=x_p\) données. Cet estimé est
\begin{align*}
 \widehat{p} = \frac{1}{1+ \exp\{- ( \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \cdots + \widehat{\beta}_p x_p)\}}.
\end{align*}

Classification de base: pour classifier des observations, il suffit de choisir un point de coupure \(c\), souvent \(c=0,5\), et de classifier une observation de la manière suivante:

\begin{itemize}
\tightlist
\item
  Si \(\widehat{p} < c\), on assigne cette observation à la catégorie zéro et \(\widehat{Y}=0\).
\item
  Si \(\widehat{p} \geq c\), on assigne cette observation à la catégorie un et \(\widehat{Y}=1\).
\end{itemize}

Si on prend \(c=0,5\) comme point de coupure, cela revient à assigner l'observation à la classe (catégorie) la plus probable, un choix fort raisonnable. Nous verrons dans une section suivante que, lorsque les conséquences de faussement classifier une observation (succès, mais échec prédit et vice-versa) ne sont pas les mêmes, il peut être avantageux d'utiliser un autre point de coupure.

Dans un cadre de prédiction, il nous faudra un critère pour juger de la qualité de l'ajustement du modèle.
Rappelez-vous que pour une réponse continue, nous avons utilisé l'erreur moyenne quadratique,
\(\mathsf{EMQ} = \mathsf{E}\{(Y-\widehat{Y})^2\}\),
pour juger de la performance d'un modèle. Comme la réponse \(Y\) est binaire ici, nous allons utiliser des critères différents.

Voyons d'abord un premier critère pour juger de la qualité d'un modèle de prédiction. Soit \(Y\) la vraie valeur de la réponse binaire et \(\widehat{Y}\) (soit 0 ou 1) la valeur de \(Y\) prédite par un modèle pour une observation choisie au hasard dans la population. Un premier critère pour juger de la performance d'un modèle est \({\mathsf P}\left(Y \neq\widehat{Y}\right)\), soit la probabilité de mal classifier une observation choisie au hasard dans
la population. Ce critère est le \textbf{taux de mauvaise classification}. Plus \({\mathsf P}\left(Y \neq\widehat{Y}\right)\) est petite, meilleure est la capacité prédictive du modèle.

Tout comme l'erreur moyenne quadratique, on ne peut pas calculer exactement le taux de mauvaise classification; tout au plus peut-on l'estimer. Pour les raisons vues au chapitre précédent, l'estimer en
calculant le taux de mauvaise classification des observations ayant servi à
l'ajustement du modèle sans aucune correction n'est pas une bonne approche.
Les approches couvertes dans le dernier chapitre pour l'estimation de l'erreur moyenne quadratique, telles la validation-croisée et la
division de l'échantillon, peuvent être utilisées pour estimer le taux de mauvaise classification \({\mathsf P}\left(Y \neq \widehat{Y}\right)\).

Cette utilisation d'un modèle de régression logistique sera illustrée avec l'exemple que nous avons traité au chapitre précédent: notre objectif final est de construire un modèle avec les 1000 clients de l'échantillon d'apprentissage et cibler ensuite lesquels des 100 000 clients restants seront choisis pour recevoir le catalogue. Les variables cibles sont:

\begin{itemize}
\tightlist
\item
  \texttt{yachat}: variable binaire égale à un si le client a acheté quelque chose dans le catalogue et zéro sinon.
\item
  \texttt{ymontant}: le montant de l'achat si le client a acheté quelque chose
\end{itemize}

Les 10 variables suivantes sont disponibles pour tous les clients et serviront de variables explicatives,

\begin{itemize}
\tightlist
\item
  \texttt{x1}: sexe de l'individu, soit homme (0) ou femme (1);
\item
  \texttt{x2}: l'âge (en année);
\item
  \texttt{x3}: variable catégorielle indiquant le revenu, soit moins de 35 000\$ (1), entre 35 000\$ et 75 000\$ (2) ou plus de 75 000\$ (3);
\item
  \texttt{x4}: variable catégorielle indiquant la région où habite le client (de 1 à 5);
\item
  \texttt{x5}: conjoint : le client a-t-il un conjoint, soit oui (1) ou non (0);
\item
  \texttt{x6}: nombre d'année depuis que le client est avec la compagnie;
\item
  \texttt{x7}: nombre de semaines depuis le dernier achat;
\item
  \texttt{x8}: montant (en dollars) du dernier achat;
\item
  \texttt{x9}: montant total (en dollars) dépensé depuis un an;
\item
  \texttt{x10}: nombre d'achats différents depuis un an.
\end{itemize}

Dans le chapitre précédent, nous avons cherché à développer un modèle pour prévoir \texttt{ymontant}, le montant dépensé, étant donné que le client achète quelque chose. Cette fois-ci, nous allons travailler avec la variable \texttt{yachat}, qui est binaire, à l'aide de la régression logistique.

Afin d'introduire différentes notions, nous allons, dans un premier temps, utiliser les 10 variables de base. À partir de la section suivante, nous chercherons à optimiser le modèle en considérant les interactions d'ordre deux. Pour ce faire, nous utiliserons des méthodes de sélections de variables. Les commandes se trouvent dans le fichier \texttt{logit2\_classification\_base.sas}. Dans le code qui suit, le fichier \texttt{train} contient les 1000 clients de l'échantillon d'apprentissage et le fichier \texttt{test} contient les 100 000 clients pour lesquels on veut prédire l'intention d'achat.

\begin{verbatim}
proc logistic data=train;
model yachat(ref='0') = x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10;
output out=pred predprobs=crossvalidate;
run;
\end{verbatim}

Le modèle utilise seulement les 10 variables de base (en fait 14 avec les indicatrices pour les variables catégorielles). Des prévisions pour les clients restants seront exportées dans le fichier \texttt{pred}, grâce à la commande \texttt{score}. L'option \texttt{ctable} permet d'obtenir la \texttt{Table\ de\ classification} (sic). Tel que nous l'avons vu au chapitre précédent, il y a 210 clients qui ont acheté quelque chose parmi les 1000. Le tableau de classification contient des estimations de plusieurs quantités intéressantes, en faisant varier le point de coupure (\texttt{Niveau\ de\ proba}). Pour chaque point de coupure, ces estimations ont été obtenues à l'aide d'une approximation de la méthode de validation croisée à \(n\) groupes (en anglais, \emph{leave-one-out cross-validation}, ou LOOCV). Ainsi, ces estimations sont valides car elles ne sont pas obtenues en utilisant les mêmes observations que celles qui ont servi à estimer le modèle.

\includegraphics{MATH60602_files/figure-latex/classification-1.pdf}

coupe

vrai positif

vrai négatif

faux positif

faux négatif

correct (\%)

sensibilité (\%)

spécificité (\%)

faux positif (\%)

faux négatif (\%)

0.02

210

209

581

0

41.9

100.0

26.5

73.5

0.0

0.04

207

320

470

3

52.7

98.6

40.5

69.4

0.9

0.06

201

398

392

9

59.9

95.7

50.4

66.1

2.2

0.08

199

451

339

11

65.0

94.8

57.1

63.0

2.4

0.10

193

480

310

17

67.3

91.9

60.8

61.6

3.4

0.12

191

512

278

19

70.3

91.0

64.8

59.3

3.6

0.14

184

547

243

26

73.1

87.6

69.2

56.9

4.5

0.16

176

572

218

34

74.8

83.8

72.4

55.3

5.6

0.18

172

598

192

38

77.0

81.9

75.7

52.7

6.0

0.20

164

611

179

46

77.5

78.1

77.3

52.2

7.0

0.22

162

626

164

48

78.8

77.1

79.2

50.3

7.1

0.24

158

639

151

52

79.7

75.2

80.9

48.9

7.5

0.26

153

645

145

57

79.8

72.9

81.6

48.7

8.1

0.28

150

657

133

60

80.7

71.4

83.2

47.0

8.4

0.30

143

667

123

67

81.0

68.1

84.4

46.2

9.1

0.32

138

679

111

72

81.7

65.7

85.9

44.6

9.6

0.34

134

695

95

76

82.9

63.8

88.0

41.5

9.9

0.36

130

699

91

80

82.9

61.9

88.5

41.2

10.3

0.38

126

708

82

84

83.4

60.0

89.6

39.4

10.6

0.40

120

715

75

90

83.5

57.1

90.5

38.5

11.2

0.42

115

723

67

95

83.8

54.8

91.5

36.8

11.6

0.44

112

731

59

98

84.3

53.3

92.5

34.5

11.8

0.46

109

736

54

101

84.5

51.9

93.2

33.1

12.1

0.48

106

739

51

104

84.5

50.5

93.5

32.5

12.3

0.50

100

744

46

110

84.4

47.6

94.2

31.5

12.9

0.52

98

748

42

112

84.6

46.7

94.7

30.0

13.0

0.54

92

750

40

118

84.2

43.8

94.9

30.3

13.6

0.56

87

753

37

123

84.0

41.4

95.3

29.8

14.0

0.58

83

761

29

127

84.4

39.5

96.3

25.9

14.3

0.60

80

766

24

130

84.6

38.1

97.0

23.1

14.5

0.62

77

769

21

133

84.6

36.7

97.3

21.4

14.7

0.64

74

771

19

136

84.5

35.2

97.6

20.4

15.0

0.66

68

772

18

142

84.0

32.4

97.7

20.9

15.5

0.68

62

774

16

148

83.6

29.5

98.0

20.5

16.1

0.70

54

775

15

156

82.9

25.7

98.1

21.7

16.8

0.72

51

777

13

159

82.8

24.3

98.4

20.3

17.0

0.74

49

778

12

161

82.7

23.3

98.5

19.7

17.1

0.76

46

778

12

164

82.4

21.9

98.5

20.7

17.4

0.78

41

781

9

169

82.2

19.5

98.9

18.0

17.8

0.80

35

783

7

175

81.8

16.7

99.1

16.7

18.3

0.82

33

783

7

177

81.6

15.7

99.1

17.5

18.4

0.84

32

783

7

178

81.5

15.2

99.1

17.9

18.5

0.86

28

784

6

182

81.2

13.3

99.2

17.6

18.8

0.88

25

786

4

185

81.1

11.9

99.5

13.8

19.1

0.90

21

787

3

189

80.8

10.0

99.6

12.5

19.4

0.92

18

787

3

192

80.5

8.6

99.6

14.3

19.6

0.94

14

788

2

196

80.2

6.7

99.7

12.5

19.9

0.96

6

788

2

204

79.4

2.9

99.7

25.0

20.6

0.98

2

790

0

208

79.2

1.0

100.0

0.0

20.8

Ce tableau contient des estimations de plusieurs quantités intéressantes, en faisant varier le point de coupure (\texttt{Niveau\ de\ proba} dans le tableau \textbf{SAS}). Pour chaque point de coupure, ces estimations ont été obtenues à l'aide de la validation-croisée \(n\) observations, soit autant de groupes que d'observations. Ainsi, ces estimations sont valides car elles ne sont pas obtenues en utilisant les mêmes observations que celles qui ont servi à estimer le modèle.
La colonne \texttt{correct} donne une estimation du taux de bonne classification, \({\mathsf P}\left(Y = \widehat{Y}\right) = 1-{\mathsf P}\left(Y \neq \widehat{Y}\right)\), ou de manière équivalente un moins le taux de mauvaise classification.

Avec un point de coupure de \(0\), on classifie toutes les observations à la classe achat (\(1\)), car \(\widehat{p}\) est forcément plus grande que zéro. Le taux de bonne classification dans ce cas de figure sera de \(21\)\%, puisque 210 individus ont acheté un produit dans le catalogue dans l'échantillon d'apprentissage.
L'autre extrême, avec un point de coupure \(c=1\), donne un taux de bonne classification de \(79\)\%.

On peut chercher dans le tableau les points de coupure qui donnent le meilleur taux de bonne classification. Ce dernier est de 84,6\% et est atteint par trois points de coupure, soit 0,52, soit 0,6, soit 0,62. Une recherche plus fine donne 0,465 comme point de coupure optimal, avec un taux de mauvaise classification de 15,3\%.

La \textbf{matrice de confusion}, qui compare les vraies valeurs avec les prédictions, peut être construite à partir des colonnes \texttt{Correct\ -\ Événement}, \texttt{Correct\ -\ Non-événement}, \texttt{Incorrect\ -\ Événement}, \texttt{Incorrect\ -\ Non-événement}. Il y a deux classifications possibles et le tableau contient, en partant du coin supérieur gauche et dans le sens des aiguilles d'une montre, le nombre de vrai positif (\(Y=1\), \(\widehat{Y}=1\)), de faux positif (\(Y=0\), \(\widehat{Y}=1\)), de vrai négatif (\(Y=0\), \(\widehat{Y}=0\)) et finalement de faux négatif (\(Y=1\), \(\widehat{Y}=0\)). Ces nombres proviennent de la validation croisée à \(n\) groupes et ne sont pas ceux qu'on obtiendrait si on appliquait directement le modèle ajusté à notre échantillon. Le taux de classification est \((\mathsf{FP}+\mathsf{FN})/n\).

\begin{table}

\caption{\label{tab:confumat}Matrice de confusion avec point de coupure 0.465}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & \$Y=1\$ & \$Y=0\$\\
\hline
\$\textbackslash{}widehat\{Y\}=1\$ & 109 & 52\\
\hline
\$\textbackslash{}widehat\{Y\}=0\$ & 101 & 738\\
\hline
\end{tabular}
\end{table}

Quatre autres quantités, dérivées à partir de la matrice de confusion, sont parfois utilisées:

\begin{itemize}
\tightlist
\item
  la \textbf{sensibilité} (\emph{sensitivity}), \({\mathsf P}\left(\widehat{Y}=1 \mid Y=1\right)\), ou \(\mathsf{VP}/(\mathsf{VP}+\mathsf{FN})\);
\item
  la \textbf{spécificité} (\emph{specificity}), \({\mathsf P}\left(\widehat{Y}=0 \mid Y=0\right)\), ou \(\mathsf{VN}/(\mathsf{VN}+\mathsf{FP})\);
\item
  le \textbf{taux de faux positifs}, \({\mathsf P}\left(Y=0 \mid \widehat{Y}=1\right)\), ou \(\mathsf{FP}/(\mathsf{VP}+\mathsf{FP})\);
\item
  le \textbf{taux de faux négatifs}, \({\mathsf P}\left(Y=1 \mid \widehat{Y}=0\right)\), ou \(\mathsf{FN}/(\mathsf{VN}+\mathsf{FN})\).
\end{itemize}

Les estimés empiriques sont simplement obtenus en calculant les rapports du nombre d'observations dans chaque classe. \textbf{SAS} rapporte ces quantités, mais notez les vieilles versions retournent le taux de faux positifs et de faux négatifs dans les deux dernières colonnes, tandis que les sorties des nouvelles version du logiciel donnent les taux de vrais positifs et de vrais négatifs.

La sensibilité mesure à quel point notre modèle est performant pour détecter un vrai positif (classe 1). La spécificité mesure à quel point notre modèle est performant pour détecter un résultat négatif (classe 0).

Plus le point de coupure augmente, plus la sensibilité et le taux de faux positifs diminuent mais plus la spécificité et le taux de faux négatifs augmentent.

La \textbf{fonction d'efficacité du récepteur}, parfois appelée courbe ROC (\emph{receiver operating characteristic}) est parfois utilisée pour représenter globalement la performance du modèle. Elle est obtenue avec l'option \texttt{plots(only)=(roc)} dans \textbf{SAS}. Il s'agit du graphe de la sensibilité en fonction de un moins la spécificité, en faisant varier le point de coupure. Un modèle parfait aurait une sensibilité et une spécificité égales à 1 (correspondant au coin supérieur gauche de la fonction d'efficacité du récepteur). Ainsi, plus le couple (\(1-\)spécificité, sensibilité) est près de (\(0\), \(1\)), meilleur est le modèle. Par conséquent, plus la courbe ROC tend vers (\(0\), \(1\)) meilleur est le pouvoir prévisionnel des variables.

L'\textbf{aire sous la courbe} (\emph{area under the curve}) est souvent utilisée en parallèle
est simplement l'aire sous la courbe de la fonction d'efficacité du récepteur. Pour le modèle logistique ajusté, on a une aire sous la courbe de 0,8847. Plus cette valeur est élevée (au plus \(1\)), mieux c'est.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e11} \end{center}

La courbe ROC et la valeur de l'aire sous la courbe (avec l'option \texttt{plots(only)=(roc)}), sont calculées avec les données d'apprentissage et ne sont pas corrigées. Si on veut les utiliser pour comparer des modèles, il faut plutôt utiliser l'option \texttt{crossvalidate} qui permet d'obtenir des estimations des probabilités par validation-croisée avec \(n\) groupes tout comme celle utilisée dans le tableau de classification.

\begin{verbatim}
proc logistic data=train;
model yachat(ref='0') = x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10;
output out=pred predprobs=crossvalidate;
run;

proc logistic data=pred ;
model yachat(ref='0') = x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10;
roc pred=xp_1;
run;
\end{verbatim}

On sauvegarde d'abord les probabilités estimées par validation-croisée dans le
fichier \texttt{pred} avec la commande \texttt{output\ out=pred\ predprobs=crossvalidate}
La variable \texttt{xp\_1} désigne cette probabilité dans le fichier \texttt{pred}. Ensuite, on
exécute de nouveau la procédure \texttt{logistic} avec ce fichier et la commande \texttt{roc}.
L'aire sous la courbe pour les prédictions avec la validation-croisée à \(n\) groupes est 0,8723: cet estimé est légèrement inférieur à celui obtenu sans
la correction (trop optimiste) qui est 0,8847.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e15} \end{center}

Un autre type de graphe qui est souvent utilisé dans des contextes de gestion
est la courbe lift (sic) (en anglais, \emph{lift chart}). Cette courbe est obtenue en ordonnant les probabilités de succès estimées par le modèle, \(\widehat{p}\), en ordre croissant et en regardant quelle pourcentage de ces derniers seraient bien classifiés (le nombre de vrais positifs sur le nombre de succès).

\textbf{SAS} ne permet pas de la tracer directement, mais le fichier \texttt{logit3\_lift\_chart.sas} contient une macro \textbf{SAS} qui permet de le faire.

\begin{verbatim}
proc logistic data=train;
model yachat(ref='0') = x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10; 
output out=pred predprobs=crossvalidate;
run;
%liftchart1(pred,yachat,xp_1,10);
\end{verbatim}

Ici, le tableau présente les 10 déciles. Si on classifiait comme acheteurs les 10\% qui ont la plus forte probabilité estimée d'achat, on détecterait 79 des 210 clients (37,6\%). En comparaison, on s'attend que 21 clients soient sélectionnés en moyenne si on prend un échantillon aléatoire de 100 personnes. Le ratio 79/21 (dernière colonne) est le \emph{lift} du modèle: il permet de détecter 3,76 fois plus de succès que le hasard.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e16} \end{center}

Le graphe @ref(fig:fig3\_e17) présente le pourcentage d'observations bien classées parmi les variables (pourcentage des probabilités prédites qui correspondent à un succès parmi les \(k\) plus susceptibles selon le modèle). La référence est la ligne diagonale, qui correspond à une détection aléatoire.

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.8\linewidth]{figures/03-logistic-e17}

\}

\caption{Taux de classement en fonction du lift}

(\#fig:fig3\_e17)
\textbackslash{}end\{figure\}

\hypertarget{classification-avec-une-matrice-de-gain}{%
\section{Classification avec une matrice de gain}\label{classification-avec-une-matrice-de-gain}}

Utiliser le taux de mauvaise classification \({\mathsf P}\left(Y \neq \widehat{Y}\right)\), comme critère de performance, revient au même que d'utiliser le taux de bonne classification \({\mathsf P}\left(Y=\widehat{Y}\right)\), car \({\mathsf P}\left(Y \neq \widehat{Y}\right) = 1-{\mathsf P}\left(Y=\widehat{Y}\right)\). On veut un modèle avec un haut taux de bonne classification (ou un faible taux de mauvaise classification).

Lorsqu'on utilise \({\mathsf P}\left(Y \neq \widehat{Y}\right)\) comme critère pour juger de la qualité d'un modèle prévisionnel, on fait l'hypothèse que le gain associé à bien classifier une observation dans la catégorie 0 lorsqu'elle est réellement dans la catégorie 0 est le même que celui associé à classifier une observation dans la catégorie 1 lorsqu'elle est réellement dans la catégorie 1: cela correspond à la matrice de gain

\begin{longtable}[]{@{}llrr@{}}
\caption{\label{tab:03-gain1} Matrice de gain correspondant au taux de bonne classification}\tabularnewline
\toprule
& & observation &\tabularnewline
\midrule
\endfirsthead
\toprule
& & observation &\tabularnewline
\midrule
\endhead
& gain & \(Y=1\) & \(Y=0\)\tabularnewline
prédiction & \(\widehat{Y}=1\) & 1 & 0\tabularnewline
& \(\widehat{Y}=0\) & 0 & 1\tabularnewline
\bottomrule
\end{longtable}

C'est-à-dire, le gain vaut 1 lorsque la prévision est bonne (les deux cas sur la diagonale) et 0 lorsque le modèle se trompe (les deux autres cas). L'unité de mesure du gain n'est pas importante pour l'instant. Le gain total est

\begin{align*}
\text{gain} &= 1 {\mathsf P}\left(\widehat{Y}=1, Y=1\right) + 1 {\mathsf P}\left(\widehat{Y}=0, Y=0\right) 
\\ &\quad + 0 {\mathsf P}\left(\widehat{Y}=1, Y=0\right)  + 0 {\mathsf P}\left(\widehat{Y}=0, Y=1\right)
\\& = {\mathsf P}\left(Y = \widehat{Y}\right).
\end{align*}
Maximiser le gain total revient donc à maximiser le taux de bonne classification.

Dans certaines situations, les gains (ou la perte si le gain est négatif) associés aux bonnes décisions et aux erreurs ne sont pas équivalents. Par exemple, un des types d'erreurs peut être plus grave que l'autre. Il peut alors être souhaitable d'en tenir compte dans le choix du modèle de classification.

Supposons que le gain de classer une observation à \(i\) (\(i \in \{0,1\}\)) lorsqu'elle vaut \(j\) (\(j \in \{0,1\}\)) en réalité est de \(c_{ij}\). La matrice de gain est alors

\begin{longtable}[]{@{}llrr@{}}
\caption{\label{tab:03-gain2} Matrice de gain pondérée en fonction d'un coût}\tabularnewline
\toprule
& & observation &\tabularnewline
\midrule
\endfirsthead
\toprule
& & observation &\tabularnewline
\midrule
\endhead
& gain & \(Y=1\) & \(Y=0\)\tabularnewline
prédiction & \(\widehat{Y}=1\) & \(c_{11}\) & \(c_{10}\)\tabularnewline
& \(\widehat{Y}=0\) & \(c_{01}\) & \(c_{00}\)\tabularnewline
\bottomrule
\end{longtable}

En pratique, l'une de ces quatre quantités peut être fixée à un car seulement les poids relatifs (les ratios) des gains sont importants. Dans ce cas, le gain moyen est
\begin{align*}
\text{gain} &= c_{11} {\mathsf P}\left(\widehat{Y}=1, Y=1\right) + c_{00}{\mathsf P}\left(\widehat{Y}=0, Y=0\right) 
\\ &\quad + c_{10} {\mathsf P}\left(\widehat{Y}=1, Y=0\right)  + c_{01} {\mathsf P}\left(\widehat{Y}=0, Y=1\right)
\end{align*}

Le meilleur modèle est alors celui qui maximise le gain moyen. Le fichier \texttt{logit4\_macro\_gain.sas} contient des macros \textbf{SAS} qui permettent d'estimer le gain moyen à l'aide de la validation croisée.

Nous allons encore une fois seulement utiliser les 10 variables de base. Mais nous allons intégrer des revenus et coûts afin de trouver le meilleur point de coupure. Rappelez-vous que le coût de l'envoi d'un catalogue est de 10\$. Le tableau des variables descriptives qui suit montre que, pour les 210 clients qui ont acheté quelque chose, le revenu moyen est de 67,29\$ (moyenne de la variable \texttt{ymontant}).

\begin{center}\includegraphics[width=0.6\linewidth]{figures/03-logistic-e18} \end{center}

Nous allons travailler en termes de revenu net. Nous pouvons donc spécifier la matrice de gain du tableau \ref{tab:03-gain3} pour notre problème. Si on n'envoit pas de catalogue, notre gain est nul. Si on envoie le catalogue à un client qui n'achète pas, on perd 10\$ (le coût de l'envoi). En revanche, notre revenu net est de 57\$ (revenu moyen moins coût de l'envoi).

\begin{longtable}[]{@{}llll@{}}
\caption{\label{tab:03-gain3} Matrice de gain pour l'envoi de catalogue}\tabularnewline
\toprule
& & observation &\tabularnewline
\midrule
\endfirsthead
\toprule
& & observation &\tabularnewline
\midrule
\endhead
& gain & \(Y=1\) & \(Y=0\)\tabularnewline
prédiction & \(\widehat{Y}=1\) & 57 & \(-10\)\tabularnewline
& \(\widehat{Y}=0\) & 0 & 0\tabularnewline
\bottomrule
\end{longtable}

L'appel de la macro \texttt{manycut\_cvlogistic}, dont les paramètres sont expliqués dans le script, se fait de la manière suivante:

\begin{verbatim}
%manycut_cvlogistic(yvar=yachat,xvar=x1  x2 x31 x32 x41 x42 x43 x44  x5  x6 x7 x8 x9 x10, n=1000,k=10,ncv=10,dataset=train,c00=0,c01=0,c10=-10,c11=57,
manycut=.05 .06 .07 .08 .09 .1 .11 .12 .13 .14 .15 .16 .17 .18 .5);
\end{verbatim}

Cette macro produit le tableau suivant. Il donne l'estimation du gain moyen (\texttt{gain}) pour différents points de coupures (\texttt{cutpoint}). Cette estimation provient d'une validation-croisée avec 10 groupes (\texttt{k=10} dans la macro). En fait, on a répété 10 fois (\texttt{ncv=10} dans la macro) la validation croisée avec 10 groupes et fait la moyenne des 10 répétitions afin d'avoir plus de précisions. Il faut essayer plusieurs points de coupure afin de trouver le meilleur.

On voit que le meilleur point de coupure, celui qui maximise le gain est 0,12. Avec ce point de coupure, on estime que le taux de bonne classification est de 0,707 et que la sensitivité est de 0,899. Ainsi, on estime qu'on va détecter 90\% des clients qui achètent.

\begin{center}\includegraphics[width=0.6\linewidth]{figures/03-logistic-e19} \end{center}

On est loin du point de coupure usuel de 0,5 (présenté à la dernière ligne). La raison est simple. Comme il est très coûteux de rater un client qui aurait acheté quelque chose, il est préférable d'envoyer le catalogue à plus de clients, quitte à ce que plusieurs d'entre eux n'achètent rien. En fait, le point de coupure de 0,5 donne un meilleur taux de bonne classification mais un gain moyen plus faible car on rate trop de clients qui achètent (la sensitivité est seulement de 48,8\%). Travailler avec la matrice de gain permet de trouver le point de coupure optimal en incorporant des notions de coûts et profits.

Ici, nous avons ajusté un seul modèle, celui contenant uniquement les 10 variables de base et nous nous sommes attardés au choix du point de coupure pour l'assignation aux classes. Il est possible qu'un autre modèle, contenant par exemple des termes d'interactions, des termes quadratiques ou d'autres transformations des variables, soit supérieur à celui-ci. Le choix du modèle de prévision se fait donc souvent en deux étapes

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  trouver les bonnes variables
\item
  trouver le bon point de coupure.
\end{enumerate}

Nous avons déjà vu des méthodes de sélections de variables au chapitre précédent. La section suivante reviendra sur ces méthodes dans le contexte de la régression logistique.

\hypertarget{suxe9lection-de-variables-en-ruxe9gression-logistique}{%
\section{Sélection de variables en régression logistique}\label{suxe9lection-de-variables-en-ruxe9gression-logistique}}

Les principes généraux, concernant la sélection de variables et de modèles, que nous avons vus au chapitre précédent sont toujours valides. Les critères \(\mathsf{AIC}\) et \(\mathsf{BIC}\) sont toujours disponibles puisqu'on estime le modèle par maximum de vraisemblance et les techniques générales de division de l'échantillon et de validation-croisée sont toujours valides. Nous allons voir comment appliquer spécifiquement ces techniques au cas de la régression logistique.

\hypertarget{recherche-exhaustive-de-tous-les-sous-moduxe8les}{%
\subsection{Recherche exhaustive de tous les sous-modèles}\label{recherche-exhaustive-de-tous-les-sous-moduxe8les}}

Rappelez-vous qu'avec une variable cible continue, nous avons utilisé avec la procédure \texttt{reg} pour faire une recherche du meilleur sous-ensemble de variables parmi tous les ensembles. Pour ce faire, on sélectionnait le meilleur modèle selon le \(R^2\) pour un nombre de variables fixe et il suffisait ensuite de trouver parmi ces variables le meilleur selon le critère d'information choisi.

Une stratégie similaire est possible avec la procédure \texttt{logistic} sauf qu'au lieu de trouver une suite de meilleurs modèles selon le \(R^2\), un autre critère est retenu. Il s'agit de la statistique du score (ou test des multiplicateurs de Lagrange) pour tester si l'ajout d'une variable est utile ou pas.

Ce paragraphe est plus technique et peut être omis. Parce qu'il n'y a pas de solution explicite pour les estimateurs du maximum de vraisemblance du modèle logistique, ajuster chacun de ces modèles est coûteux. La statistique de score, qui est basée sur la vraisemblance, ne nécessite que d'obtenir le maximum de vraisemblance sous l'hypothèse nulle; cela permet d'éviter des ajustements coûteux lors de comparaisons. L'algorithme employé par \textbf{SAS} utilise une méthode de recherche arborescente dite méthode de séparation et d'évaluation, qui ne nécessite pas de tester tous les modèles; à noter que la solution à \(k\) variables n'est pas nécessairement imbriquée dans celle à \(k+1\) variables. Lorsque la taille d'échantillon tend vers l'infini, la statistique du rapport de vraisemblance et la statistique de score sont équivalentes. Choisir le modèle selon la statistique du score équivaut alors à choisir le modèle qui maximise la vraisemblance (ou qui minimise la quantité \(-2 \ell\)). Ainsi, pour ce nombre fixé de variables, cela va donner le modèle avec le meilleur \(\mathsf{AIC}\) (et \(\mathsf{BIC}\)). Par conséquent, on peut trouver le meilleur modèle, globalement, en minimisant le \(\mathsf{AIC}\) (ou le \(\mathsf{BIC}\)) en faisant varier le nombre de variables. Par contre, cela n'est pas nécessairement vrai pour une taille d'échantillon finie. Le meilleur modèle selon le critère score n'est pas nécessairement celui qui maximise la vraisemblance. Mais en pratique, cette approximation est plus que suffisante et on va procéder comme on a fait avec la procédure \texttt{reg}.

À la section précédente, nous avons inclus les 10 variables de base (14 avec les indicatrices pour les variables catégorielles) dans notre exemple d'envoi ciblé. Nous allons ici faire une recherche de type \texttt{all-subset} parmi ces 14 variables. Le code est dans le fichier \texttt{logit5\_selection\_variables.sas}.

\begin{verbatim}
proc logistic data=train;
model yachat(ref='0') = x1 x2 x31 x32 x41 x42 x43 x44 x5 
    x6 x7 x8 x9 x10 / selection=score best=1;
run;
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e20} \end{center}

Le meilleur modèle avec une seule variable, selon la statistique du score, est celui avec \texttt{x8}, le meilleur avec deux variables est celui avec \texttt{x2} et \texttt{x8}, et ainsi de suite. Nous voulons ensuite choisir parmi ces 14 modèles, celui qui minimise le \(\mathsf{AIC}\) ou le \(\mathsf{BIC}\). Le problème est que ces critères ne sont pas fournis (contrairement aux sorties de la procédure \texttt{reg}). La solution longue consiste à ajuster chacun de ces modèles, à extraire le \(\mathsf{AIC}\) et le \(\mathsf{BIC}\) et à ainsi trouver le meilleur modèle. Mais le faire manuellement en spécifiant plusieurs modèles est trop long. La macro \texttt{logistic\_aic\_BIC\_score}, qui se trouve dans le fichier \texttt{logit6\_macro\_all\_subset.sas} ajuste tous ces modèles automatiquement.

\begin{verbatim}
%logistic_aic_BIC_score(yvariable=yachat,
                        xvariables=x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10,
                        dataset=train, minvar=1, maxvar=14);
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e21} \end{center}

On voit que le meilleur modèle selon le \(\mathsf{AIC}\) a neuf variables, contre sept pour le \(\mathsf{BIC}\). Nous verrons plus loin, dans un tableau synthèse, comment auraient performé ces modèles s'ils avaient été utilisés pour cibler les clients restants.

\hypertarget{recherche-suxe9quentielle}{%
\subsection{Recherche séquentielle}\label{recherche-suxe9quentielle}}

Faire une recherche de tous les sous-modèles possibles devient impraticable lorsqu'il y a trop de variables en jeu. La procédure \texttt{logistic} permet aussi une recherche de type séquentielle classique. Ceci permet aussi d'utiliser la même approche en deux temps présentée au chapitre précédent. Dans un premier temps, on fait une recherche séquentielle pour sélectionner un nombre de variables qui sera assez petit afin qu'une recherche exhaustive de tous les sous-modèles soit possible. Dans un second temps, on fait cette recherche avec ces variables uniquement.

Si on inclut tous les termes quadratiques et les termes d'interactions d'ordre deux, nous avons 104 variables potentielles: c'est trop pour une recherche exhaustive.

Faisons d'abord une recherche séquentielle classique avec les 104 variables, en prenant 0,05 comme critère d'entrée et de sortie. Le code est

\begin{verbatim}
proc logistic data=train;
model yachat(ref='0') = x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10
cx2 cx6 cx7 cx8 cx9 cx10
i_x2_x1 i_x2_x5 i_x2_x31 i_x2_x32 i_x2_x41 i_x2_x42 i_x2_x43 i_x2_x44
i_x2_x7 i_x2_x6 i_x2_x8 i_x2_x9 i_x2_x10
i_x1_x5 i_x1_x31 i_x1_x32 i_x1_x41 i_x1_x42 i_x1_x43 i_x1_x44
i_x1_x7 i_x1_x6 i_x1_x8 i_x1_x9 i_x1_x10
i_x5_x31 i_x5_x32 i_x5_x41 i_x5_x42 i_x5_x43 i_x5_x44
i_x5_x7 i_x5_x6 i_x5_x8 i_x5_x9 i_x5_x10
i_x31_x41 i_x31_x42 i_x31_x43 i_x31_x44
i_x31_x7 i_x31_x6 i_x31_x8 i_x31_x9 i_x31_x10
i_x32_x41 i_x32_x42 i_x32_x43 i_x32_x44
i_x32_x7 i_x32_x6 i_x32_x8 i_x32_x9 i_x32_x10
i_x41_x7 i_x41_x6 i_x41_x8 i_x41_x9 i_x41_x10
i_x42_x7 i_x42_x6 i_x42_x8 i_x42_x9 i_x42_x10
i_x43_x7 i_x43_x6 i_x43_x8 i_x43_x9 i_x43_x10
i_x44_x7 i_x44_x6 i_x44_x8 i_x44_x9 i_x44_x10
i_x7_x6 i_x7_x8 i_x7_x9 i_x7_x10
i_x6_x8 i_x6_x9 i_x6_x10
i_x8_x9 i_x8_x10
i_x9_x10 
  / selection=stepwise  sle=.05 sls=.05;  
run;
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e22} \end{center}

Il y a eu 17 étapes dans la recherche séquentielle. La variable \texttt{i\_x7\_x8} est la première à être incluse, suivie de \texttt{x2} et ainsi de suite. Le modèle final contient 13 variables.
En faisant une recherche séquentielle avec 0,5 comme comme critère d'entrée et de sortie, nous obtenons un sous-ensemble de 47 variables. On peut ensuite faire une recherche exhaustive de tous les sous-modèles avec ces 47 variables en utilisant la macro; le fichier \texttt{logit5\_selection\_variables.sas} contient le code commenté.

Le modèle qui a le plus petit \(\mathsf{AIC}\), soit 579,701, est un modèle avec 28 variables. Le \(\mathsf{BIC}\) mène à un modèle beaucoup plus parcimonieux qui inclut huit variables.

\hypertarget{algorithme-glouton-et-crituxe8res-alternatifs-avec-hpgenselect}{%
\subsection{\texorpdfstring{Algorithme glouton et critères alternatifs avec \texttt{hpgenselect}}{Algorithme glouton et critères alternatifs avec hpgenselect}}\label{algorithme-glouton-et-crituxe8res-alternatifs-avec-hpgenselect}}

Nous avons vu au chapitre précédent que la procédure \texttt{glmselect} permet de faire une recherche de type séquentielle avec un critère autre que la valeur-\(p\) du test de Wald pour rajouter ou retirer des variables explicatives du modèle final. Cette procédure est limitée à la régression linéaire, mais la procédure \texttt{hpgenselect} permet de faire une sélection de variables pour d'autres types de modèles, incluant la régression logistique. Cette fonction n'est pas disponible dans \textbf{SAS University Edition}.

Le code suivant fait une recherche séquentielle en ajoutant ou retranchant les variables selon leur valeur-\(p\) (\texttt{select=sl}), la seule méthode disponible pour l'instant. En revanche, le modèle final peut-être choisi selon d'autres critères.

\begin{verbatim}
proc hpgenselect data=train;
model yachat(ref='0') = x1 x2 x31 x32 x41 x42 x43 x44 x5 x6 x7 x8 x9 x10
cx2 cx6 cx7 cx8 cx9 cx10
i_x2_x1 i_x2_x5 i_x2_x31 i_x2_x32 i_x2_x41 i_x2_x42 i_x2_x43 i_x2_x44
i_x2_x7 i_x2_x6 i_x2_x8 i_x2_x9 i_x2_x10
i_x1_x5 i_x1_x31 i_x1_x32 i_x1_x41 i_x1_x42 i_x1_x43 i_x1_x44
i_x1_x7 i_x1_x6 i_x1_x8 i_x1_x9 i_x1_x10
i_x5_x31 i_x5_x32 i_x5_x41 i_x5_x42 i_x5_x43 i_x5_x44
i_x5_x7 i_x5_x6 i_x5_x8 i_x5_x9 i_x5_x10
i_x31_x41 i_x31_x42 i_x31_x43 i_x31_x44
i_x31_x7 i_x31_x6 i_x31_x8 i_x31_x9 i_x31_x10
i_x32_x41 i_x32_x42 i_x32_x43 i_x32_x44
i_x32_x7 i_x32_x6 i_x32_x8 i_x32_x9 i_x32_x10
i_x41_x7 i_x41_x6 i_x41_x8 i_x41_x9 i_x41_x10
i_x42_x7 i_x42_x6 i_x42_x8 i_x42_x9 i_x42_x10
i_x43_x7 i_x43_x6 i_x43_x8 i_x43_x9 i_x43_x10
i_x44_x7 i_x44_x6 i_x44_x8 i_x44_x9 i_x44_x10
i_x7_x6 i_x7_x8 i_x7_x9 i_x7_x10
i_x6_x8 i_x6_x9 i_x6_x10
i_x8_x9 i_x8_x10
i_x9_x10 
/ link=logit distribution=binary;
selection method=stepwise(select=sl choose=BIC);
run;
\end{verbatim}

Avec le critère \(\mathsf{BIC}\), on obtient 12 variables tandis que \texttt{choose=aic} donne 13 variables (seule la variable \texttt{x41} est ajoutée). Il s'agit des mêmes variables que celles sélectionnées par une sélection séquentielle classique en prenant 0,05 comme critère d'entrée et de sortie.

\hypertarget{performance-des-diffuxe9rents-moduxe8les-pour-lexemple-des-clients-cibles}{%
\section{Performance des différents modèles pour l'exemple des clients cibles}\label{performance-des-diffuxe9rents-moduxe8les-pour-lexemple-des-clients-cibles}}

Nous allons conclure, pour l'instant, notre exemple dans cette section, en évaluant la performance de différentes stratégies. Le critère de performance sera le suivant : revenu net de la stratégie si elle était appliquée aux 100 000 clients restants. Pour chacun des 100 000 clients à catégoriser, nous allons calculer la quantité suivante :

\begin{itemize}
\tightlist
\item
  Si le client n'est pas ciblé pour l'envoi d'un catalogue par le modèle, alors le revenu est nul.
\item
  Si le client est ciblé pour l'envoi d'un catalogue par le modèle et qu'il n'achète rien, le revenu est de \(-10\)\$ (le coût de l'envoi).
\item
  Si le client est ciblé pour l'envoi d'un catalogue par le modèle et qu'il achète quelque chose, le revenu est de (\texttt{ymontant}\(-10\))\$, c'est-à-dire, le montant qu'il dépense moins le \(10\)\$ du coût de l'envoi.
\end{itemize}

Pour une stratégie donnée, chaque individu n'appartient qu'à une seule des catégories. Le revenu net de la stratégie est la somme des revenus pour les 100 000 clients. Parmi ces derniers, 23 179 auraient acheté si on leur avait envoyé le catalogue et ces clients auraient généré des revenus de 1 601 212\$. Si on enlève le coût des envois (100 000 X 10\$ = 1 000 000\$), on obtient que la stratégie de référence permet un revenu net de 601 212\$.

Nous allons investiguer deux types de stratégies :

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  une basée sur la régression logistique seulement en utilisant le modèle pour prévoir l'achat et
\item
  une basée sur la combinaison de la régression logistique et la régression linéaire en utilisant un modèle pour prévoir l'achat et un autre pour prévoir le montant.
\end{enumerate}

\hypertarget{stratuxe9gies-en-utilisant-seulement-la-ruxe9gression-logistique}{%
\subsection{Stratégies en utilisant seulement la régression logistique}\label{stratuxe9gies-en-utilisant-seulement-la-ruxe9gression-logistique}}

Dans ce cas, nous allons estimer la probabilité d'achat avec un modèle de régression logistique. Nous allons ensuite trouver le meilleur point de coupure, avec une matrice de gain adéquatement choisie, afin d'avoir une règle d'assignation optimale. Nous avons déterminé des modèles potentiels à la section précédente. De plus, nous avons déjà vu comment trouver le meilleur point de coupure en spécifiant une matrice de gain, afin de maximiser le gain moyen à partir de la matrice de gain du tableau \ref{tab:03-gain3}. Nous allons donc trouver le meilleur point de coupure pour quelques-uns des modèles choisis à la section précédente, pour ensuite évaluer le revenu net de ces modèles.

Il faut encore une fois bien comprendre qu'en pratique, on ne pourrait pas faire cette comparaison, car on ne sait pas d'avance si les clients futurs vont acheter ou non. Mais dans cet exemple, les variables \texttt{yachat} et \texttt{ymontant} sont fournies pour ces 100 000 clients afin qu'on puisse voir ce qui se serait passé avec les différentes stratégies.

La stratégie de référence est celle qui consiste à envoyer le catalogue aux 100 000 clients sans les sélectionner. Le tableau qui suit montre des statistiques pour les variables \texttt{ymontant} et \texttt{yachat} pour les 100 000 clients à scorer. Le tableau \ref{tab:03-summarylog} résume la performance des différentes stratégies basées exclusivement sur le modèle logistique.

\begin{longtable}[]{@{}lllllll@{}}
\caption{\label{tab:03-summarylog} Résumé des caractéristiques des modèles logistiques avec (a) référence, soit l'envoi sans sélection à tous les clients; (b) 10 variables de base sans sélection; (c) toutes les variables, incluant les termes quadratiques et les interactions d'ordre 2; (d) sélection séquentielle classique avec critère d'entrée et de sortie à 0,05; (e) idem que (d), mais avec meilleur modèle selon le \(\mathsf{AIC}\); (f) idem que (d), mais avec meilleur modèle selon le \(\mathsf{BIC}\); (g) recherche exhaustive avec 47 variables sélectionnées par recherche séquentielle et modèle final sélectionné selon le \(\mathsf{BIC}\); (h), idem mais sélection avec \(\mathsf{AIC}\). Les points de coupure optimaux ont été déterminés par validation-croisée sur l'échantillon d'apprentissage, tandis que la performance du modèle (sensibilité, taux de faux positifs et taux de bonne classification) ont été calculés à partir de l'échantillon test de 100 000 individus.}\tabularnewline
\toprule
\begin{minipage}[b]{0.37\columnwidth}\raggedright
modèle\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
\# variables\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\raggedright
point de coupure\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedright
sensibilité\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\raggedright
taux de faux positifs\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\raggedright
taux de bonne classification\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
revenu net\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.37\columnwidth}\raggedright
modèle\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
\# variables\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\raggedright
point de coupure\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedright
sensibilité\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\raggedright
taux de faux positifs\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\raggedright
taux de bonne classification\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
revenu net\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.37\columnwidth}\raggedright
(a)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
---\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
---\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
100\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
76,8\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
23,2\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
601212\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
(b)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
14\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0,12\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
89\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
56,2\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
70,9\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
940569\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
(c)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
104\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0,08\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
85,8\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
52,6\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
74,6\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
937150\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
(d), (e)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
13\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0,14\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
85,7\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
49,1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
77,5\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
969350\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
(f)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
12\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0,19\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
81\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
44,7\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
80,4\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
943935\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
(g)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
8\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0,16\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
86\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
48,1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
78,3\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
985069\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
(h)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
28\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0,15\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
83,5\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
47,4\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
78,8\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
952672\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Nous avons vu plus tôt, qu'avec les 10 variables de base, le meilleur point de coupure est de 0,12. En utilisant cette stratégie sur les 100 000 clients, le revenu net aurait été de 940 569\$. C'est une énorme amélioration, de plus de 56\%, par rapport à la stratégie de référence qui consiste à envoyer le catalogue à tout le monde (revenu net de 601 212\$). Si on inclut tous les termes quadratiques et les termes les interactions d'ordre deux (104 variables en tout), le revenu net est inférieur avec une valeur de 937 150\$. Ici, le modèle est trop complexe et surajusté. Si on fait une sélection de variables (4 méthodes sont présentées), suivie de la détermination du meilleur point de coupure, on fait alors toujours mieux qu'avec le modèle incluant les 10 variables de base seulement. L'approche qui aurait fait le mieux est la recherche séquentielle pour réduire le nombre de variables considérées à 47, suivi d'une recherche exhaustive pour trouver le modèle avec le plus petit \(\mathsf{BIC}\) : cette approche aurait généré 985 069\$ de revenus nets. Il s'agit d'un gain de 4,7\% par rapport au modèle avec les 10 variables de base.

\hypertarget{stratuxe9gies-alternatives}{%
\subsection{Stratégies alternatives}\label{stratuxe9gies-alternatives}}

Nous venons tout juste d'étudier des stratégies qui consistent essentiellement, à estimer \({\mathsf P}\left(\texttt{yachat}=1\right)\) et un point de coupure afin de décider à qui envoyer le catalogue en partant du postulat que tous les clients dépensent le même montant; le tout est basé uniquement sur la régression logistique. Le revenu moyen peut être estimé à partir de l'équation
\[{\mathsf E}\left(\texttt{ymontant}\right) = {\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right) {\mathsf P}\left(\texttt{yachat
}=1\right),
\]
c'est-à-dire, la moyenne du montant dépensé est égale à la moyenne du montant dépensé étant donné qu'il y a eu achat, fois la probabilité qu'il ait eu achat. Une autre stratégie possible consiste donc à développer deux modèles : un pour \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\) et un autre pour \({\mathsf P}\left(\texttt{yachat}=1\right)\) et à les combiner afin d'obtenir des prévisions du montant dépensé.

Nous avons déjà développé des modèles de régression linéaire pour \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\) au chapitre précédent et nous venons de développer des modèles de régression logistique pour \({\mathsf P}\left(\texttt{yachat}=1\right)\) dans ce chapitre. Nous avons donc tous les ingrédients pour implanter cette stratégie.

Nous allons cibler les clients dont la prévision du montant dépensé est plus grande que 10\$ (le coût de l'envoi du catalogue).

Les possibilités de modèles sont nombreuses. Par exemple, si on a cinq modèles potentiels pour \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\) et cinq pour \({\mathsf P}\left(\texttt{yachat}=1\right)\), il y a 25 combinaisons possibles. Ici, nous allons seulement présenter les résultats pour deux combinaisons :

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  pour \(\texttt{ymontant}\), nous allons utiliser les variables choisies par \texttt{glmselect} avec les options \texttt{select=aic,\ choose=bic}, tandis que pour \(\texttt{yachat}\), nous allons utiliser les variables choisies par la procédure séquentielle suivie d'une recherche exhaustive avec le critère BIC
\item
  à la fois pour \(\texttt{ymontant}\) et \(\texttt{yachat}\), nous allons utiliser les variables choisies en faisant une sélection séquentielle classique (tests-\(t\)) avec critères d'entrée et de sortie fixés à 0,05.
\end{enumerate}

Pour obtenir les prévisions, nous allons estimer conjointement les modèles pour \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\) et pour \({\mathsf P}\left(\texttt{yachat}=1\right)\) avec un modèle Tobit de type 2 (aussi appelé modèle Heckit), dont une brève description est donnée dans la section \ref{tobit2}.
L'avantage de l'estimation simultanée est que l'on a pas à sélectionner le point de coupure, puisque l'on enverra le catalogue uniquement si le montant prédit pour \({\mathsf E}\left(\texttt{ymontant}\right)\) (non-conditionnel) est supérieur à 10\$.
Les résultats du modèle Tobit sur l'échantillon de validation sont rapportés dans le tableau \ref{tab:03-tobit}.

\begin{longtable}[]{@{}clllr@{}}
\caption{\label{tab:03-tobit} Matrice de gain pour l'envoi de catalogue}\tabularnewline
\toprule
\begin{minipage}[b]{0.21\columnwidth}\centering
modèle Tobit de type II\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
sensibilité\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
faux positifs (\%)\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright
bonne classification (\%)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\raggedleft
revenu net\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.21\columnwidth}\centering
modèle Tobit de type II\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
sensibilité\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
faux positifs (\%)\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedright
bonne classification (\%)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\raggedleft
revenu net\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.21\columnwidth}\centering
(1)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
88,3\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
50,9\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
76,1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedleft
997 238\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.21\columnwidth}\centering
(2)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
86,3\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
49,9\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedright
76,9\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedleft
977 422\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Il s'avère qu'on aurait eu des performances semblables aux stratégies basées uniquement sur la régression logistique vues à la sous-section précédente. La première combinaison aurait tout de même produit un revenu net de 997 238\$, supérieur au revenu net de 985 069\$, qui était le meilleur trouvé à la sous-section précédente.

Pour conclure cet exemple, il s'avère donc que la régression logistique permet d'effectuer un bon ciblage des clients potentiels afin de maximiser les revenus. L'approche générale consistant à obtenir des prévisions pour \({\mathsf P}\left(\texttt{yachat}=1\right)\) et ensuite trouver le meilleur point de coupure est très générale. D'autres types de modèles (arbre de classification, forêt aléatoire, réseau de neurones) pourraient être utilisés à la place de la régression logistique.

Nous reviendrons une dernière fois sur cet exemple dans le chapitre traitant des données manquantes. Nous verrons alors comment procéder si des valeurs manquantes sont présentes dans les variables explicatives.

\hypertarget{tobit2}{%
\subsection{Modèle Tobit de type 2}\label{tobit2}}

Cette partie est plus technique et peut être omise.

Il ne serait pas justifié d'ajuster séparément les deux modèles pour \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\) et \({\mathsf P}\left(\texttt{yachat}=1\right)\) et de calculer les prévisions en prenant le produit: \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right){\mathsf P}\left(\texttt{yachat}=1\right)\). Cela provient du fait que le modèle pour \({\mathsf E}\left(\texttt{ymontant} \mid \texttt{yachat}=1\right)\) aurait été estimé seulement avec les clients qui ont acheté quelque chose et qu'ensuite on l`appliquerait (au moment de calculer les prévisions) à la fois aux clients qui vont acheter et à ceux qui ne vont pas acheter. Il y a donc un biais de sélection dans l'échantillon qui a servi à ajuster le modèle au départ. Une manière de contourner ce problème est d'ajuster conjointement les deux modèles. Le modèle de Tobit de type 2 permet de faire cela. Ce modèle est basé sur l'hypothèse que les deux variables observées (\(Y_1\) et \(Y_2\)) proviennent de deux variables latentes non observées (\(Y_1^{\star}\) et \(Y_2^{\star}\)), où
\begin{align*}
Y_1 = \begin{cases}
1 & \text{ si } Y_1^{\star} \ge 0, \\
0 & \text{ si } Y_1^{\star} < 0,
\end{cases}
\qquad \qquad 
Y_2 = \begin{cases}
Y_2^{\star} & \text{ si } Y_1^{\star} \ge 0, \\
0 & \text{ si } Y_1^{\star} < 0.
\end{cases}
\end{align*}
Dans notre exemple, \(Y_1\) correspond à \(\texttt{yachat}\) et \(Y_2\) à \(\texttt{ymontant}\).
Ce qui lie les deux équations est le fait qu'on suppose que les variables sont binormales: les deux termes d'erreur sont de loi normale et sont corrélés, \(\boldsymbol{\varepsilon} \sim \mathcal{N}_2(\boldsymbol{0}_2, \boldsymbol{\Sigma})\). Les variables dépendantes observées sont :
\begin{align*}
Y_{1}^{\star} &= \beta_{01} + \beta_{11} X_{11} + \cdots + \beta_{1p}X_{p1} + \varepsilon_{1}\\
Y_{2}^{\star} &= \beta_{02} + \beta_{12} X_{12} + \cdots + \beta_{1p}X_{q2} + \varepsilon_{2}
\end{align*}
Notez que les variables explicatives ne sont pas nécessairement les mêmes dans les deux équations. En estimant conjointement les deux équations, on élimine le biais de sélection mentionné plus haut. La procédure \texttt{qlim} (de \textbf{SAS/ETS}, pas disponible dans \emph{SAS University Edition}) permet d'estimer ce modèle. Cependant, \texttt{qlim} ne fait pas de sélection de variables. Le choix des variables doit être fait avant avec les méthodes qu'on a vues. De plus, pour être précis, le modèle Tobit ajuste un modèle probit et non logistique à la variable binaire.

\hypertarget{extensions-du-moduxe8le-de-ruxe9gression-logistique-uxe0-plus-de-deux-catuxe9gories}{%
\section{Extensions du modèle de régression logistique à plus de deux catégories}\label{extensions-du-moduxe8le-de-ruxe9gression-logistique-uxe0-plus-de-deux-catuxe9gories}}

Supposons que la variable \(Y\) que vous cherchez à modéliser est une variable catégorielle pouvant prendre trois valeurs ou plus. Voici quelques exemples :

\begin{itemize}
\tightlist
\item
  Destination de vacances l'année dernière (Québec, États-Unis, ailleurs).
\item
  Si les élections avaient lieu aujourd'hui au Québec, pour quel parti voteriez-vous (PLQ, PQ, CAQ, QS).
\item
  Combien de fois êtes-vous allé au cinéma l'année dernière: moins de cinq fois (\(\texttt{1}\)), entre cinq et 10 fois (\(\texttt{2}\)), ou plus de 10 fois (\(\texttt{3}\)).
\item
  Quelle importance accordez-vous au service après-vente? Un parmi « pas important » (\(\texttt{1}\)), « peu important »(\(\texttt{2}\)), « moyennement important » (\(\texttt{3}\)), « assez important » (\(\texttt{4}\)), « très important » (\(\texttt{5}\)).
\end{itemize}

Dans les deux premiers exemples, la variable réponse \(Y\) est nominale (elle n'a pas d'ordre) alors qu'elle est ordinale dans les deux derniers. Pour une variable ordinale, le modèle logit multinomial peut être utilisé mais il existe d'autres possibilités comme le modèle logit cumulé. Nous couvrirons ces deux modèles.

\hypertarget{ruxe9gression-logistique-multinomiale}{%
\subsection{Régression logistique multinomiale}\label{ruxe9gression-logistique-multinomiale}}

Afin de simplifier la notation, on suppose qu'il y a une seule variable explicative \(X\) à disposition et que la variable \(Y\) représente trois catégories, une parmi 0, 1 et 2.

En régression logistique, \(Y\) est une variable binaire qui vaut soit 0, soit 1 et la probabilité de succès est
\begin{align*}
\ln\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1 X_{i}, \qquad p_i = {\mathsf P}\left(Y_i=1 \mid X_i\right) = \mathrm{expit}(\beta_0 + \beta_1X_i).
\end{align*}
Dans ce modèle logistique, \(\ln(p_i)-\ln(1-p_i) = \ln\{{\mathsf P}\left(Y_i=1 \mid X_i\right)\} - \ln\{{\mathsf P}\left(Y_i=0 \mid X_i\right)\}\) peut être vu comme étant le logit de la catégorie 1 en utilisant 0 comme catégorie de référence.
Le modèle logistique multinomial procède de même en fixant une catégorie de référence et en modélisant le logit de chacune des autres catégories par rapport à la catégorie de référence. Avec \(K+1\) catégories et en choisissant la catégorie 0 comme référence, le modèle devient
\begin{align*}
 \ln\left(\frac{p_{i1}}{p_{i0}}\right) = \beta_{01} + \beta_{11} X_i, \, \ldots, \, \ln\left(\frac{p_{iK}}{p_{i0}}\right) = \beta_{0K} + \beta_{1K} X_i,
\end{align*}
où \(p_{ik} = {\mathsf P}\left(Y_i=k \mid X_i\right)\) \((k=0, \ldots, K)\). Comme en régression logistique, on peut facilement exprimer ce modèle en termes des différentes probabilités,
\begin{align*}
 p_{i0} &= {\mathsf P}\left(Y_i=0 \mid X_i\right) = \frac{1}{1+ \sum_{j=1}^K\exp(\beta_{0j}+\beta_{1j}X_i)}\\
 p_{ik} &= {\mathsf P}\left(Y_i=k \mid X_i\right) = \frac{\exp(\beta_{0k}+\beta_{1k}X_i)}{1+ \sum_{j=1}^K\exp(\beta_{0k}+\beta_{1k}X_i)}, \qquad k =1, \ldots, K.
\end{align*}
On voit facilement que la somme des probabilités égale 1, c'est-à-dire \(p_{i0} + \cdots + p_{iK} = 1\). En fait, le modèle logit multinomial ne fait que combiner plusieurs logit dans un seul modèle. L'interprétation des paramètres se fait comme en régression logistique sauf qu'il faut y aller équation par équation.

Destination vacances. Le fichier \texttt{logit6.sas7bdat} contient 100 observations obtenues par voie de sondage auprès d'adultes âgés de 18 à 45 ans. Le fichier contient les réponses aux questions suivantes:

\begin{itemize}
\tightlist
\item
  \texttt{y1}: quelle a été votre destination vacances l'année dernière: Québec (\(\texttt{0}\)), États-Unis (\(\texttt{1}\)) ou ailleurs (\(\texttt{2}\))?
\item
  \texttt{y2}: combien de fois êtes-vous allé au cinéma l'année dernière: moins de 5 fois (\(\texttt{1}\)), entre 5 et 10 fois (\(\texttt{2}\)), ou plus de 10 fois (\(\texttt{3}\)).
\item
  \texttt{x}: âge (en année) du répondant.
\end{itemize}

Nous allons modéliser la destination vacance \(Y_1\) à l'aide d'une régression logistique multinomiale avec l'âge comme variable explicative.

\begin{center}\includegraphics[width=0.5\linewidth]{figures/03-logistic-e31} \end{center}

On voit que les gens qui ont passé leurs vacances au Québec (\(Y_1=0\)) ont 26,5 ans en moyenne. Ils sont plus jeunes que ceux qui ont passé leurs vacances aux États-Unis (âge moyen de 33 ans). Finalement, ceux dont la destination vacances était ailleurs sont les plus vieux avec une moyenne de 37,3 ans.

Pour le modèle logit multinomial, nous allons prendre \(Y_1=0\) comme catégorie de référence. Les commandes sont

\begin{verbatim}
proc logistic data=multi.logit6 ;
model y1(ref='0') = x / clparm=pl clodds=pl expb link=glogit;
run; 
\end{verbatim}

L'option \texttt{link=glogit} spécifie le type de fonction de lien, ici celle du modèle logistique multinomial.

\begin{center}\includegraphics[width=0.75\linewidth]{figures/03-logistic-e23} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e24} \end{center}

\begin{center}\includegraphics[width=0.82\linewidth]{figures/03-logistic-e25} \end{center}

Comme il y a trois catégories pour la variable dépendante, il y a deux équations pour le modèle ajusté,
\begin{align*}
\ln \left\{\frac{{\mathsf P}\left(Y_{1i}=1 \mid X_i\right)}{{\mathsf P}\left(Y_{1i}=0 \mid X_i\right)} \right\} = -4,10 + 0,13X_i, \qquad \qquad \ln \left\{\frac{{\mathsf P}\left(Y_{1i}=2 \mid X_i\right)}{{\mathsf P}\left(Y_{1i}=0 \mid X_i\right)} \right\} = -7,98+0,22X_i. 
\end{align*}

Plus l'âge du répondant augmente, plus la probabilité qu'il ait passé ses vacances aux États-Unis par rapport au Québec augmente. En fait, pour chaque augmentation de 1 de l'âge, le rapport des cote pour \(Y_1=1\) par rapport à \(Y_1=0\) est multipliée par \({1,133}=\exp({0,1253})\). Cette valeur est donnée dans la dernière colonne du tableau. De plus, cet effet est significatif car la valeur-\(p\) est inférieure à \(10^{-4}\).

De même, plus l'âge du répondant augmente, plus la probabilité qu'il ait passé ses vacances ailleurs qu'aux États-Unis ou au Québec par rapport au Québec augmente. En fait, pour chaque augmentation de l'âge d'un an, le rapport des cote pour \(Y_1=1\) par rapport à \(Y_1=0\) est multiplié par \({1,25}\). Cet effet est également statistiquement significatif.

Nous venons donc de comparer chacune des catégories \(Y_1=1\) et \(Y_1=2\) à la catégorie de référence \(Y_1=0\). Pour une comparaison directe entre \(Y_1=1\) et \(Y_1=2\), il suffit de changer la catégorie de référence et de resoumettre le programme \textbf{SAS}.

\hypertarget{ruxe9gression-logistique-cumulative-uxe0-cotes-proportionnelles}{%
\subsection{Régression logistique cumulative à cotes proportionnelles}\label{ruxe9gression-logistique-cumulative-uxe0-cotes-proportionnelles}}

Si les modalités de la réponse sont ordinales, la régression logistique multinomiale est toujours appropriée. Il peut néanmoins être préférable d'utiliser un modèle qui utilise l'ordre des modalités pour obtenir un modèle plus facile à interpréter et plus parcimonieux. Le modèle de régression logistique cumulative à cotes proportionnelles est un simplification sous l'hypothèse que les cotes sont proportionnelles.

Supposons que la variable \(Y\) est ordinale et peut prendre les \(K+1\) valeurs ordonnées de la plus petite à la plus grande selon les catégories de \(Y\) (\(0, 1, 2, \ldots, K\)). Supposons que l'on dispose de \(p\) variables explicatives \(X_1, \ldots, X_p\).

Soit \(p_{ik}={\mathsf P}\left(Y_i \mid X_{i1}, \ldots, X_{ip}\right)\) (\(k=0, 1, \ldots, K\)) la probabilité que \(Y_{ik}\) prenne la valeur \(k\). On dénote la fonction de survie
\[S_{ij}=\sum_{k=j}^K p_{ik}= {\mathsf P}\left(Y_{i} \geq j\mid X_{i1}, \ldots, X_{ip}\right), \qquad j=1, \ldots, K.
\]
La quantité \(S_{ij}\) est la probabilité que \(Y_i\) soit plus grand ou égal à \(j\); \(S_{i0}\) est égal à 1 et \(S_{ik} = {\mathsf P}\left(Y_i=K \mid X_{i1}, \ldots, X_{ip}\right)=p_{iK}\).

Le modèle logistique cumulé spécifie que
\begin{align*}
\ln \left( \frac{S_{ij}}{1-S_{ij}}\right) = \beta_{0j} + \beta_1 X_{i1} + \cdots + \beta_p X_{ip}, \qquad \qquad  j=1, \ldots, K.
\end{align*}

Il y a donc \(K-1\) équations logistiques. Les paramètres quantifiant les effets des variables explicatives, \(\beta_1, \ldots, \beta_K\) sont les mêmes pour chacune des log-cotes, mais il y a une ordonnée à l'origine différente par log de rapport de cote. Par conséquent, pour modéliser une variable ordinale \(Y\) ayant \(K+1\) valeurs possibles avec p variables explicatives, le modèle cumulatif logistique utilise \(p + K\) paramètres. Le modèle logit multinomial, qui peut également être utilisé pour les données ordinales, utilise \(p\cdot (K+1)\) paramètres. Le modèle multinomial ordonné est donc plus parcimonieux et, pour autan qu'il soit approprié, mènera à des estimations des paramètres plus précises qu'avec le modèle de régression logistique multinomiale. Les deux modèles sont identiques au modèle de régression logistique si la variable ordinale a deux modalités.

La cote \(S_{ij}/(1-S_{ij})\) mesure à quel point il est plus probable que \(Y_i\) prenne une valeur plus grande ou égale à \(j\) par rapport à une valeur plus petite que \(j\), viz.
\begin{align*}
\frac{S_{ij}}{1-S_{ij}} = \exp( \beta_{0j} + \beta_1X_{i1} + \cdots + \beta_p X_{ip}).
\end{align*}
Dans cet exemple, aucune fonction autre qu'additive en \(X\), ni aucune interaction n'est présente. Si le paramètre \(\beta_j\) est positif, cela indique que plus \(X_j\) prend une valeur élevée, plus la variable \(Y\) a tendance à prendre aussi une valeur élevée. Inversement, si le paramètre \(\beta_j\) est négatif, cela indique que plus \(X_j\) prend une valeur élevée, plus la variable \(Y\) a tendance à prendre une valeur basse. Plus précisément, pour chaque augmentation d'une unité de \(X_j\), la cote \(S_k/(1-S_k)\) est multipliée par \(\exp(\beta_j)\), peu importe la valeur de \(Y\). Ainsi, la cote d'être dans une catégorie plus élevée, par rapport à une catégorie moins élevée, est multipliée par \(\exp(\beta_j)\). En terme de probabilités cumulées d'excéder \(k\),
\begin{align*}
S_{ik} = {\mathsf P}\left(Y_i \geq k \mid X_{i1}, \ldots, X_{ip}\right) = \mathrm{expit}(\beta_{0k} + \beta_1 X_{i1} + \cdots + \beta_p X_{ip}), \qquad j =1, \ldots, K.
\end{align*}
En utilisant ces expressions, on peut obtenir la probabilité de chaque catégorie,
\begin{align*}
{\mathsf P}\left(Y_i = k \mid X_{i1}, \ldots, X_{ip}\right) ={\mathsf P}\left(Y_i \geq k \mid X_{i1}, \ldots, X_{ip}\right) -{\mathsf P}\left(Y_i \geq k+1 \mid X_{i1}, \ldots, X_{ip}\right) = S_{k} - S_{k+1}.
\end{align*}

Nous considérons maintenant la variable \(Y_2\) du fichier \texttt{logit6.sas7bdat}, qui donne le nombre de visites au cinéma. Pour cet exemple, nous allons chercher à modéliser \(Y_2\) à l'aide de \(X\) (âge) en utilisant le modèle multinomial cumulé à cotes proportionnelles.

\begin{tabular}{r|r|r|r|r|r}
\hline
y2 & n & mean & std.err & min & max\\
\hline
1 & 44 & 33.50 & 7.23 & 18 & 44\\
\hline
2 & 38 & 30.45 & 8.16 & 18 & 44\\
\hline
3 & 18 & 25.11 & 6.58 & 18 & 39\\
\hline
\end{tabular}

Ainsi, les répondants qui sont allés moins de cinq fois au cinéma ont en moyenne 33,5 ans, ceux qui sont allés entre cinq et 10 fois ont 30,4 ans en moyenne, et ceux qui sont allés plus de 10 fois ont 25,1 ans en moyenne. Il y a une progression et on voit que les répondants plus jeunes vont plus souvent au cinéma.

Nous utilisons exactement le même programme que pour une régression logistique habituelle. Si la variable \(Y\) prend plus de deux valeurs, \textbf{SAS} utilisera automatiquement le modèle de régression multinomiale cumulé.

\begin{verbatim}
proc logistic data=multi.logit6 descending;
model y2 = x / clparm=pl clodds=pl expb;
run;
\end{verbatim}

L'option \texttt{descending} impose la paramétrisation discutée précédemment. Sans cette option, ce serait plutôt les probabilités de prendre une valeur plus petite, par rapport à une plus grande qui serait modélisée. Le modèle est le même, mais les signes des paramètres des variables explicatives seraient inversés.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e26} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e27} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e28} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e29} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/03-logistic-e30} \end{center}

Avant toute chose, il faut s'assurer que le modèle est approprié. Rappelez-vous que l'une des hypothèses de ce modèle est que les effets des variables explicatives sont les mêmes pour chaque équation. Le tableau « Test de score pour l'hypothèse des cotes proportionnelles » est un test de l'hypothèse nulle

\begin{itemize}
\tightlist
\item
  \(\mathcal{H}_0\) : l'effet de chaque variable est le même pour les \(K\) logit du modèle multinomial logistique, soit \(\beta_{11} = \cdots =\beta_{1K}\), \(\ldots\), \(\beta_{p1} = \cdots =\beta_{pK}\).
\end{itemize}

Une très petite valeur-\(p\) (rejet de \(\mathcal{H}_0\)) pour ce test serait une indication que le modèle de régression multinomiale ordinale n'est pas approprié. Comme la valeur-\(p\) est 0,2577 ici, on ne rejette pas \(\mathcal{H}_0\) et il n'y a pas lieu de douter de cette hypothèse. On peut donc aller de l'avant et interpréter le modèle.

Ici, l'effet estimé de l'âge (\(X\)) est \(-{0,0916}\) et ce paramètre est significativement différent de zéro (valeur-\(p\) de 0,0004). Rappelez-vous que \(Y_2\) représente le nombre d'entrées au cinéma dans la dernière année.

Ainsi, plus l'âge augmente, plus \(Y_2\) a tendance à prendre une petite valeur, c'est-à-dire, plus la personne a tendance à aller moins souvent au cinéma. Plus précisément, lorsque l'âge augmente de 1, la cote d'être dans une catégorie plus élevée de \(Y_2\), par rapport à une catégorie plus basse, est multipliée par 0,912 (la cote diminue donc et aussi la probabilité d'être dans une catégorie plus élevée).

\hypertarget{analyse-de-regroupements-1}{%
\chapter{Analyse de regroupements}\label{analyse-de-regroupements-1}}

\hypertarget{introduction-4}{%
\section{Introduction}\label{introduction-4}}

On cherche à créer des groupes (\emph{clusters}) d'individus homogènes en utilisant \(p\) variables \(X_1, \ldots, X_p\). Plus précisément, on veut combiner des sujets en groupes (interprétables) de telle sorte que les individus d'un même groupe soient le plus semblables possible par rapport à certaines caractéristiques et que les groupes soient le plus différent possible.\\
Nous disposons des observations pour \(n\) individus et \(X_{ij}\) dénote la valeur de la \(j\)e variable explicative pour le \(i\)e sujet: les variables correspondant au sujet \(S_i\) sont donc \(X_{i1}, \ldots, X_{ip}\).

Il y a une certaine analogie avec l'analyse factorielle. En analyse factorielle, on cherche à déterminer s'il y a des groupes de \textbf{variables} corrélées entre elles. On cherche donc à regrouper des variables. En analyse de regroupements, on cherche plutôt à créer des groupes de \textbf{sujets} similaires. Les deux méthodes servent pour l'analyse exploratoire.

Étapes d'une analyse de regroupements

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Choisir les variables pertinentes à l'analyse.
\item
  Décider comment seront mesurées les « distances » entre les sujets. Cela revient à choisir une mesure de dissemblance.
\item
  Décider quelle méthode sera utilisée (méthode hiérarchique, méthode non hiérarchique).
\item
  Choisir le nombre de groupes, soit à partir de connaissances à priori, soit en se basant sur l'analyse de regroupements elle-même.
\item
  Interpréter les groupes obtenus.
\item
  Utiliser ces groupes dans d'autres analyses, le cas échéant.
\end{enumerate}

\hypertarget{mesures-de-dissemblance}{%
\subsection{Mesures de dissemblance}\label{mesures-de-dissemblance}}

Une mesure de dissemblance sert à quantifier la distance entre deux sujets \(S_i\) et \(S_j\) en se basant sur les \(p\) variables \(X_1, \ldots, X_p\). Plus cette mesure est petite, plus les sujets \(S_i\) et \(S_j\) sont similaires. Même s'il y a des exceptions, la plupart des mesures de dissemblances \(d\) ont les propriétés suivantes :

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(d(S_i, S_j) \geq 0\) (positivité);
\item
  \(d(S_i, S_i)=0\);
\item
  \(d(S_i, S_j)=d(S_j, S_i)\) (symmétrie);
\item
  \(d(S_i, S_j)\) augmente au fur et à mesure que les deux sujets deviennent plus différents.
\end{enumerate}

Lorsque toutes les variables sont continues, une mesure de dissemblance souvent utilisée est la distance euclidienne entre sujets, soit
\begin{align*}
d(S_i, S_j) = \sqrt{(X_{i1}-X_{j1})^2 + \cdots + (X_{ip}-X_{jp})^2}.
\end{align*}

La distance euclidienne est tout simplement la longueur du segment qui relie les deux points dans l'espace.

Nous verrons plus tard d'autres mesures de dissemblances incluant des mesures qui peuvent être utilisées avec des variables binaires, nominales et ordinales.
\#\#\# Méthodes hiérarchiques et non hiérarchiques

Les \textbf{méthodes hiérarchiques} assignent les individus aux groupes à l'aide d'un algorithme glouton en partant du cas à \(n\) groupes où chaque sujet est un groupe à part entière. La distance entre chaque paire de groupe est calculée. Les deux groupes ayant la distance la plus petite sont regroupés pour ne laisser que \(n-1\) groupes. La distance entre chaque paire de groupe est à nouveau calculée (pour les groupes). Les deux groupes ayant la distance la plus petite sont regroupés pour ne former qu'un seul groupe et ainsi de suite. Le processus se continue ainsi jusqu'à ce que tous les sujets soient regroupés en un seul groupe.

Avec une méthode hiérarchique, on n'a pas besoin de spécifier le nombre de groupes à priori. Cependant, une fois qu'un sujet est assigné à un groupe, il ne peut le quitter pour être réassigné à un autre groupe plus tard. Ce qui différencie les différentes méthodes hiérarchiques est la manière dont est calculée la distance entre deux groupes.

Pour les \textbf{méthodes non hiérarchiques}, le nombre de groupe est spécifié au départ et un algorithme cherche, à partir d'une solution initiale, la meilleure distribution des sujets à travers ce nombre de groupe d'une manière itérative. Avec ces méthodes, l'assignation d'un sujet peut être modifiée d'une itération à l'autre. Il faut cependant spécifier le nombre de groupe et les « centres » de ces groupes au départ. La solution peut être très sensible au choix des centres initiaux.

\hypertarget{segmentation-de-seniors-en-voyage-organisuxe9}{%
\section{Segmentation de seniors en voyage organisé}\label{segmentation-de-seniors-en-voyage-organisuxe9}}

Les données sont inspirées de

\begin{quote}
Hsu, C. H. C. et Lee E.-J. (2002). Segmentation of Senior Motorcoach Travelers. \emph{Journal of Travel Research}, \textbf{40}, 364-373.
\end{quote}

Les buts de l'étude étaient

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Regrouper les gens de 55 et plus qui participent à des voyages organisés en autobus en groupes homogènes selon des caractéristiques reliées au choix de l'opérateur et du voyage.
\item
  Examiner les caractéristiques de ces groupes.
\item
  Examiner les caractéristiques démographiques de ces groupes.
\end{enumerate}

Nous allons nous intéresser principalement aux deux premiers points ici. Un questionnaire a été élaboré afin d'évaluer l'importance de 55 caractéristiques des opérateurs de voyages organisés en autobus et des voyages eux-mêmes à l'aide d'une échelle de Likert à cinq points, allant de extrêmement important (\(\texttt{5}\)) à pas important du tout (\(\texttt{1}\))). Des données sont disponibles pour 150 sujets (il y en avait 817 dans l'article). Elles se trouvent dans le fichier \texttt{cluster1.sas7bdat}.

Au lieu de faire une analyse de regroupements avec les 55 items du questionnaire, les auteurs ont choisi de faire une analyse factorielle avec rotation varimax au préalable afin de réduire le nombre de variables à six facteurs interprétables :

\begin{itemize}
\tightlist
\item
  Activités sociales (\(X_1\)): formé de cinq items
\item
  Politiques de l'opérateur et références (\(X_2\)) : formé de six items.
\item
  Horaires flexibles (\(X_3\)): formé de trois items.
\item
  Santé et sécurité (\(X_4\)) : formé de quatre items.
\item
  Matériel publicitaire (\(X_5\)): formé de deux items.
\item
  Réputation (\(X_6\)): formé de deux items.
\end{itemize}

On voit donc que 22 items, parmi les 55, sont utilisés dans la définition de ces six facteurs. Dans l'article, les auteurs ont décidé d'inclure ces 22 items dans l'analyse de regroupements. Pour notre part et afin de simplifier l'exemple, nous allons plutôt créer six nouvelles échelles en faisant la moyenne des items de chaque facteur ci-haut et utiliser seulement ces six échelles dans l'analyse de regroupements. Les valeurs de ces six variables pour les 150 sujets se trouvent dans le fichier \texttt{cluster1a.sas7bdat} et sont toutes dans l'intervalle \([1,5]\), puisqu'elles représentent la moyenne d'échelles de Likert.

\begin{center}\includegraphics[width=0.6\linewidth]{figures/04-clustering-e1} \end{center}

\hypertarget{muxe9thodes-hiuxe9rarchiques}{%
\section{Méthodes hiérarchiques}\label{muxe9thodes-hiuxe9rarchiques}}

Comme nous venons de le voir, cette méthode débute avec \(n\) groupes, un par sujet, et procède en regroupant des groupes formés au préalable d'une manière hiérarchique jusqu'à ce que tous les sujets ne forment qu'un seul groupe. Le nombre de groupe retenu pourra être sélectionné à l'aide de certains critères que nous verrons plus tard.

À une étape donnée, il faut choisir quels groupes seront combinés. Les deux groupes dont la distance est la plus faible seront combinés. Il faut donc être en mesure de calculer la distance entre deux groupes. Nous allons décrire la méthode de Ward, qui compte parmi les plus populaires. Nous reviendrons brièvement sur d'autres méthodes plus loin.

\hypertarget{muxe9thode-de-ward}{%
\subsection{Méthode de Ward}\label{muxe9thode-de-ward}}

Cette méthode est basée sur un critère d'homogénéité global des groupes. Pour un groupe donné, cette homogénéité est mesurée par la somme des carrés des observations par rapport à la moyenne du groupe. L'homogénéité globale est alors la somme des homogénéités de tous les groupes. Plus l'homogénéité globale est petite, plus les groupes sont homogènes. À une étape donnée, les deux groupes qui causent la plus petite hausse de l'homogénéité globale (la plus petite perte d'information) sont regroupés. La méthode de Ward donne des groupes compacts d'apparence sphérique.

Plus précisément, supposons qu'à une étape du processus hiérarchique, nous avons \(M\) groupes et que nous voulons passer à \(M-1\) groupes. Pour un groupe \(k\) (parmi \(1, 2, \ldots, M\)), définissons la somme des carrés des distances par rapport à la moyenne du groupe, \(\mathsf{SCD}_k\). Plus \(\mathsf{SCD}_k\) est petite, plus le groupe est compact et homogène.

On peut calculer cette distance pour tous les \(M\) groups et définir l'\textbf{homogénéité globale} comme la somme de l'homogénéité de tous les groupes,
\begin{align*}
\mathsf{SCD}_G = \mathsf{SCD}_1 + \cdots + \mathsf{SCD}_M.
\end{align*}
Plus l'homogénéité globale \(\mathsf{SCD}_G\) est petite, mieux c'est. Pour passer de \(M\) à \(M-1\) groupes, la méthode de Ward va regrouper les deux groupes qui feront que \(\mathsf{SCD}_G\) sera la plus petite possible.

On procède à une analyse simplifiée des données pour le voyage organisé avec deux variables et vingt observations afin d'être en mesure de visualiser l'algorithme de groupement. Les données se trouvent dans le fichier \texttt{cluster1a}.

\begin{center}\includegraphics[width=0.9\linewidth]{MATH60602_files/figure-latex/04-plottrueclust-1} \end{center}

Comme souvent, les données ont été simulées et le nombre de regroupements est donc connu (de même que l'appartenance des différentes observations aux regroupements). La variable qui donne l'identité du vrai groupe est \texttt{cluster\_vrai}. Il est important de bien comprendre qu'en pratique, on ne connaît ni le nombre de groupes, ni quelle observation appartient à quel groupe. Ici, on se servira du fait qu'on connaît les vrais groupes pour examiner la performance de l'analyse de regroupements.

La première analyse utilise la méthode de Ward. Les commandes \textbf{SAS} se trouvent dans \texttt{cluster1\_simplifie.sas}; la présentation de la procédure et de la syntaxe est différée. L'historique de regroupement est décrit dans la sortie \textbf{SAS}. La première colonne donne le nombre de groupes. Au départ, les observations 16 et 19 sont regroupées, il y a maintenant 19 groupes. Ensuite, les observations 11 et 13 sont regroupées, il y a maintenant 18 groupes. Au moment de passer de 14 à 13 groupes, c'est le groupe formé à l'étape 16 qui est fusionné avec l'observation 2 et ainsi de suite. La colonne \texttt{Fréq} donne le nombre d'observations dans le groupe qui vient d'être formé.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/04-clustering-e3} \end{center}

Les quantités \texttt{sprsq} et \texttt{rsq} sont des statistiques qui peuvent servir de guide pour choisir le nombre de groupes. Le \(\mathsf{RSQ}\) est une mesure similaire au \(R^2\) régression linéaire qui mesure globalement à quel point les groupes sont homogènes. Elle prend une valeur entre 0 et 1 où 0 et plus le \(\mathsf{RSQ}\) est élevé, meilleur le regroupement.
On définit le \(\mathsf{RSQ}\) comme la proportion de la variabilité expliquée par les groupes. C'est une version standardisée de la somme des homogénéités, \(\mathsf{SCD}_G\),
\begin{align*}
\mathsf{RSQ} = \frac{\mathsf{SCD}_G}{\mathsf{SCD}_T},
\end{align*}
où \(\mathsf{SCD}_T\) est la somme des carrés des distances par rapport à la moyenne lorsque toutes les observations sont dans un même groupe. Le graphique @ref(fig:fig4\_e4) montre l'évolution du \(\mathsf{RSQ}\) en fonction du nombre de groupes.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/04-clustering-e4} \end{center}

L'idée est généralement de choisir un petit nombre de groupe avec un \(\mathsf{RSQ}\) assez élevé.
Ici, on voit que le \(\mathsf{RSQ}\) chute brutalement en passant de trois à deux groupes (il passe de 78,2\% de variabilité expliquée à 48,6\%). Ainsi, choisir trois groupes semble raisonnable.

L'autre mesure, le \(\mathsf{SPRSQ}\) ou \(R\) carré semi-partiel, mesure la perte d'homogénéité résultant du fait que l'on vient de former un nouveau groupe. Comme on veut des groupes homogènes, on veut qu'elle soit petite. Plus précisément, supposons que les groupes \(k_1\) et \(k_2\) viennent d'être regroupés à une étape donnée. Soient \(\mathsf{SCD}_{k_1}\) et \(\mathsf{SCD}_{k_2}\) les homogénéités de ces deux groupes et \(\mathsf{SCD}_{k}\) l'homogénéité du nouveau groupe formé en fusionnant les deux.
On définit la perte d'homogénéité (relative) en combinant ces deux groupes
\begin{align*}
\mathsf{SPRSQ} = \frac{\mathsf{SCD}_k - \mathsf{SCD}_{k_1} - \mathsf{SCD}_{k_2}}{\mathsf{SCD}_T}
\end{align*}
On peut ainsi tracer une courbe pour le \(\mathsf{SPRSQ}\) en fonction du nombre de groupes, comme dans le graphique @ref(fig:fig4\_e5).

\begin{center}\includegraphics[width=0.8\linewidth]{figures/04-clustering-e5} \end{center}

La procédure \textbf{SAS} qui permet d'effectuer une analyse de regroupements hiérarchique est \texttt{cluster}. Le fichier \texttt{cluster2\_complet.sas} explique les différentes options disponibles.

\begin{verbatim}
proc cluster data=temp method=ward outtree=temp1 nonorm rsquare ccc;
var x1-x6;
copy id cluster_vrai x1-x6;
ods output stat.cluster.ClusterHistory=criteres;
run;
\end{verbatim}

On peut représenter graphique le R carré (Figure @ref(fig:fig4\_e7)), le Rcarré semi partiel (Figure @ref(fig:fig4\_e8)) et le critère de classification cubique (Figure @ref(fig:fig4\_e9)) en fonction du nombre de groupes. Ce derniers est plus technique mais son interprétation est simple, plus sa valeur est élevée, mieux c'est. Par contre, il peut avoir plusieurs maximums locaux et, de ce fait, ce n'est donc pas toujours évident de choisir le nombre de groupes avec ce critère. Il semble encore une fois que choisir trois groupes semble raisonnable.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/04-clustering-e8} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/04-clustering-e9} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{figures/04-clustering-e10} \end{center}

Parfois, l'information est présentée sous forme de dendogramme, qui trace l'arbre et la fusion des groupes. On peut ainsi retracer l'historique de la procédure hiérarchique. Celui produit par \textbf{SAS} donne, à un facteur multiplicatif près, le \(\mathsf{SPRSQ}\). Il n'y a donc pas de nouvelles informations ici. On voit que c'est lorsqu'on passe de trois à deux groupes, qu'il y a une bonne perte d'homogénéité.

En pratique, on ne peut jamais savoir si on a bel et bien regroupé ensemble les bons sujets. Mais ici, comme il s'agit de données artificielles qui ont été générées, nous connaissons la composition des vrais groupes. Il s'avère qu'il y en a effectivement trois. De plus, la solution à trois groupes obtenue avec la méthode de Ward a bien réussi à retrouver les groupes car il y a 146 sujets sur 150 (97,3\%) qui sont bien regroupés, et quatre qui font partie d'un mauvais groupe. Ceci est un exemple facile où les observations sont bien séparées. Ce n'est pas toujours aussi simple en pratique.

\textbf{Interprétation des groupes}: la méthode la plus simple consiste à inspecter les moyennes des variables de chaque groupe et de voir s'il en découle une interprétation raisonnable. La procédure \texttt{tree} permet d'extraire la solution avec un nombre spécifié de groupes et il est ensuite facile (avec la procédure \texttt{means}) d'obtenir ces moyennes (voir le fichier \texttt{cluster2\_complet.sas}).

\begin{center}\includegraphics[width=0.55\linewidth]{figures/04-clustering-e11} \end{center}

\begin{center}\includegraphics[width=0.55\linewidth]{figures/04-clustering-e12} \end{center}

\begin{center}\includegraphics[width=0.55\linewidth]{figures/04-clustering-e13} \end{center}

Le groupe 1 est le groupe où les sujets ont les valeurs, en moyenne, les plus faibles pour les six variables. Le groupe 2 est celui où les sujets ont les valeurs, en moyenne, les plus élevées pour les 6 variables sauf pour la variable \(X_1\) (activité sociale). Le groupe 3 est celui où les sujets ont, en moyenne, la valeur la plus élevée de la variable \(X_1\) et des valeurs moyennes inférieures au groupe 3 mais supérieures au groupe 2 pour les cinq autres variables.

Dans l'article, les auteurs ont baptisé les sujets du groupe 1, les « indépendants », ceux du groupe 2, les « dépendants » et ceux du groupe 3, les « sociables ». On peut également tester l'égalité des moyennes des variables pour les différents groupes avec une ANOVA. On peut aussi être plus sophistiqué et explorer les groupes en modélisant les effets des variables en ce qui a trait à l'appartenance aux groupes. Traditionnellement, l'analyse discriminante est utilisée à cette fin. Il est aussi possible d'utiliser un arbre de classification ou une autre méthode prévisionnelle, telle la régression multinomiale logistique. La variable identifiant le groupe d'appartenance obtenu avec l'analyse de regroupement sert alors de variable dépendante \(Y\). Ce type d'analyse permet de creuser un peu plus pour essayer de comprendre la structure des groupes formés.

\hypertarget{muxe9thodes-non-hiuxe9rarchiques}{%
\section{Méthodes non hiérarchiques}\label{muxe9thodes-non-hiuxe9rarchiques}}

Contrairement aux méthodes hiérarchiques, il faut spécifier le nombre de groupe désiré dès le départ pour les méthodes non hiérarchiques. La méthode des \(k\) moyennes (\(k\) \emph{means}) sera la seule décrite ici. Cette méthode utilise la distance euclidienne et est donc seulement applicable avec des variables quantitatives. Supposons que l'on veuille \(k\) groupes. La méthode des \(k\) moyennes peut être décrite en trois étapes :

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  On sélectionne \(k\) germes (« seeds ») qui vont agir comme centres préliminaires des groupes.

  \begin{enumerate}
  \def\labelenumii{\roman{enumii})}
  \tightlist
  \item
    Ces germes peuvent être les centres des groupes obtenus à partir d'une autre méthode comme une méthode hiérarchique.
  \item
    Ces germes peuvent être choisis à même l'ensemble de données. Par exemple, on peut choisir \(k\) sujets au hasard.
  \end{enumerate}
\item
  On assigne dans l'ordre les observations au groupe le plus proche (distance euclidienne par rapport au germe). Soit on assigne toutes les observations avant de mettre à jour les germes, soit on met à jour les germes après chaque assignation d'un sujet. Le nouveau germe d'un groupe est la moyenne des observations faisant partie du groupe.
\item
  On peut répéter le processus jusqu'à ce que les changements des germes des groupes deviennent négligeables ou nuls. Les groupes finaux sont formés en assignant les sujets au groupe le plus proche.
\end{enumerate}

Nous allons utiliser cette procédure pour raffiner la solution obtenue précédemment avec la méthode de Ward en utilisant les moyennes des groupes comme centres préliminaires. Le fichier \texttt{cluster3\_non-hierarchique.sas} explique les différentes options. La syntaxe de la procédure \textbf{SAS} \texttt{fastclus} est la suivante:

\begin{verbatim}
proc fastclus data=temp seed=initial distance maxclusters=3 out=temp3 maxiter=30;
var x1 x2 x3 x4 x5 x6;
run;
\end{verbatim}

Voici une partie de la sortie \textbf{SAS}:

\begin{center}\includegraphics[width=0.7\linewidth]{figures/04-clustering-e14} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{figures/04-clustering-e15} \end{center}

\begin{center}\includegraphics[width=0.5\linewidth]{figures/04-clustering-e16} \end{center}

Évidemment, comme la solution obtenue avec la méthode de Ward est déjà excellente (146 observations de bien regroupées sur 150), on ne pourra pas avoir une amélioration notable. Il y a peu de changements par rapport à la solution de la méthode de Ward. Les tailles des groupes étaient de (43, 75, 32) avant. Elles sont maintenant (45, 77, 28). Le R carré passe de 65,7\% (avec Ward) à 66.2\%.

L'interprétation des groupes est la même que précédemment.

\begin{center}\includegraphics[width=0.7\linewidth]{figures/04-clustering-e17} \end{center}

Il y a maintenant 148 sujets sur 150 (98,7\%) qui sont bien regroupés et 2 qui font partie d'un mauvais groupe. C'est une légère amélioration par rapport à la solution avec la méthode de Ward pour laquelle il y avait 146 sujets sur 150 qui étaient bien regroupés. Mais encore une fois, en pratique on n'aura pas accès à cette information.

\hypertarget{standardisation-des-variables}{%
\section{Standardisation des variables}\label{standardisation-des-variables}}

Dans les exemples précédents, nous avons utilisé les variables \(X_1\) à \(X_6\) telles quelles. En général, plus une variable a une grande variance, plus elle aura de l'influence sur le calcul de la distance euclidienne. Ainsi, en utilisant les variables telles quelles, nous accordons plus de poids aux variables avec de grandes variances. Ceci peut être bon ou mauvais. Cela dépend de la structure des groupes.

Avec la procédure \texttt{stdize}, on peut standardiser au préalable les variables avant de faire l'analyse. Par défaut, les variables seront standardisées afin d'avoir une moyenne de zéro et une variance de un. On peut ensuite faire les analyses comme précédemment. Le fichier \texttt{cluster4\_standardisation.sas} contient les commandes pour standardiser les variables et ensuite refaire l'analyse de regroupements avec la méthode de Ward. Notez que les six variables ont des variances semblables, donc les résultats ne devraient donc pas être trop affectés en standardisant les variables. Il s'avère effectivement que les résultats changent très peu si on standardise les variables. Les tailles des groupes de la solution à trois groupes sont (44, 75, 31) (comparativement à (43, 75, 32) sans la standardisation). Il s'avère que 147 des 150 sujets sont bien regroupés (comparativement à 146 sans la standardisation). Il s'agit d'une très légère amélioration.

\hypertarget{considuxe9rations-pratiques}{%
\section{Considérations pratiques}\label{considuxe9rations-pratiques}}

Il peut être intéressant de comparer les résultats provenant d'une même méthode avec des nombres différents de groupes et aussi comparer ceux provenant de plusieurs méthodes (voir plus loin pour la description de certaines autres méthodes). Le choix de la méthode et du nombre de groupe n'est pas facile et devrait être basé sur des considérations pratiques et d'interprétation (comme en analyse factorielle). Il n'est pas rare qu'on obtienne des résultats très différents d'une méthode à l'autre pour un même ensemble de données.

Avec une méthode non hiérarchique, il est préférable de fournir des germes de départ « raisonnablement bon » (provenant d'une méthode hiérarchique par exemple) plutôt que de laisser l'algorithme les choisir au hasard.

Le choix des variables est important. En général on veut créer des groupes d'individus qui sont homogènes par rapport à un certain aspect de leur comportement ou de leur situation. On ne doit alors inclure que les variables pertinentes à cet aspect. Par exemple, si le but de l'analyse est de segmenter nos clients selon leurs habitudes de consommation (genre de boutiques fréquenté, fréquence, etc.), on ne voudra peut-être pas inclure des variables démographiques. En fait, souvent l'analyse de regroupements servira justement à créer des groupes qui seront comparés (avec l'analyse de variance par exemple) par rapport à d'autres variables qui n'ont pas été utilisées pour créer les groupes. Dans notre exemple sur les voyages organisés, on a segmenté les voyageurs en trois groupes (indépendants, dépendants et sociables). Les auteurs de l'article (voir page 369 de l'article) ont comparé les trois groupes selon l'expérience de voyage, la taille de la communauté où ils habitent (avec des ANOVA), selon leur âge, leur revenu et leur éducation (avec des tests d'indépendance du khi-deux).

Le problème majeur avec l'analyse de regroupements est qu'il n'y a pas de façon claire de quantifier la performance de notre analyse. Lorsqu'on développe un modèle de prédiction (régression linéaire ou logistique par exemple), on peut estimer la performance de notre modèle d'une manière objective à l'aide de l'erreur quadratique de généralisation (régression linéaire) ou du taux de bonne classification (régression logistique). Ces quantités peuvent être estimées d'une manière objective en utilisant une méthode telle la validation croisée ou la division de l'échantillon. On ne peut faire de même avec l'analyse de regroupements car on n'a pas de variable réponse à prédire. Tout comme pour l'analyse factorielle, les connaissances à priori, le jugement, et les considérations pratiques font partie d'une analyse de regroupements.

\hypertarget{exploration-graphique-pruxe9alable-et-analyse-en-composantes-principales}{%
\section{Exploration graphique préalable et analyse en composantes principales}\label{exploration-graphique-pruxe9alable-et-analyse-en-composantes-principales}}

Comme c'est le cas avec n'importe quelle analyse statistique, il est nécessaire de tenter d'explorer les données graphiquement. On peut parfois réussir à visualiser les groupes d'observations.

Une première idée consiste à faire le graphe de toutes les paires de variables mais ceci possède deux limites,

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\tightlist
\item
  il y aura beaucoup de graphes si le nombre de variables est grand et
\item
  on examine seulement les relations bivariées.
\end{enumerate}

\begin{center}\includegraphics[width=0.9\linewidth]{MATH60602_files/figure-latex/04-pairplot-1} \end{center}

Il n'est pas nécessairement évident de détecter des groupes d'observations ainsi, alors qu'on n'a déjà que six variables. On connaît les vrais groupes ici mais ce n'est pas le cas en pratique.

\hypertarget{analyse-en-composantes-principales}{%
\subsection{Analyse en composantes principales}\label{analyse-en-composantes-principales}}

L'analyse en composantes principales peut être vue comme une méthode de réduction de la dimensionnalité. En fait, elle peut servir à faire de l'analyse factorielle et nous en avons déjà parlé dans le chapitre correspondant. En partant de \(p\) variables \(X_1, \ldots, X_p\), on forme de nouvelles variables qui sont des combinaisons linéaires des variables originales,
\begin{align*}
C_j &= w_{j1} X_1 + w_{j2} X_2 + \cdots + w_{jp} X_p, \qquad (j=1, \ldots, p),
\end{align*}
de telle sorte que

\begin{itemize}
\tightlist
\item
  La première variable formée, \(C_1\), appelée première composante principale, possède la variance maximale parmi toutes les combinaisons linéaires sous la contrainte \(w_{11}^2 + \cdots + w_{1p}^2=1\).
\item
  La deuxième composante principale \(C_2\) possède la variance maximale parmi toutes les combinaisons linéaires qui sont non corrélées avec \(C_1\) sous la contrainte \(w_{21}^2 + \cdots + w_{2p}^2=1\).
\item
  La troisième composante principale \(C_3\) possède la variance maximale parmi toutes les combinaisons linéaires qui sont non corrélées avec \(C_1\) et \(C_2\) sous la contrainte \(w_{31}^2 + \cdots + w_{3p}^2=1\).
\end{itemize}

et ainsi de suite. Les contraintes sont nécessaires afin de standardiser le problème car il serait possible d'avoir des variances infinies sinon. Ainsi, les composantes principales forment un ensemble de variables non corrélées entre elles, qui récupèrent en ordre décroisant le plus possible de la variance des variables originales. La somme des variances des \(p\) composantes principales est égale à la somme des variances des \(p\) variables originales.

On espère donc en général qu'un petit nombre de composantes principales réussira à expliquer la plus grande partie de la variance totale. On pourra alors se servir de ces dernières dans d'autres analyses mais ici, on va seulement s'en servir comme outil graphique préalable à une analyse de regroupements.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/04-clustering-e18} \end{center}

Cette étape est généralement effectuée avant l'analyse de regroupement.
Le fichier \texttt{cluster5\_acp.sas} contient les commandes pour faire une analyse en composantes principales et sauvegarder les composantes principales afin d'en faire des graphiques. La sortie inclut une mesure de la variance cumulée des \(k\) premières valeurs propres. La colonne \texttt{Proportion} donne la proportion de la variance totale qui est expliquée par la composante correspondante. La colonne \texttt{Cumulé} donne le cumul de variance totale expliquée par les composantes jusque là. Ainsi, les deux premières composantes principales reproduisent 76,7\% de la variance totale originale.

Même en ne connaissant pas l'appartenance des observations au regroupement, on distingue assez clairement trois groupes. Le panneau droit du graphique @ref(fig:04\_acp) montre les deux composantes principales, mais avec l'identification des vrais groupes (qu'on ne connaîtra pas en pratique).

\begin{center}\includegraphics[width=0.9\linewidth]{MATH60602_files/figure-latex/04_acp-1} \end{center}

\hypertarget{calcul-alternatif-des-distances-pour-le-regroupement-hiuxe9rarchique}{%
\section{Calcul alternatif des distances pour le regroupement hiérarchique}\label{calcul-alternatif-des-distances-pour-le-regroupement-hiuxe9rarchique}}

Nous avons utilisé la méthode de Ward afin de calculer la distance entre les groupes et procéder au passage de \(n\) groupes à un groupe, avec l'approche hiérarchique. Supposons que nous avons choisi une mesure de dissemblance \(d(S_i, S_j)\) quelconque (distance euclidienne par exemple) pour mesurer la distance entre deux sujets. D'autres méthodes sont disponibles dans la procédure \texttt{cluster}, et les regroupements utilisent
Voici comment sont choisis les regroupements avec ces méthodes.

\begin{itemize}
\tightlist
\item
  Méthode du plus proche voisin ou méthode de liaison simple (\emph{nearest neighbor}, \_single linkage*): utilise la distance minimale entre chaque paire de sujets (un pour chaque groupe) provenant des deux groupes.
\item
  Méthode du voisin le plus éloigné ou méthode de liaison complète (\emph{complete linkage}): utilise la distance maximale entre toutes les paires de sujets (un pour chaque groupe) provenant des deux groupes.
\item
  Méthode de liaison moyenne (\emph{average linkage}): utilise la moyenne des distances entre toutes les paires de sujets (un pour chaque groupe) provenant des deux groupes.
\item
  Méthode du barycentre (\emph{centroid}): utilise la distance entre les représentants moyens de chaque groupe où le représentant moyen d'un groupe est le barycentre, soit la moyenne variable par variable, des sujets formant le groupe.
\end{itemize}

Le fichier \texttt{cluster6\_voisin\_eloigne.sas} contient les commandes pour utiliser la méthode du voisin le plus éloigné (avec l'option \texttt{method=complete}). Le graphe plus bas donne cette distance pour les deux groupes qui viennent d'être fusionnés. Il s'agit donc du maximum des distances entre chaque paire de sujets (un pour chaque groupe) provenant des deux groupes fusionnés.

\begin{center}\includegraphics[width=0.8\linewidth]{figures/04-clustering-e19} \end{center}

Comme on veut que cette distance soit petite pour les groupes fusionnés, on pourrait être tenté d'arrêter à trois groupes ici sur la base de la Figure @ref(fig:fig4\_e19). L'interprétation des groupes ne change pas comparativement aux analyses précédentes. La taille des groupes, (44, 71, 35) change un peu par rapport à la solution avec la méthode de Ward qui donnait des tailles de (43, 75, 32). En fait, avec la méthode du voisin le plus éloigné, 141 des 150 sujets sont dans le bon groupe (comparativement à 146 avec la méthode de Ward). Elle fait donc un peu moins bien pour cet exemple.

On peut comparer les performances des regroupements hiérarchiques selon la méthode de groupement. La \href{https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html}{page web de scikit-learn developers} montre la performance sur des exemples jouets, qui montre que selon les hypothèses et la structure, aucune ne performe mieux que les autres dans tous les exemples.

\hypertarget{autres-mesures-de-dissemblance}{%
\section{Autres mesures de dissemblance}\label{autres-mesures-de-dissemblance}}

Nous avons déjà mentionné que lorsque toutes les variables sont continues, la mesure de dissemblance la plus utilisée est la distance euclidienne. Dans la procédure \texttt{cluster}, \textbf{SAS} utilise la distance euclidienne au carré par défaut. Pour utiliser la distance euclidienne elle-même, il faut mettre le mot clé \texttt{nosquare} dans les options suivant l'appel à \texttt{proc\ cluster} (première ligne). Pour utiliser une mesure de dissemblance autre que la distance euclidienne (au carré ou pas), on peut utiliser la procédure \texttt{distance} au préalable pour calculer les distances, et ensuite fournir la matrice des distances directement à \texttt{cluster} en lieu et place des observations.

Il existe un très grand nombre d'autres mesures de dissemblance pour variables quantitatives, ordinales, nominales et binaires. Voici une brève description de certaines d'entre elles, qui sont disponibles dans \texttt{proc\ distance}.

Mesures de dissemblance pour variables quantitatives :

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Distance euclidienne ou distance euclidienne au carré
\item
  Distance de Manhattan, ou taxi-distance:
  \begin{align*}
  d(S_i, S_j) = |X_{i1}-X_{j1}| + \cdots + |X_{ip}- X_{jp}|
  \end{align*}
\end{enumerate}

Mesure de dissemblance pour variables nominales:

Le plus simple est d'utiliser la mesure d'appariement simple (\emph{simple matching}). Cette mesure est simplement de la proportion des variables pour lesquelles les deux sujets ont des valeurs différentes.

Mesures de dissemblances pour variables ordinales:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Une manière simple consiste à les traiter comme des variables continues, et utiliser la distance euclidienne ou la distance de Manhattan. On peut aussi les standardiser au préalable si elles ne sont pas sur la même échelle (par exemple, si certaines vont de 1 à 5 et d'autres de 1 à 7).
\item
  On peut aussi les traiter comme des variables nominales avec la mesure d'appariement simple; ce faisant, on n'utilise pas l'ordre entre les modalités.
\end{enumerate}

Le fichier \texttt{cluster7\_cityblock.sas} contient les commandes pour refaire l'analyse des regroupements du voyage organisé avec la taxi-distance et la méthode de liaison moyenne (\texttt{method=average}) dans \texttt{proc\ cluster}.

Encore une fois, l'interprétation des groupes ne change pas comparativement aux analyses précédentes. La taille des groupes, (45, 75, 30) change un peu par rapport à la solution avec la méthode de Ward qui donnait des tailles de (43, 75, 32). Avec la distance de Manhattan et la méthode de liaison moyenne, 148 des 150 sujets sont dans le bon groupe (comparativement à 146 avec la méthode de Ward). Cette combinaison fait donc mieux pour cet exemple.

\bibliography{book.bib,packages.bib}

\end{document}
