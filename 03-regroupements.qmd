# Analyse de regroupements {#analyse-regroupements}


```{r}
#| label: setup-section3
#| include: false
#| message: false
#| warning: false
# automatically create a bib database for R packages
library(knitr)
library(kableExtra)
library(hecmulti)
set.seed(1014)

knitr::opts_chunk$set(
  collapse = TRUE,
  cache = TRUE,
  out.width = "100%",
  fig.align = 'center',
  fig.width = 8.5,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.table.format = function() {
  if (knitr::is_latex_output())
    "latex" else "html"
})

options(dplyr.print_min = 6, dplyr.print_max = 6)
options(knitr.graphics.auto_pdf = TRUE)
options(scipen = 1, digits = 3)
library(viridis)
library(ggplot2, warn.conflicts = FALSE, quietly = TRUE)
library(poorman, quietly = TRUE, warn.conflicts = FALSE)


safe_colorblind_palette <- MetBrewer::met.brewer("Hiroshige",10)

options(ggplot2.continuous.colour="viridis")
options(ggplot2.continuous.fill = "viridis")
scale_colour_discrete <- scale_color_manual(MetBrewer::met.brewer("Hiroshige",10))
scale_fill_discrete <- scale_fill_manual(MetBrewer::met.brewer("Hiroshige",10))
theme_set(theme_classic())
```


## Introduction

Si la publicité ciblée personnalisée a pris de l'essort ces derniers années en commercialisation, la segmentation de consommateurs reste une partie prenante essentielle de toute campagne de publicité ou de développement de produits.

L'analyse de regroupement est une technique d'**analyse descriptive** qui sert à combiner des sujets en groupes de telle sorte que les individus d'un même groupe soient le plus semblables possible  et que les groupes soient le plus différent possible les uns des autres, avec des valeurs aberrantes clairement identifiées. Cette similarité est définie selon des caractéristiques provenant de variables explicatives. Le résultat de l'analyse de regroupement sera une étiquette associée à chaque observation l'assignant à un regroupement ou l'identifiant comme aberrance, nous permettant ainsi de caractériser (par le biais de statistiques descriptives) les différents **segments** obtenus.

Il y a une certaine analogie avec l'analyse factorielle. En analyse factorielle, on cherche à déterminer s'il y a des groupes de **variables** corrélées entre elles et à les regrouper pour réduire le nombre de variables. En analyse de regroupements, on cherche plutôt à créer des groupes de **sujets** similaires. Les deux méthodes servent pour l'analyse exploratoire.

Pour créer les regroupements, on utilisera $p$ variables explicatives $X_1, \ldots, X_p$ pour chacune des $n$ observations, où $X_{ij}$ dénotera la valeur de la $j$e variable explicative pour le $i$e sujet.



:::{.callout-tip}
## Étapes d'une analyse de regroupements

1. Choisir les variables pertinentes à l'analyse. Cette étape peut nécessiter de créer, transformer de nouvelles variables ou d'aggréger les données.
2. Décider quel méthode (algorithme, modèle, mesure de dissemblance) sera utilisé pour la segmentation.
3. Choisir les hyperparamètres de l'algorithme (nombre de regroupements, rayon, etc.) 
4. Procéder à l'analyse de regroupements.
5. Calculer une mesure de qualité.
6. Assigner les étiquettes aux observations et calculer un représentant-type de groupe. 
7. Interpréter les regroupements obtenus.


:::

## Données


Voici en vrac quelques exemples de bases de données sur lesquelles on pourrait effectuer une analyse de regroupements. 

Les programmes de fidélisation font partie de la stratégie de commercialisation de plusieurs grandes chaînes (pharmacies, épiceries): en échange de rabais et d'offres promotionnelles, la clientèle fournit des informations sociodémographique (nom, adresse, date de naissance, etc.) et utilise un identifiant numérique, une carte ou une application pour inscrire chaque achat: ce faisant, le système peut traquer les habitudes de consommation.^[Il existe bien sûr d'autres méthodes de traque pour les personnes qui n'ont pas de compte client, notamment par le biais de numéros de carte de débit ou de crédit qui permettent de regrouper les transactions.] Créer des regroupements permet de mieux cerner les besoins et habitudes de segments de consommateurs et ainsi d'adapter l'offre promotionnelle. Les algorithmes utilisés pour l'analyse de regroupements peuvent également servir à la résolution d'entité, qui consiste à fusionner les profils de bases de données sans identifiant unique client.


Un autre exemple d'application de l'analyse de regroupements est la segmentation de la clientèle de transport en commun. Dans la région métropolitaine de Montréal, l'[Agence régionale de transport métropolitain](https://www.artm.quebec/) recueille des informations sur les passages et  transactions par le biais des cartes à puce Opus (achat de passes mensuelles ou de billets unitaires, lieu de l'achat, etc.) ainsi que les passages (heure, type de véhicule, emplacement approximatif pour les services d'autobus ou station de métro). En créant des regroupements, une agence de transport peut ainsi ajuster son offre et proposer des abonnements ou des produits qui reflètent les besoins de sa clientèle. Un exemple extrême de traquage de compagnie de transport est [*Nederlandse Spoorwegen* (NS)](https://www.ns.nl/en/): toute personne qui veut voyager en train sur les chemins de fers néerlandais doit acheter une carte à puce et la charger, en plus de composter son billet au départ et à l'arrivée de son voyage. Cette approche, qui peut sembler intrusive, permet néanmoins de mesurer précisément la demande sur les lignes en fonction du moment de la journée et de l'associer à chaque client.

Dans ce chapitre, nous utiliserons des données simulées inspirées de campagnes de financement d'organismes de charité. Ces dernières font souvent du démarchage publicitaire auprès de donateurs ou envoient par publipostage des demandes de dons à toutes les adresses postales. Ces efforts ont un coût important: nous essaierons de créer des catégories de donateurs afin de mieux cibler les donateurs et donatrices et le moment adéquat pour ce démarchage.

Souvent, les bases de données marketing sont souvent de nature longitudinale: chaque ligne correspond à une transaction, mais plusieurs d'entre elles peuvent être le fait d'une même personne/compte. Une fois l'analyse exploratoire des données complétée, on procédera à l'aggrégation des observations par compte client, puisque la segmentation doit être effectuée à cette échelle. C'est également à ce stade qu'on pourra créer de nouvelles variables explicatives à partir de l'information présente dans la base de données: par exemple, on pourrait considérer la fréquence moyenne d'achat, le montant moyen par transaction, le mode du moment de la journée, la variabilité de cette fréquentation, le pourcentage des ventes provenant d'articles en solde, la variabilité du montant du panier, etc. Cette liste, non exhaustive, illustre l'étape cruciale de l'extraction de l'information utilisée dans l'analyse statistique: il faut être conscients que la qualité de la segmentation dépend du choix de variables employées.


## Choix des variables

L'analyste est libre de choisir quelles variables seront incluses dans le modèle. Le choix des variables est important: en général on veut créer des groupes d'individus qui sont homogènes par rapport à certains aspects de leur comportement ou de leur situation. On ne doit alors inclure que les variables pertinentes à cet aspect. Inclure de nombreuses variables pour lesquelles il y a une forte similitude entre individus contribue à diluer les différences.

Par exemple, si le but de l'analyse est de segmenter nos clients selon leurs habitudes de consommation (genre de boutiques fréquenté, fréquence, etc.), on n'inclura pas des variables démographiques qui feraient ressortir les différences de genre, d'âge, de revenu, etc.. En fait, souvent l'analyse de regroupements servira justement à créer des groupes qui seront comparés par rapport à d'autres variables qui n'ont pas été utilisées pour créer les groupes. 

La compréhension de la base de données est cruciale pour comprendre le comportement. Si on essaie de faire une segmentation du comportement d'utilisateurs et utilisatrices de transports en commun à partir d'informations auxiliaires comme le temps de passage, le nombre de correspondance et la fréquence d'utilisation, il peut être utile de créer de nouvelles variables (par exemple, une variable indicatrice qui indique si la personne voyage durant les heures de traffic entre 7h30 et 9h et 16h à 18h, le nombre hebdomadaire moyen de jours ouvrables pendant lesquels elle se déplac, etc). L'inclusion des ces variables auxiliaires peut augmenter la qualité de la segmentation.

Pour voir si certaines variables sont inutiles, il peut être utile de comparer les représentants des groupes (par exemple, le barycentre ou une observation lambda du groupe) pour voir si les moyennes ou caractéristiques diffèrent. Si ce n'est pas le cas, on pourrait envisager de recommencer la procédure en enlevant cette variable.

Si on a un nombre important de variables explicatives à disposition, il est parfois utile de réduire préalablement la dimension (par exemple, en effectuant une analyse en composantes principales) et à ne retenir que les premières composantes pour faciliter la tâche. Cette approche n'est pas la panacée: quelquefois, cette réduction de la dimension masque les différences entre groupe et mène à une segmentation inférieure à l'utilisation des variables originales.

Malheureusement, il n'est pas évident de prime abord de déterminer quelles variables inclure dans la base de données pas plus qu'il n'est facile de juger de la qualité d'une segmentation ou du nombre de regroupements à effectuer. Les choix individuels auront un impact certain sur les regroupements obtenus: on recommande d'essayer plusieurs alternatives et de vérifier graphiquement ou à l'aide de critères d'ajustement si les regroupements obtenus sont homogènes et compacts.

Si certaines variables définissent naturellement des groupes, par exemple l'âge des personnes, et fait qu'ils et elles ont des caractéristiques intrinsèquement différentes, il peut être utile de faire une segmentation indépendamment pour chacun de ces sous-groupes. 

La base de données `dons` contient `r nrow(hecmulti::dons)` observations pour `r ncol(hecmulti::dons)` variables. Elle a été crée en regroupement les identifiants: de nombreuses variables explicatives sont dérivées des données brutes, notamment le temps entre dons, les statistiques descriptives (montant moyen, minimum maximum) pour les dons monétaires. Ces choix de variables sont loins d'être anodins et peuvent influencer la segmentation décrite dans ce chapitre. Une rapide exploration des données révèle que près de `r round(100*sum(is.na(dons$vdons))/nrow(dons), 0)`% des employé(e)s n'ont pas donné à l'organisme. Une poignée de dons sont très élevés, mais la plupart des montants tourne autour de 5$, 10$, 20$, etc.


```{r}
#| label: tbl-statdescriptdons
#| echo: false
#| cache: true
#| eval: true
#| tbl-cap: "Statistiques descriptives des variables du jeu de données dons."
data(dons, package = "hecmulti")
tibble::tibble(moyenne = apply(dons, 2, mean, na.rm = TRUE),
            "écart-type" = apply(dons, 2, sd, na.rm = TRUE),
             min = apply(dons, 2, min, na.rm = TRUE),
             max = apply(dons, 2, max, na.rm = TRUE),
             manquant = apply(dons, 2, function(x){sum(is.na(x))})) |>
knitr::kable(digits = 2,
             booktabs = TRUE) |>
kableExtra::kable_styling(full_width = TRUE)
```

La grande proportion de données manquantes pose un problème immédiat pour la segmentation, puisque la plupart des procédures ne permettent pas de traiter ces dernières et éliminent d'office ces lignes. Ici, plusieurs valeurs manquantes (`NA`) peuvent être logiquement remplacées par des valeurs numériques: par exemple, la valeur cumulative des dons (`vdons`) d'une personne qui n'a jamais donné est nulle.^[Imputer par la moyenne ou utiliser une méthode plus sophistiqué serait illogique (et incorrect).] En revanche, le temps d'attente entre deux dons pour une personne qui a fait un don ou moins n'est pas bien défini.

Si on essaie de créer manuellement des groupes, il apparaît logique de séparer en trois segments initiaux la base de données: les personnes qui n'ont jamais donné à l'organisme de charité mais dont les caractéristiques sont connues, les personnes qui ont fait un seul don et celles qui ont fait des dons multiples. Un algorithme ferait de toute façon vraisemblablement ressortir cette information, mais nous empêcherait d'exploiter pleinement l'ensemble des variables explicatives et de ses dérivées. On pourra effectuer la segmentation séparément sur chaque groupe avec en intrant des variables explicatives différentes.

Dans notre exemple, on pourrait aisément créer de nouvelles variables pour faire ressortir des informations jugées pertinentes. Est-ce qu'on s'intéresse au montant moyen des dons, soit `vdons/ndons`? Est-ce que la valeur des radiations nous intéresse, ou bien devrait-on plutôt considérer le pourcentage de la valeur promise réalisée? Pour rappel, les intrants de l'analyse de regroupement (soit le choix des variables) est laissé à la discrétion de l'analyste. 

## Mesures de dissemblance et de similarité


Les algorithmes de segmentation comparent les observations entre elles selon une mesure de similitude: ainsi, la matrice de données est réduite à une mesure de distance entre observations (soit les lignes de la base de données). Comment mesurer si deux observations appartiennent à un même regroupement et sont similaires? Idéalement, on aimerait avoir une situation comme dans la @fig-regroupements-bidons où les regroupements sont clairement visibles. On aimerait que la similarité entre observations d'un même groupe, ou intra-groupe, soit élevée et que la similarité entre groupe soit faible. Les regroupements devraient être éloignés les uns des autres, tandis que les observations au sein de ces regroupements devraient être proches. Dans la plupart des cas, il y aura des observations isolées qui n'appartiennent pas nécessairement logiquement à l'un ou l'autre des groupes: on appelle parfois ces observations aberrances.

```{r}
#| label: fig-regroupements-bidons
#| fig-cap: "Données simulées avec deux regroupements hypothétiques."
#| echo: false
set.seed(1234)
dat <- rbind(
  mvtnorm::rmvnorm(n = 50, 
                 mean = c(-10,0), 
                 sigma = rWishart(n = 1, 
                                  df = 5, 
                                  Sigma = diag(0.25,2,2))[,,1]),
  mvtnorm::rmvt(n = 100, 
                sigma = cbind(c(2,-1), c(-1,1.2)),
                df = 3, 
                delta = c(6,8)))
dat <- data.frame(dat)
colnames(dat) <- c("x1", "x2")
ggplot(data = dat, aes(x = x1, y = x2)) + 
  geom_point() + 
  labs(x = expression(X[1]),
       y = expression(X[2])) + 
  theme_minimal()

```


Une **mesure de dissemblance** sert à quantifier la proximité de deux objets à partir de leurs coordoonnées. Elle mesure la distance entre deux vecteurs d'observations $\mathbf{X}_i$ et $\mathbf{X}_j$ en se basant sur les $p$ variables explicatives $X_1, \ldots, X_p$. Plus la dissemblance est petite, plus les sujets $\mathbf{X}_i$ et $\mathbf{X}j$ sont similaires. Même s'il y a des exceptions, la plupart des mesures de dissemblances $d$ ont les propriétés mathématiques suivantes:

1) $d(\mathbf{X}_i, \mathbf{X}_j) \geq 0$ (positivité), avec égalité (distance nulle) si et seulement si $\mathbf{X}_i=\mathbf{X}_j$ (mêmes caractéristiques pour toutes les variables explicatives);
2) $d(\mathbf{X}_i, \mathbf{X}_j)=d(\mathbf{X}_j, \mathbf{X}_i)$ (symmétrie);

Toute mesure de distance^[Une fonction de distance respecte en plus l'inégalité du triangle.] est une mesure de dissemblance. La mesure de dissemblance la plus utilisée en pratique est la distance euclidienne entre sujets, soit
\begin{align*}
d(\mathbf{X}_i, \mathbf{X}_j; l_2) = \left\{(X_{i1}-X_{j1})^2 + \cdots + (X_{ip}-X_{jp})^2\right\}^{1/2}.
\end{align*}
C'est tout simplement la longueur du segment qui relie les deux points dans l'espace. 

Plus généralement, la distance de Minkowski ou distance $l_q$ entre les vecteurs ligne $\mathbf{X}_i$ et $\mathbf{X}_j$ est
\begin{align*}
d(\mathbf{X}_i, \mathbf{X}_j; l_q) = \left( \sum_{k=1}^p |X_{ik}-X_{jk}|^q \right)^{1/q},\qquad q > 0;
\end{align*}
la distance Euclidienne correspondant à $q=2$, et la distance de Manhattan à $q=1$^[La distance de Manhattan est la somme des valeurs absolues entre chaque composante. En deux dimensions, si on considère une ville comme New York dont les rues sont quadrillées, cela revient à marcher le long des rues alors que la distance Euclidienne traverse les édifices.] . Finalement, si $q=\infty$, la distance se réduit à $\max_{k=1}^p |X_{ik}-X_{jk}|$, soit le maximum des différences entre coordonnées des vecteurs d'observations.


Il existe un très grand nombre d'autres mesures de dissemblance pour variables quantitatives, ordinales, nominales et binaires. Si les variables sont toutes binaires, la mesure d'appariement simple (*simple matching*), qui mesure la proportion des variables pour lesquelles les deux sujets ont des valeurs différentes, est une mesure de dissemblance adéquate.

Dans le cas de jeux de données avec des variables mixes, une option populaire est la distance de Gower [@Gower:1971]. Cette dernière compare deux individus selon leurs caractéristiques et est construite à partir de similarité, avec $(1-\mathbf{S})^{1/2}$ comme dissimilarité. La similarité entre deux individus est définie comme
\begin{align*}
S_{ij} = \frac{\sum_{k=1}^p s_{ijk} \delta_{ijk}}{\sum_{k=1}^p \delta_{ijk}}
\end{align*}
où $\delta_{ijk}$ est un poids qui vaut zéro si la variable $\mathrm{X}_k$ est manquante pour l'un ou l'autre des individus. 

On distingue trois type de variables dans la distance de Gowers: 

- les variables binaires asymmétrique  de type absence/présence donnent une valeur de $\delta=1, s=1$ si les deux sont présentes $X_{ik}=X_{jk}=1$, $\delta_{ijk}=1$ et $s_{ijk}=0$ si $X_{ik} \neq X_{jk}$ et $\delta_{ijk}=0$ si $X_{ik}=X_{jk}=0$.
- $s_{ijk}=1$ les variables qualitatives ont la même modalité et $s_{ijk}=0$ sinon
- $s_{ijk} = 1-|X_{ik}-X_{jk}|/R_k$ pour une variable continue, où $R_k$ est l'étendue de la variable $R_k=\max_{i} X_{ik} - \min_i X_{ik}$ dans l'échantillon. 

La dissemblance résultante pour les types mixtes vaut 1 quand toutes les variables sont similaires/égales et zéro si elles sont complètement différentes/maximalement distantes.

On peut traiter les variables ordinales comme continues ou les traiter comme des variables nominales avec la mesure d'appariement simple; ce faisant, on n'utilise pas l'ordre entre les modalités.

Dans plusieurs cas, on se trouvera en présence de valeurs manquantes dans le jeu de données. Cela peut arriver pour plusieurs raisons valables (aucun candidature ne représente un partir dans une circonscription donnée lors d'une élection, l'information est manquante, une femme ne peut avoir de cancer de la prostate, etc.) Il faut bien penser à vérifier si l'algorithme de votre choix peut gérer ces valeurs manquantes. Sinon, ces dernières devront être imputées ou vous devrez faire sans les variables explicatives correspondantes.


Il existe d'autres façons de définir la similarité entre observations. Par exemple, si on considère la dissemblance Euclidienne, on pourrait créer une matrice de similarité $\mathbf{S}$ $n \times n$ où les entrées indiquent si les observations sont à distance $\varepsilon$ les unes des autres (si oui, en assignant $1$ et autrement $0$) ou encore avec $1$ pour les $m$ plus proches voisins et $0$ autrement. Cette approche sera utilisée lors de la présentation des regroupement spectraux.

Les définitions des distances révèlent que chaque variable explicative a le même poids. En revanche, plus une variable a une grande variance, plus elle aura de l'influence sur le calcul de la distance, ce qui peut être bon ou mauvais selon la structure des groupes. Règle générale, il est préférable d'éviter qu'une variable domine dans la segmentation. La standardisation des variables et les transformations préalables effectuées sur les variables (log, arcsin, etc.) impacteront le résultat. On peut standardiser au préalable les variables avant de faire l'analyse. Par défaut, les variables continues seront centrées et réduites, ou standardisées, afin d'avoir une moyenne de zéro et une variance de un (`scale`). On peut ensuite faire les analyses comme précédemment. Si on a des valeurs aberrantes, cela peut impacter le calcul des moyennes et variances; d'autres estimateurs de localisation et d'échelles plus robustes, par exemple la médiane et la déviation absolue par rapport à la médiane (`mad`) peuvent alors être plus adéquats pour diminuer l'impact des valeurs aberrantes même si le coût de calcul associé est plus conséquent. Notez qu'il est illogique de standardiser les variables binaires et catégorielles.

```{r}
#| label: standardisation
#| eval: false
#| echo: true
data(cluster, package = "hecmulti")
# Standardisation usuelle 
# (soustraire moyenne, diviser par écart-type)
cluster_std <- scale(cluster)
# Standardisation robuste
cluster_std_rob <- apply(cluster, 
                         MARGIN = 2, 
                         function(x){
                           (x - median(x))/mad(x)})
# apply permet d'appliquer une fonction
# par ligne, colonne ou cellule
# MARGIN = 2 indique colonne 
# (on centre chaque colonne tour à tour)
```

## Algorithmes pour la segmentation

L'analyse de regroupements est une branche de l'apprentissage non-supervisé: contrairement à la classification, il n'existe pas de vraies étiquettes sur lesquelles se baser pour déterminer la qualité d'une segmentation. Des critères graphiques, des mesures d'homogénéité peuvent néanmoins déterminer à quel points les segments créés sont distincts les uns des autres.

L'analyse de regroupements cherche à créer une division de $n$ observations de $p$ variables en $k$ regroupements. Il existe un grand nombre d'algorithmes qui permettent de partitionner les données en regroupements à partir d'un jeu de données ou d'une matrice de dissemblance ou de similarité. Les sections suivantes survoleront différents algorithmes en s'attardant à l'heuristique de l'implémentation, aux différentes étapes de la procédure, aux hyperparamètres qui influencent le résultat (par ex., le nombre de groupes, la distance minimale entre regroupements, la forme des regroupements, les éléments représentatifs) qui détermine la sortie ainsi que les forces et faiblesses des algorithmes. À l'ère des mégadonnées, la complexité d'un algorithme de regroupements, une mesure du nombre d'opérations nécessaires pour effectuer le calcul, impactera le choix possible: l'algorithme de regroupements hiérarchiques (agglomératif ou divisif), de même que l'algorithme de partition autour des médoïdes (PAM) sont à proscrire dans ces scénarios. Outre l'algorithme, il y a des coûts associés au calcul de la matrice de dissemblance entre chacune des paires des $n$ observations: cette opération nécessite $\mathrm{O}(n^2p)$ flops pour le calcul et $\mathrm{O}(n^2)$ entrées de stockage.

Les méthodes de regroupement peuvent être regroupées grossièrement dans les catégories suivantes:

1. méthodes basées sur la connectivité (regroupements hiérarchiques, AGNES et DIANA)
2. méthodes basées sur les centroïdes et les médoïdes ($k$-moyennes, $k$-médoides  PAM, CLARA)
3. mélanges de modèles (mélanges Gaussiens, etc.)
4. méthodes basées sur la densité (DBScan)
5. méthodes spectrales

Nous survolerons uniquement les caractéristiques des principales méthodes.

### K-moyennes

L'algorithme des $K$-moyennes est un des plus couramment employé en raison de son faible coût.

L'idée est la suivante: on assigne chaque observation à un de $K$ regroupements et on calcule la distance entre cette dernière et un prototype $\boldsymbol{\mu}_k$ pour le regroupement $k$. La fonction objective que l'on cherche à minimiser est
$$
\min_{\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_K}\min_{\stackrel{r_{ik} \in \{0, 1\}}{r_{i1} + \cdots + r_{iK}=1}}\underset{\text{distance entre obs. $i$ et son prototype}}{\sum_{i=1}^n \sum_{k=1}^K r_{ik}d(\mathbf{X}_i,  \boldsymbol{\mu}_{k})}
$$ {#eq-fobjKmoy}
où $r_{ik}=1$ si l'observation $\mathbf{X}_i$ (soit la $i$e ligne de la base de données) est assignée au groupe $k$. Si on utilise la distance Euclidienne carrée, alors la fonction objective correspond à la somme du carré des erreurs et la solution $\widehat{\boldsymbol{\mu}}_k$ est le barycentre des $n_k$ observations du groupe $k$, soit
\begin{align*}
\widehat{\boldsymbol{\mu}}_k = \frac{\sum_{i} r_{ik} \mathbf{X}_i}{n_k}, \quad k = 1, \ldots, K;
\end{align*}
d'où l'appelation $K$-moyennes. Il n'est pas possible de déterminer l'allocation optimale de $n$ observations en $K$ groupes (problème NP complet), mais il est en revanche possible de trouver rapidement une solution approximative au problème.

Pour ce faire, on sélectionne préalablement un nombre $K$ de regroupements et les coordonnées de départ pour les prototypes. L'algorithme itère entre deux étapes:

1. **Assignation** (étape E): calculer la distance entre chaque observation et les prototypes; assigner chaque observation au prototype le plus près.
2. **Mise à jour** (étape M): calculer les nouveaux prototypes, soit le barycentre (la moyenne variable par variable) des observations assignées aux regroupements si on utilise la distance Euclidienne.

En pratique, l'algorithme convergera rapidement vers une solution locale. Cette dernière est simplement une assignation pour laquelle, d'une itération à l'autre, aucune observation ne change de groupe.
La @fig-kmoy-animation montre une animation avec un jeu de données fictif et $K=3$ regroupements.


Dans la sortie des $K$-moyennes, chaque observation est associée à un seul regroupement, encodée par la responsabilité $r_{ik}$. L'algorithme présenté offre une forme de partitionnement dite rigide: chaque observation est assignée à un seul regroupement puisque $r_{ik}$ est binaire. Si cette appartenance unique peut être logique pour les points à proximité du barycentre, ceux situés à l'intersection des frontières qui définissent les différents regroupements pourraient légitimement faire partie d'un ou l'autre de ces derniers.


```{r}
#| label: fig-kmoy-animation
#| echo: false
#| eval: true
#| cache: true
#| fig-cap: "Animation de l'algorithme des $K$-moyennes avec $K=3$ regroupements."
#| out-width: '100%'
#| fig-show: 'animate'
#| fig-format: 'png'
set.seed(1234)

library(animation)
clusters <- rbind(
  TruncatedNormal::rtmvnorm(
    n = rpois(n = 1, lambda = 100), 
    mu = c(-1,0), 
    sigma = cbind(c(2,-1),c(-1,1.2)),
    lb = c(0,0)),
  TruncatedNormal::rtmvnorm(n = rpois(n = 1, lambda = 225), 
                   mu = c(7,4), 
                   sigma = diag(rep(3, 2)), lb = c(0,0)),
  cbind(runif(n = 10, min = 20, max = 20), 
        runif(n = 10, min = 20, max = 20)),
  0.5 + qgamma(p = copula::rCopula(n = 300, copula = copula::claytonCopula(param = 5)), shape = 8, scale = 1)
  )
colnames(clusters) <- c("variable 1", "variable 2")
animation::ani.options(ani.dev = "png",
            ani.type = "png")
animation::kmeans.ani(
  x = clusters,
  centers = 3,
  hints = c("Étape M: calcul du barycentre", "Étape E: mise à jour des étiquettes"),
  pch = 20,
  col = viridis::viridis(n = 3)
)
```

Voici quelques forces et faiblesses de la méthode des $K$-moyennes

- Les $K$-moyennes créent des regroupements globulaires d'apparence sphérique si on utilise la distance Euclidienne: cela revient à faire une séparation linéaire de l'espace.
- Chaque observation est assignée à un seul des $K$ regroupements (assignation rigide).
- Comme toutes les observations font partie des $K$ groupes, les valeurs aberrantes ne sont pas traitées à part.
- La présence de valeurs aberrantes impacte le barycentre des observations du groupe. Comme ce dernier donne le prototype du groupe,  l'algorithme manque de robustesse.
- L'algorithme des $K$-moyennes a une complexité linéaire dans la dimension et dans le nombre de variables. Ce faible coût de calcul est un avantage avec des mégadonnées et en haute dimension.
- L'algorithme converge rapidement vers une solution et on a des garanties que la solution est un maximum local, puisque l'algorithme minimise les répartitions et les prototypes tour à tour.
- L'algorithme est sensible aux valeurs initiales des prototypes.


Le champ des applications des $K$-moyennes est parfois surprenant. Par exemple, [cet article de FiveThirtyEight propose une segmentation des électeurs démocrates new-yorkais](https://fivethirtyeight.com/features/the-5-political-boroughs-of-new-york-city/) ou des [quartiers de Los Angeles](https://fivethirtyeight.com/features/the-6-political-neighborhoods-of-los-angeles/). Un autre exemple incongru est la compression d'images: la Figure @fig-decelles montre une image du bâtiment Decelles (coin supérieur gauche) et la reconstruction avec trois, quatre et 10 couleurs obtenues en appliquant l'algorithme des $K$-moyennes sur la matrice formée par les valeurs des canaux (rouge, vert, bleu) de l'image.


```{r fig-decelles}
#| echo: false
#| eval: true 
#| out-width: '100%'
#| fig-align: "center" 
#| fig-cap: "Compression d'image avec l'algorithme des $K$-moyennes: image originale (en haut à gauche), compression avec trois (en haut à droite), quatre (en bas à gauche) et 10 (en bas à droite) couleurs."
knitr::include_graphics("figures/kmoyennes_decelles.png")
```

#### Choix des hyperparamètres

L'algorithme des $K$-moyennes comporte plusieurs paramètres, dit hyperparamètres, qui sont fixés par l'utilisateurs préalablement à la segmentation. Ces derniers incluent le nombre de groupes $K$, les valeurs initiales des prototypes et le choix de la mesure de distance.

Toutes les distances $l_q$ peuvent être utilisées, mais le choix de la distance Euclidienne carrée est particulièrement commode et populaire^[La fonction objective s'apparente alors à la somme du carré des erreurs, et donc il y a une analogie à faire avec la vraisemblance d'un modèle Gaussien en dimension $p$ de covariance sphérique. Cela légitimise l'emploi de critères d'information pour le choix du nombre de regroupements.] entraîne une partition linéaire de l'espace, comme l'illustre la @fig-voronoikmoy. Sauf indication contraire, on supposera dans ce qui suit que la distance entre un point et un prototype est calculée avec la distance Euclidienne au carré.


Quelquefois, on peut vouloir prédire les étiquettes de groupes de nouvelles observations. Sans réentraîner l'algorithme, on pourrait ainsi assigner de nouvelles observations au barycentre le plus près.

```{r}
#| label: fig-voronoikmoy
#| echo: false
#| eval: true
#| fig-cap: "Partitions de Voronoï pour les barycentres obtenus dans la solution des $K$-moyennes."
km1 <- kmeans(clusters, 
              centers = 3, 
              nstart = 10, 
              iter.max = 25,
              algorithm = "Lloyd")
tesselation <- deldir::deldir(x = km1$centers[,1], 
                              y = km1$centers[,2])
# tesselation$rw <- c(0,20,0,20)
tiles <- deldir::tile.list(tesselation)

plot(clusters,
     pch = 20,
     xlab = "", 
     ylab = "",
     col = viridis::viridis(n = 3)[km1$cluster])
plot(tiles, 
     pch = 1, 
     add = TRUE)
```



Comme mentionné précédemment, les regroupements obtenus peuvent varier fortement en fonction des valeurs de départ. Une segmentation sera supérieure à une autre si elle a une plus petite valeur de la fonction objective de l'@eq-fobjKmoy: les points seront moins dispersés autour de leurs prototypes. 

ILLUSTRATION AVEC JEU DE DONNÉES ET MAUVAISES VALEURS DE DÉPART

Pour réduire l'impact et s'assurer d'une segmentation de qualité, on peut prendre plusieurs valeurs de départ initiales différentes et faire l'analyse de regroupement en sélectionnant celle qui a la plus petite valeur de la fonction objective de l'@eq-fobjKmoy. 

On peut également utiliser $K$-moyennes++, qui propose de choisir des barycentres éloignés les uns des autres (ce qui réduit typiquement le nombre d'itérations). Cette méthode d'initialisation sélectionne une observation au hasard et on l'assigne comme premier prototype, disons $\boldsymbol{\mu}_1$. Par la suite, on procède avec $k=2, \ldots, K$ aux étapes suivantes:

1. calcul de la distance carrée minimale entre l'observation $\mathbf{X}_i$ et les prototypes précédemment choisis,
\begin{align*}
p_i = \min \{d(\mathbf{X}_i, \boldsymbol{\mu}_1; l_2)^2, \ldots, d(\mathbf{X}_i, \boldsymbol{\mu}_{k-1}; l_2)^2)\}
\end{align*}
2. Choisir la valeur initiale du $k$e prototype au hasard parmi les observations avec une probabilité de $p_i/\sum_{j} p_j$ pour l'observation $\mathbf{X}_i$.

À la fin, on obtiendra $K$ valeurs initiales qui serviront à l'initialisation. Ce faisant, on peut espérer ne pas avoir à faire plusieurs allocations aléatoires, puisque les valeurs de départ choisies sont raisonnablement éloignées les unes des autres.


Fonctionnement de l'algorithme avec illustrations
Frontières de séparation linéaires de l'espace

Algorithme EM: minimum local

Détail: initialisation rigide

Initialisation aléatoire et $K$-moyennes ++
Choix du nombre de groupements et profils




## Mélange de modèles

L'algorithme des $K$-moyennes fait une allocation rigide: chaque observation est assignée à un seul regroupement, ignorant de ce fait l'incertitude rattachée à l'étiquetage des observations.
Les frontières de la région, obtenue en calculant l'intersection des courbes sphériques de regroupement, sont linéaires.

Peut-être plus problématique, la distance euclidienne non pondérée impose des regroupement sphériques de taille semblable: la qualité des regroupements des $K$ moyennes est donc mauvaise si les regroupements ne sont pas sphériques ou globulaires, ou sont de concentration inégale.


Une approche plus générale considère que $X_1, \ldots, X_p$ sont tirées d'un mélange à $K$ composantes de lois aléatoires spécifiées. Généralement, on spécifie une loi normale multidimensionnelle pour le $k$e groupe $G$,
\begin{align*}
\boldsymbol{X} \mid G=k \sim \mathsf{No}_p(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{align*}
On suppose qu'on a $K$ groupes, chacun caractérisé par une densité de dimension $p$, soit $f_k(\boldsymbol{X}_i;\boldsymbol{\theta}_k)$ si $\boldsymbol{X}_i$ provient du groupe $k$ pour $k=1, \ldots, K$.

On réécrit la vraisemblance en fonction de $\pi_k$, la probabilité qu'une observation $\mathbf{X}$ tombe dans le groupe $k$,
\begin{align*}
 L(\boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_K; \pi_1, \ldots, \pi_K, \mathbf{X})= \prod_{i=1}^n \sum_{k=1}^K \pi_k
f_{k}(\boldsymbol{X}_i,
\boldsymbol{\theta}_{k}).
\end{align*}
Si on connaissait les poids du mélange $\boldsymbol{\pi}$ avec $\sum_{k=1}^K \pi_k = 1$ et $\pi_k \in (0,1)$, on peut simplement obtenir les estimation du maximum de vraisemblance pour le paramètre de moyenne et de variance. Comme les poids sont inconnus, on estime le modèle à l'aide de l'algorithme d'espérance-maximisation. Le paramètres retournés correspondent à un maximum local, et on peut également obtenir un estimé de la variabilité de ces paramètres. Ainsi, le mélange de modèle nous donne accès à la fois à l'incertitude des paramètres et la probabilité $\pi_k$ qu'une observation appartiennent au groupe $G_k$.

Si $p$ est élevé, la structure de covariance non structurée possède trop de paramètres pour être utile. On limitera ce nombre en choisissant plutôt une paramétrisation plus parsimonieuse qui impose des contraintes sur la forme des ellipsoïdes, propres ou communs à tous les groupes.



| Nom |   volume  |  forme  |  orientation |
|:-----|:------:|:------:|:------:|
| EII |   constant  |  sphérique  |  non-définie |
| VII |     variable  | sphérique  |  non-définie |
| EEE |   constant  |  constante  |  constant |
| EEV |  constant | constante  | variable |
| EVV |  constant | variable  | variable |
| VVV |   variable  |  variable  | variable |

Le choix du nombre de composantes, de même que la forme du modèle, est basée sur la sélection bayésienne de modèles. On utilise une approximation basée sur le critère d'information bayésien, ou $\mathsf{BIC}$, en choisissant le nombre de composantes qui minimise le $\mathsf{BIC}$. 


- Avantage: plus flexible que $K$-moyennes
- Avantage: composante uniforme permet de gérer aberrance
- Désavantage: algorithme EM mène à un minimum local
- Désavantage: coût de calcul élevé, dépend de la dimension du problème (complexité en $p$)

Mélanges de modèles Gaussiens

Hyperparamètres: nombre de groupes $K$, forme des ellipsoïdes

Partition autour de médoïdes

Algorithmes PAM et CLARA





## Analyse exploratoire graphique

Comme c'est le cas avec n'importe quelle analyse statistique, il est utile d'explorer les données graphiquement. On peut parfois réussir à visualiser les groupes d'observations, ce qui nous permettra une fois l'analyse de regroupement complétée de vérifier la qualité de cette dernière. 

On pourrait produire un nuage de points pour chaque paire de variables mais cette idée est problématique pour deux raisons: il y aura beaucoup de graphes si le nombre de variables est grand et on examine seulement les relations bivariées.

On peut utiliser les outils du chapitre précédent et réduire le nombre de variables en considérant plutôt les composantes principales. Dans notre exemple, on va seulement s'en servir comme outil graphique pour une analyse de regroupements en réduisant la dimension afin de permettre la visualisation des regroupements obtenus. Ces dernières pourraient aussi servir de variables pour l'analyse de regroupement: une fois les étiquettes obtenues, il suffirait de calculer les statistiques descriptives sur les variables originales. 


```{r}
#| label: fig-eboulis-cluster
#| echo: false
#| out-width: '100%'
#| fig-align: "center"
#| fig-cap: "Diagramme d'éboulis de la matrice de corrélation des données `dons` pour les donateurs multiples."
data(dons, package = "hecmulti")
dons |>
  dplyr::filter(ndons > 1) |>
  cor(use = "pairwise.complete.obs") |>
  eigen() |>
  hecmulti::eboulis()
```


### Regroupements spectraux

L'algorithme de $K$-moyennes est un des plus populaires pour la segmentation en raison de son faible coût de calcul et de sa simplicité.

### Algorithm DBSCAN

### Mélanges de modèles 

### Regroupements spectraux

## Mesure de la qualité des regroupements

### Mesures d'adéquation

### Choix des hyperparamètres
